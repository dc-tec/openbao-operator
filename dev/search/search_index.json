{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OpenBao Operator","text":"<p>Welcome to the documentation for the OpenBao Operator, a Kubernetes native operator for managing OpenBao clusters.</p> <p>Experimental Status</p> <p>This operator is currently in an experimental phase and is actively seeking feedback. It is not recommended for production environments at this time.</p> <ul> <li> <p> User Guide</p> <p>Step-by-step guides to deploy, configure, and operate OpenBao clusters on Kubernetes.</p> <p> Getting Started</p> </li> <li> <p> Security</p> <p>Threat modeling, RBAC design, admission policies, and security hardening guidelines.</p> <p> Explore Security</p> </li> <li> <p> Architecture</p> <p>Deep dive into the controller design, reconciliation loops, and key lifecycle flows.</p> <p> View Ecosystem</p> </li> <li> <p> Contributing</p> <p>Setup your development environment, build targets, and testing strategies.</p> <p> Start Contributing</p> </li> </ul>"},{"location":"#why-openbao-operator","title":"Why OpenBao Operator?","text":"<ul> <li> <p> Automated Lifecycle</p> <p>Seamlessly provision, scale, and upgrade clusters with zero downtime using advanced state management.</p> </li> <li> <p> Security First</p> <p>Secure-by-default configuration with automated TLS rotation, sealed unsealing, and strict RBAC profiles.</p> </li> <li> <p> Day 2 Operations</p> <p>Built-in backup/restore controllers, automated snapshots, and detailed metrics for production reliability.</p> </li> <li> <p> Kubernetes Native</p> <p>Designed with standard CRDs, detailed Status conditions, and full integration with the Kubernetes ecosystem.</p> </li> </ul>"},{"location":"#community","title":"Community","text":"<p>Connect with other users and contributors:</p> <ul> <li> <p> GitHub</p> <p>Report bugs, request features, and contribute code.</p> <p> Go to Repository</p> </li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Compatibility Matrix \u2014 Supported Kubernetes and OpenBao versions.</li> </ul>"},{"location":"architecture/","title":"Architecture: OpenBao Supervisor Operator","text":"<p>This document provides a comprehensive overview of the OpenBao Operator's architecture.</p> <ul> <li> <p> Overview</p> <p>High-level design and supervisor pattern.</p> <p> Jump to Overview</p> </li> <li> <p> Components</p> <p>Controller manager, interacting controllers, and state.</p> <p> Jump to Components</p> </li> <li> <p> Security</p> <p>Least-privilege RBAC and zero-trust model.</p> <p> Security Docs</p> </li> <li> <p> API Spec</p> <p>CRD specification and status fields.</p> <p> Jump to API</p> </li> </ul>"},{"location":"architecture/#1-architecture-overview","title":"1. Architecture Overview","text":"<p>The OpenBao Operator adopts a Supervisor Pattern. It delegates data consistency to the OpenBao binary while managing the external ecosystem: PKI lifecycle, Infrastructure state, and Safe Version Upgrades.</p>"},{"location":"architecture/#11-tenancy-models","title":"1.1 Tenancy Models","text":"<p>The operator supports two architectural modes:</p> <ol> <li>Multi-Tenant (Default): Uses a Provisioner controller to manage RBAC and namespaces dynamically. Adopts a Zero-Trust model where the Controller has limited permissions.</li> <li>Single-Tenant: Designed for direct embedding. The Provisioner is disabled, and the Controller manages the target namespace directly with full permissions.</li> </ol>"},{"location":"architecture/#12-system-components","title":"1.2 System Components","text":"<pre><code>graph TD\n    User([User]) --&gt;|CRD| API[Kubernetes API]\n\n    subgraph \"Operator Controller Manager\"\n        Workload[Workload Ctrl]\n        AdminOps[AdminOps Ctrl]\n        Status[Status Ctrl]\n        Provisioner[Provisioner Ctrl]\n    end\n\n    API --&gt;|Watch| Operator[Operator Manager]\n    Operator --&gt; Workload &amp; AdminOps &amp; Status &amp; Provisioner\n\n    Workload --&gt;|Reconcile| STS[StatefulSet]\n    Workload --&gt;|Reconcile| Svc[Services]\n    AdminOps --&gt;|Backup| ObjStore[Object Storage]\n\n    STS --&gt;|Run| Pods[OpenBao Pods]\n    Pods --&gt;|Data| PVC[Persistent Volumes]</code></pre>"},{"location":"architecture/#13-component-interaction","title":"1.3 Component Interaction","text":"<pre><code>sequenceDiagram\n    autonumber\n    actor User\n    participant API as K8s API\n    participant Workload as Workload Ctrl\n    participant AdminOps as AdminOps Ctrl\n    participant Pods as OpenBao Pods\n\n    User-&gt;&gt;API: Apply OpenBaoCluster\n    API-&gt;&gt;Workload: Reconcile Event\n    Workload-&gt;&gt;API: Create ConfigMaps &amp; Secrets\n    Workload-&gt;&gt;API: Create StatefulSet\n    API-&gt;&gt;Pods: Schedule Pods\n    Pods-&gt;&gt;Pods: Peer Discovery &amp; Join\n\n    loop Monitoring\n        AdminOps-&gt;&gt;API: Check Status\n        opt Backup Scheduled\n            AdminOps-&gt;&gt;Pods: Trigger Snapshot\n        end\n    end</code></pre>"},{"location":"architecture/#14-assumptions","title":"1.4 Assumptions","text":"<p>Core Assumptions</p> <ul> <li>Storage: Default StorageClass available.</li> <li>Network: Working DNS for StatefulSet identity.</li> <li>Version: OpenBao v2.4.0+ (required for static auto-unseal).</li> </ul>"},{"location":"architecture/#cross-cutting-concerns","title":"Cross-Cutting Concerns","text":""},{"location":"architecture/#observability","title":"Observability","text":"Metric Description <code>openbao_cluster_ready_replicas</code> Number of Ready replicas <code>openbao_reconcile_duration_seconds</code> Reconciliation duration <code>openbao_upgrade_status</code> Upgrade status (0=Idle, 1=Upgrading)"},{"location":"architecture/#api-specification","title":"API Specification","text":"<p>The <code>OpenBaoCluster</code> CRD defines the desired state.</p>"},{"location":"architecture/#spec-desired-state","title":"Spec (Desired State)","text":"<pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nspec:\n  version: \"2.4.4\"       # (1)!\n  image: \"openbao:2.4.4\" # (2)!\n  replicas: 3            # (3)!\n  tls:\n    enabled: true        # (4)!\n  unseal:\n    type: awskms         # (5)!\n  profile: Hardened      # (6)!\n</code></pre> <ol> <li>Semantic OpenBao version.</li> <li>Container image reference.</li> <li>Number of replicas (default: 3).</li> <li>Enable Operator-managed TLS.</li> <li>Auto-unseal mechanism (static or external).</li> <li>Security posture (<code>Hardened</code> or <code>Development</code>).</li> </ol>"},{"location":"architecture/#status-observability","title":"Status (Observability)","text":"<pre><code>status:\n  phase: Running  # (1)!\n  activeLeader: pod-0  # (2)!\n  readyReplicas: 3  # (3)!\n  conditions:  # (4)!\n    - Type: Available\n      Status: True\n</code></pre> <ol> <li>High-level lifecycle phase.</li> <li>Current Raft leader.</li> <li>Number of ready pods.</li> <li>Standard Kubernetes conditions.</li> </ol>"},{"location":"architecture/backup-manager/","title":"BackupManager (Snapshots to Object Storage)","text":"<p>Responsibility</p> <p>Schedule and execute Raft snapshots, stream them to object storage, and manage retention.</p> <p>User Guide</p> <p>For operational instructions, see the Backups User Guide and Restore User Guide.</p>"},{"location":"architecture/backup-manager/#1-backup-workflow","title":"1. Backup Workflow","text":"<p>The Manager uses a stateless executor pattern: the Operator only schedules Kubernetes Jobs; it does not handle data itself.</p> <pre><code>graph TD\n    Trigger[Trigger: Cron or Manual] --&gt; Prelight{Pre-flight Checks}\n\n    Prelight -- Fail --&gt; Retry[Retry Later]\n    Prelight -- Pass --&gt; Job[Create Backup Job]\n\n    subgraph K8s_Job [Executor Pod]\n        Job --&gt; Auth[Authenticate to Vault]\n        Auth --&gt; Stream[Stream Snapshot]\n        Stream --&gt; Upload[Upload to S3/GCS/Azure]\n    end\n\n    Upload --&gt; Verify{Verify?}\n    Verify -- Success --&gt; Status[Update Status: LastSuccessfulBackup]\n    Verify -- Fail --&gt; StatusFail[Update Status: ConsecutiveFailures]\n\n    classDef process fill:transparent,stroke:#9333ea,stroke-width:2px,color:#fff;\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Job,Auth,Stream,Upload process;\n    class Trigger,Prelight read;\n    class Status,StatusFail write;</code></pre>"},{"location":"architecture/backup-manager/#2-execution-phases","title":"2. Execution Phases","text":""},{"location":"architecture/backup-manager/#phase-1-pre-flight-checks","title":"Phase 1: Pre-flight Checks","text":"<p>Before spawning a Job, the Operator verifies the cluster is stable:</p> <ol> <li>Healthy: Cluster Phase must be <code>Running</code>.</li> <li>Stable: No Upgrade (<code>Status.Upgrade == nil</code>) or Restore (<code>Status.Restore == nil</code>) in progress.</li> <li>Exclusive: No other backup is currently running.</li> </ol>"},{"location":"architecture/backup-manager/#phase-2-the-executor-job","title":"Phase 2: The Executor Job","text":"<p>The Operator creates a Kubernetes Job named <code>backup-&lt;cluster&gt;-&lt;timestamp&gt;</code>. This Job:</p> <ul> <li>Runs the specialized <code>bao-backup</code> binary.</li> <li>Connects to the active Leader.</li> <li>Streams the snapshot directly to object storage (no local disk buffering required).</li> </ul>"},{"location":"architecture/backup-manager/#3-configuration-authentication","title":"3. Configuration &amp; Authentication","text":"<p>The BackupManager supports multiple authentication methods for object storage.</p> Static CredentialsWeb Identity (IRSA) <p>Best for: On-prem S3, MinIO, or simple setups.</p> <p>Mounts a Kubernetes Secret containing access keys.</p> <pre><code>spec:\n  backup:\n    target:\n      credentialsSecretRef:\n        name: my-s3-keys\n</code></pre> <p>The Secret must contain keys appropriate for the provider (e.g., <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>).</p> <p>Best for: AWS EKS (Production).</p> <p>Uses a ServiceAccount with an IAM Role annotation. No long-lived secrets are stored in the cluster.</p> <pre><code>spec:\n  backup:\n    target:\n      roleArn: arn:aws:iam::123456789012:role/openbao-backup\n</code></pre> <p>The Operator projects the ServiceAccount token into the backup Job, and the AWS SDK uses it to assume the role.</p>"},{"location":"architecture/backup-manager/#4-scheduling-retention","title":"4. Scheduling &amp; Retention","text":"<ul> <li>Cron: Uses standard cron syntax (e.g., <code>0 2 * * *</code> for daily at 2 AM).</li> <li>Retention:</li> <li>If using Static Credentials, the Operator can enforce a <code>retentionPolicy</code> (e.g., \"keep last 5\").</li> <li>If using Web Identity, we recommend using bucket-native lifecycle policies (e.g., S3 Lifecycle Rules) for better efficiency.</li> </ul>"},{"location":"architecture/backup-manager/#5-naming-convention","title":"5. Naming Convention","text":"<p>Backups are stored with a predictable path structure for easy retrieval during disaster recovery:</p> <pre><code>&lt;pathPrefix&gt;/&lt;namespace&gt;/&lt;cluster&gt;/&lt;timestamp&gt;-&lt;short-uuid&gt;.snap\n</code></pre>"},{"location":"architecture/cert-manager/","title":"Cert Manager (TLS Lifecycle)","text":"<p>Responsibility: Bootstrap PKI, Rotate Certificates, and signal Hot Reloads.</p> <p>The CertManager ensures that OpenBao always has valid TLS certificates without requiring manual intervention or pod restarts.</p>"},{"location":"architecture/cert-manager/#1-operating-modes","title":"1. Operating Modes","text":"<p>Controlled by <code>spec.tls.mode</code>.</p> Feature OperatorManaged (Default) External ACME CA Source Generated Self-Signed Root CA User provided Secret Let's Encrypt / ACME Server Cert Operator Generated (ECDSA P-256) User provided Secret OpenBao Internal Rotation Automatic (Operator) External (cert-manager) Automatic (OpenBao) Reload Automatic (Signal) Automatic (Signal) Internal"},{"location":"architecture/cert-manager/#2-architecture","title":"2. Architecture","text":""},{"location":"architecture/cert-manager/#tls-rotation-loop-operatormanaged","title":"TLS Rotation Loop (<code>OperatorManaged</code>)","text":"<p>allows the operator to rotate certificates atomically without downtime.</p> <pre><code>graph TD\n    Check{Check Expiry} --&gt;|Expired?| Generate[Generate New Cert]\n    Generate --&gt;|Update| Secret[(\"fa:fa-key TLS Secret\")]\n\n    Secret -.-&gt;|Watch| Operator\n    Operator --&gt;|Compute Hash| Annotation[\"Pod Annotation\\n(openbao.org/tls-hash)\"]\n    Annotation --&gt;|Update| StatefulSet\n\n    StatefulSet --&gt;|Reconcile| Pods[OpenBao Pods]\n\n    classDef process fill:transparent,stroke:#9333ea,stroke-width:2px,color:#fff;\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Check,Generate process;\n    class Secret,Annotation,StatefulSet write;\n    class Operator,Pods read;</code></pre>"},{"location":"architecture/cert-manager/#hot-reload-mechanism","title":"Hot Reload Mechanism","text":"<p>We avoid <code>ShareProcessNamespace</code> or <code>exec</code> privileges by using a lightweight wrapper process (PID 1).</p> <pre><code>sequenceDiagram\n    participant Op as Operator\n    participant K8s as Kubernetes\n    participant Pod as Pod (PID 1 Wrapper)\n    participant Bao as OpenBao (PID 2)\n\n    Op-&gt;&gt;K8s: Update Secret (New Cert)\n    K8s-&gt;&gt;Pod: Update Volume Mount (/vault/tls/server.crt)\n    Pod-&gt;&gt;Pod: Detect File Change (fsnotify)\n    Pod-&gt;&gt;Bao: Send SIGHUP\n    Bao-&gt;&gt;Bao: Reload Certificate\n\n    note right of Op: No Pod Restart Required</code></pre>"},{"location":"architecture/cert-manager/#3-implementation-details","title":"3. Implementation Details","text":"OperatorManagedExternalACME <p>Best for: Most users, development, and self-contained environments.</p> <ol> <li>Bootstrap: Checks for <code>Secret/&lt;cluster&gt;-tls-ca</code>. If missing, generates a 10-year ECDSA P-256 Root CA.</li> <li>Issuance: Checks <code>Secret/&lt;cluster&gt;-tls-server</code>. If missing or near expiry (default &lt; 24h), signs a new certificate using the CA.</li> <li>Trust: The CA is automatically mounted to all OpenBao pods and exported to a <code>ConfigMap</code> for clients.</li> </ol> <p>Best for: Production environments using <code>cert-manager</code> or external PKI.</p> <ol> <li>Passive: The operator stops generating secrets.</li> <li>Wait: It waits for <code>Secret/&lt;cluster&gt;-tls-ca</code> and <code>Secret/&lt;cluster&gt;-tls-server</code> to be created by an external tool.</li> <li>Signaling: It continues to monitor the content of these secrets. When your external tool rotates the cert, the operator detects the hash change and triggers the hot-reload signal.</li> </ol> <p>Best for: Public-facing clusters needing trusted browser certificates.</p> <ol> <li>Passthrough: The operator renders ACME configuration (<code>tls_acme_...</code>) directly into <code>config.hcl</code>.</li> <li>No Secrets: Kubernetes Secrets are NOT used. OpenBao stores certs internally (or in a persistent cache).</li> <li>No Reload: The Operator is effectively bypassed for TLS logic. OpenBao handles its own rotation loop.</li> </ol>"},{"location":"architecture/components/","title":"Component Design","text":"<p>The OpenBao Operator uses a Split-Controller Architecture. Instead of a single monolithic reconciliation loop, we divide responsibilities across three specialized controllers per <code>OpenBaoCluster</code>.</p>"},{"location":"architecture/components/#1-controller-hierarchy","title":"1. Controller Hierarchy","text":"<p>We separate Workload (Pod churn), Operations (Upgrades/Backups), and Status (Updates) to prevent head-of-line blocking and status write contention.</p> <pre><code>graph TD\n    Manager[Manager Process] --&gt;|Starts| Workload[(\"fa:fa-server Workload Controller\")]\n    Manager --&gt;|Starts| Admin[(\"fa:fa-tools AdminOps Controller\")]\n    Manager --&gt;|Starts| Status[(\"fa:fa-pen-to-square Status Controller\")]\n\n    subgraph Roles [\"Responsibilities\"]\n        Workload --&gt;|Delegates to| Infra[Infra Manager]\n        Workload --&gt;|Delegates to| Cert[Cert Manager]\n\n        Admin --&gt;|Delegates to| Upgrade[Upgrade Manager]\n        Admin --&gt;|Delegates to| Backup[Backup Manager]\n\n        Status --&gt;|Aggregates| Conditions[Status Conditions]\n    end\n\n    classDef process fill:transparent,stroke:#9333ea,stroke-width:2px,color:#fff;\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Manager process;\n    class Workload,Admin,Status write;\n    class Infra,Cert,Upgrade,Backup,Conditions read;</code></pre>"},{"location":"architecture/components/#2-controllers","title":"2. Controllers","text":"Controller Role Why Separate? Workload Reconciles StatefulSet, Services, ConfigMaps, and Secrets. High churn. Needs to react fast to Pod failures. AdminOps Handles Upgrades, Backups, and Restores. Long-running operations. Should not block Pod recovery. Status Aggregates status from other controllers and writes to API. Prevents <code>ResourceVersion</code> conflicts by serializing status updates."},{"location":"architecture/components/#3-internal-managers","title":"3. Internal Managers","text":"<p>Controllers delegate complex business logic to specialized Internal Managers.</p> <pre><code>classDiagram\n    class OpenBaoClusterReconciler {\n        +Reconcile()\n    }\n\n    class InfraManager {\n        +SyncStatefulSet()\n        +SyncService()\n    }\n\n    class CertManager {\n        +SyncPKI()\n        +RotateCerts()\n    }\n\n    class UpgradeManager {\n        +ReconcileUpdate()\n    }\n\n    OpenBaoClusterReconciler --&gt; InfraManager : Uses\n    OpenBaoClusterReconciler --&gt; CertManager : Uses\n    OpenBaoClusterReconciler --&gt; UpgradeManager : Uses\n\n    style OpenBaoClusterReconciler fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff\n    style InfraManager fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff\n    style CertManager fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff\n    style UpgradeManager fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff</code></pre>"},{"location":"architecture/components/#domain-managers","title":"Domain Managers","text":"<ul> <li>Infrastructure Manager: The \"heart\" of the operator. Generates <code>config.hcl</code> and manages the <code>StatefulSet</code>.</li> <li>Cert Manager: Handles TLS interactions. Supports <code>OperatorManaged</code> (internal CA), <code>ACME</code> (LetsEncrypt), and <code>External</code> (Bring your own).</li> <li>Init Manager: Auto-initializes new clusters, handling the <code>sys/init</code> call and root token encryption.</li> <li>Upgrade Manager: Powering both Rolling and Blue/Green upgrades. Manages the state machine for complex transitions.</li> <li>Backup Manager: Runs snapshot jobs on a Cron schedule.</li> </ul>"},{"location":"architecture/components/#shared-libraries","title":"Shared Libraries","text":"<ul> <li><code>internal/provisioner</code>: Handles RBAC and Namespace creation for Tenants.</li> <li><code>internal/config</code>: A pure-functional HCL generator that renders OpenBao configuration.</li> </ul>"},{"location":"architecture/infra-manager/","title":"InfrastructureManager (Config &amp; StatefulSet)","text":"<p>Responsibility: The \"Heart\" of the operator. It translates the high-level <code>OpenBaoCluster</code> spec into a running <code>StatefulSet</code> with a valid <code>config.hcl</code>.</p>"},{"location":"architecture/infra-manager/#1-reconciliation-pipeline","title":"1. Reconciliation Pipeline","text":"<p>The Manager follows a strict Render-Then-Apply pipeline to ensure configuration consistency.</p> <pre><code>graph TD\n    Spec[OpenBaoCluster Spec] --&gt;|Render| Config[config.hcl]\n    Spec --&gt;|Render| Resources[StatefulSet / Services]\n\n    Config --&gt;|Hash| Checksum{Config Match?}\n    Resources --&gt;|Hash| ResChecksum{Resource Match?}\n\n    Checksum -- No --&gt; UpdateCM[Update ConfigMap]\n    ResChecksum -- No --&gt; UpdateSS[Update StatefulSet]\n\n    UpdateCM --&gt; Rollout[Rolling Update]\n    UpdateSS --&gt; Rollout\n\n    classDef process fill:transparent,stroke:#9333ea,stroke-width:2px,color:#fff;\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Spec read;\n    class Config,Resources process;\n    class UpdateCM,UpdateSS,Rollout write;</code></pre>"},{"location":"architecture/infra-manager/#2-configuration-generation","title":"2. Configuration Generation","text":"<p>We do not use a static ConfigMap. We generate it dynamically from the Spec.</p> <pre><code>ui = true\n\nlistener \"tcp\" {\n  address = \"0.0.0.0:8200\"\n  cluster_address = \"0.0.0.0:8201\"\n\n  # Injected: Points to Secret mounts\n  tls_cert_file = \"/etc/bao/tls/tls.crt\" # (1)!\n  tls_key_file  = \"/etc/bao/tls/tls.key\"\n}\n\nstorage \"raft\" {\n  path = \"/bao/data\"\n  node_id = \"${HOSTNAME}\"\n\n  retry_join {\n    # Injected: Discovery via Kubernetes Labels\n    auto_join = \"provider=k8s label_selector=\\\"openbao.org/cluster=prod-cluster\\\"\" # (2)!\n    leader_tls_servername = \"openbao-cluster-prod-cluster.local\"\n  }\n}\n\nservice_registration \"kubernetes\" {} # (3)!\n</code></pre> <ol> <li>Paths are automatically adjusted based on <code>spec.tls.mode</code> (e.g., ACME mode removes these).</li> <li>Enables automatic peer discovery without manual <code>join</code> commands.</li> <li>Ensures Pods register themselves as endpoints.</li> </ol>"},{"location":"architecture/infra-manager/#3-auto-unseal-integration","title":"3. Auto-Unseal Integration","text":"<p>The Manager automatically configures the <code>seal</code> stanza based on <code>spec.unseal</code>.</p> Static (Default)External KMS <p>If <code>spec.unseal</code> is omitted, the operator manages the unseal keys.</p> <ol> <li>Generate: Creates 32 random bytes.</li> <li>Store: Saves to <code>Secret/&lt;cluster&gt;-unseal-key</code>.</li> <li>Mount: Mounts at <code>/etc/bao/unseal/key</code>.</li> <li>Config: <pre><code>seal \"static\" {\n  current_key    = \"file:///etc/bao/unseal/key\"\n  current_key_id = \"operator-generated-v1\"\n}\n</code></pre></li> </ol> <p>If <code>spec.unseal.type</code> is set (e.g., <code>awskms</code>, <code>gcpckms</code>), the operator delegates to the provider.</p> <ol> <li>No Secret: Does NOT create an unseal key Secret.</li> <li>Mount Creds: Mounts <code>spec.unseal.credentialsSecretRef</code> to <code>/etc/bao/seal-creds</code>.</li> <li>Config: Renders the specific seal block:     <pre><code>seal \"awskms\" {\n  region     = \"us-east-1\"\n  kms_key_id = \"alias/my-key\"\n}\n</code></pre></li> </ol>"},{"location":"architecture/infra-manager/#4-image-verification-cosign","title":"4. Image Verification (Cosign)","text":"<p>When <code>spec.imageVerification.enabled</code> is <code>true</code>, we enforce supply chain security.</p> <pre><code>sequenceDiagram\n    participant Op as Operator\n    participant Reg as Registry (OCI)\n    participant SS as StatefulSet\n\n    Op-&gt;&gt;Reg: Fetch Image Digest\n    Op-&gt;&gt;Reg: Fetch Signature (Cosign)\n    Op-&gt;&gt;Op: Verify Signature (Public Key)\n\n    alt Verification Failed\n        Op--xSS: Block Update\n        Op-&gt;&gt;Status: Set Condition False\n    else Verified\n        Op-&gt;&gt;SS: Update with Digest (sha256:...)\n    end</code></pre> Policy Behavior <code>Block</code> (Default) Stops reconciliation. No unsafe image runs. <code>Warn</code> Logs error, emits Event, but Allows the update."},{"location":"architecture/infra-manager/#5-reconciliation-semantics","title":"5. Reconciliation Semantics","text":"<ul> <li>OwnerReferences: All resources (ConfigMaps, Services, StatefulSets) are owned by the <code>OpenBaoCluster</code> CR. Deleting the CR deletes the cluster.</li> <li>Least Privilege: The controller only watches <code>OpenBaoCluster</code>. It does not watch child resources (except via OwnerReference garbage collection) to reduce API load.</li> <li>Discovery: Uses <code>leader_tls_servername</code> to support strict mTLS verification between peers.</li> </ul>"},{"location":"architecture/init-manager/","title":"InitManager (Cluster Initialization)","text":"<p>Responsibility</p> <p>Automate the \"Day 1\" bootstrap of a new cluster.</p> <p>To avoid split-brain scenarios during initial formation, the Operator enforces a Single-Pod Bootstrap pattern.</p>"},{"location":"architecture/init-manager/#1-workflow","title":"1. Workflow","text":"<p>The InitManager coordinates the first start of the cluster.</p> <pre><code>sequenceDiagram\n    participant Op as Operator\n    participant SS as StatefulSet (Replicas=1)\n    participant Pod as Pod-0\n    participant Bao as OpenBao API\n    participant Secret as Root Token Secret\n\n    Note over Op, SS: Phase 1: Bootstrap\n    Op-&gt;&gt;SS: Create having Replicas=1\n    Op-&gt;&gt;Pod: Wait for Running state\n\n    loop Health Check\n        Op-&gt;&gt;Bao: GET /sys/health\n        Bao--&gt;&gt;Op: 501 Not Initialized\n    end\n\n    Note over Op, Secret: Phase 2: Initialize\n    Op-&gt;&gt;Bao: PUT /sys/init\n    Bao--&gt;&gt;Op: Returns {root_token, keys}\n\n    Op-&gt;&gt;Secret: Store Root Token\n    Op-&gt;&gt;Secret: Store Unseal Key (if Static)\n\n    Note over Op, SS: Phase 3: Scale\n    Op-&gt;&gt;SS: Service now Initialized\n    Op-&gt;&gt;SS: Scale to spec.replicas (e.g., 3)</code></pre>"},{"location":"architecture/init-manager/#2-execution-phases","title":"2. Execution Phases","text":"Phase 1: BootstrapPhase 2: InitializePhase 3: Scale Up <p>Goal: Start a single, stable node.</p> <p>Regardless of <code>spec.replicas</code>, a new cluster always starts with 1 replica.</p> <ul> <li>Why? Raft requires a stable leader to form a cluster. Starting 3 uninitialized nodes simultaneously can lead to race conditions on who becomes the first leader.</li> <li>Mechanism: The InfrastructureManager caps <code>replicas: 1</code> as long as <code>status.initialized</code> is <code>false</code>.</li> </ul> <p>Goal: Bootstrap the Raft cluster and generate root material.</p> <p>Once <code>pod-0</code> is running, the InitManager takes over:</p> <ol> <li>Detection: Checks internal status. If it finds <code>openbao-initialized=true</code> label or receives <code>200 OK</code> from <code>/sys/health</code>, it assumes adoption of an existing cluster and skips to Phase 3.</li> <li>Execution: If uninitialized, it calls <code>sys/init</code>.</li> <li>Security:<ul> <li>The Root Token is encrypted and stored immediately in a Secret.</li> <li>The Unseal Key (if using Static mode) is stored in a separate Secret.</li> <li> <p>Security</p> The initialization response is held in memory only for the duration of the request and is NEVER logged. </li> </ul> </li> </ol> <p>Goal: Expand to High Availability.</p> <p>Once initialization is confirmed (and the root token is safely stored):</p> <ol> <li>The Operator sets <code>status.initialized = true</code>.</li> <li>The InfrastructureManager observes this and updates the StatefulSet to the full <code>spec.replicas</code>.</li> <li>New pods join the existing leader (Pod-0) via the <code>retry_join</code> configuration.</li> </ol>"},{"location":"architecture/init-manager/#3-reconciliation-semantics","title":"3. Reconciliation Semantics","text":"<ul> <li>One-Time Operation: The InitManager is designed to be idempotent but typically runs only once in the cluster's lifecycle.</li> <li>Failure Handling: If <code>sys/init</code> fails (network, timeout), the operator retries. The cluster remains at <code>replicas: 1</code> until success.</li> <li>Adoption: You can bring an existing OpenBao cluster under management. If the operator detects it is already initialized, it simply adopts it and moves to Scale Up.</li> </ul>"},{"location":"architecture/init-manager/#4-autopilot-configuration","title":"4. Autopilot Configuration","text":"<p>After successful initialization, the InitManager configures Raft Autopilot for automatic dead server cleanup.</p> <p>Always-On Feature</p> <p>Autopilot dead server cleanup is enabled by default for all clusters. This prevents orphaned Raft peers from accumulating when pods are terminated.</p>"},{"location":"architecture/init-manager/#default-configuration","title":"Default Configuration","text":"Setting Default Value Description <code>cleanup_dead_servers</code> <code>true</code> Enable automatic removal of failed peers <code>dead_server_last_contact_threshold</code> <code>5m</code> Time before a server is considered dead <code>min_quorum</code> <code>max(3, replicas/2+1)</code> Minimum servers before pruning allowed"},{"location":"architecture/init-manager/#customization","title":"Customization","text":"<p>Override defaults via <code>spec.configuration.raft.autopilot</code>:</p> <pre><code>spec:\n  configuration:\n    raft:\n      autopilot:\n        cleanupDeadServers: true\n        deadServerLastContactThreshold: \"5m\"\n        minQuorum: 2\n</code></pre>"},{"location":"architecture/init-manager/#disabling-autopilot-cleanup","title":"Disabling Autopilot Cleanup","text":"<p>To disable automatic dead server cleanup:</p> <pre><code>spec:\n  configuration:\n    raft:\n      autopilot:\n        cleanupDeadServers: false\n</code></pre> <p>Manual Cleanup Required</p> <p>When disabled, you must manually remove dead Raft peers via <code>bao operator raft remove-peer</code>.</p>"},{"location":"architecture/restore-manager/","title":"OpenBaoRestore Controller (Restore Lifecycle)","text":"<p>Responsibility</p> <p>Reconcile <code>OpenBaoRestore</code> resources and orchestrate snapshot restores via a Kubernetes Job.</p> <p>User Guide</p> <p>For operational instructions, see the Restore User Guide.</p>"},{"location":"architecture/restore-manager/#1-design-philosophy","title":"1. Design Philosophy","text":"<ul> <li>CRD-Based: Restores are modeled as <code>OpenBaoRestore</code> objects, not as a mode of <code>OpenBaoCluster</code>. This ensures GitOps stability and provides an audit log of restore operations.</li> <li>Stateless Controller: The controller polls the restore Job rather than watching it, minimizing RBAC requirements.</li> <li>Safety First: Restores use a distinct Operation Lock to prevent conflicts with Backups or Upgrades.</li> </ul>"},{"location":"architecture/restore-manager/#2-restore-lifecycle","title":"2. Restore Lifecycle","text":"<p>The controller drives the <code>OpenBaoRestore</code> through a defined phase machine.</p> <pre><code>graph TD\n    Start[User Creates OpenBaoRestore] --&gt; Pending\n\n    Pending --&gt; Validating{Validate}\n    Validating -- Invalid --&gt; Failed[Phase: Failed]\n    Validating -- Valid --&gt; Lock{Acquire Lock}\n\n    Lock -- Locked --&gt; Pending\n    Lock -- Acquired --&gt; Running[Phase: Running]\n\n    subgraph Execution [Restore Job]\n        Running --&gt; Job[Launch Job]\n        Job --&gt; Pull[Pull Snapshot]\n        Pull --&gt; Restore[Restore to Vault]\n    end\n\n    Restore -- Success --&gt; Completed[Phase: Completed]\n    Restore -- Error --&gt; Retrying{Retry?}\n\n    Retrying -- Yes --&gt; Job\n    Retrying -- No --&gt; Failed\n\n    classDef phase fill:transparent,stroke:#9333ea,stroke-width:2px,color:#fff;\n    classDef terminal fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef error fill:transparent,stroke:#dc2626,stroke-width:2px,color:#fff;\n\n    class Pending,Validating,Running,Execution phase;\n    class Completed terminal;\n    class Failed error;</code></pre>"},{"location":"architecture/restore-manager/#3-workflow-steps","title":"3. Workflow Steps","text":"<ol> <li>Validation:<ul> <li>Target Cluster exists.</li> <li>Snapshot source is accessible.</li> <li>No conflicting operations (unless <code>spec.force</code> is used).</li> </ul> </li> <li>Operation Lock:<ul> <li>The controller sets <code>OpenBaoCluster.status.operationLock = \"Restore\"</code>.</li> <li>This blocks the BackupManager and UpgradeManager from starting new operations.</li> </ul> </li> <li>Execution:<ul> <li>A Kubernetes Job is spawned with the <code>bao-restore</code> binary.</li> <li>It downloads the snapshot from object storage (S3/GCS/Azure).</li> <li>It uses a temporary token (or valid credentials) to authenticate and hit the <code>sys/storage/raft/snapshot-force</code> endpoint.</li> </ul> </li> <li>Completion:<ul> <li>On success, the lock is released.</li> <li>The cluster may need to be unsealed manually or via auto-unseal.</li> </ul> </li> </ol>"},{"location":"architecture/restore-manager/#4-interaction-with-other-managers","title":"4. Interaction with Other Managers","text":"<p>Conflict Prevention</p> <p>The Operation Lock is the primary mechanism for safety.</p> <ul> <li>Backups: Will skip scheduled runs if a Restore is locked.</li> <li>Upgrades: Will pause reconciliation if a Restore is locked.</li> </ul> <p>To override this check during emergencies (e.g., restoring a broken cluster where the lock is stuck), use <code>spec.overrideOperationLock</code>.</p>"},{"location":"architecture/restore-manager/#5-see-also","title":"5. See Also","text":"<ul> <li>Backup Manager</li> <li>Lifecycle Flows</li> </ul>"},{"location":"architecture/upgrade-manager/","title":"UpgradeManager (Rolling &amp; Blue/Green)","text":"<p>User Guide</p> <p>For operational instructions, see the Upgrades User Guide.</p> <p>Responsibility: Orchestrate safe version updates while maintaining Raft consensus.</p>"},{"location":"architecture/upgrade-manager/#1-upgrade-strategies","title":"1. Upgrade Strategies","text":"<p>The Manager supports two distinct strategies, controlled by <code>spec.updateStrategy</code>.</p> Rolling Update (Default)Blue/Green <p>Goal: Update pods one-by-one with minimal downtime.</p> <p>The Manager uses StatefulSet Partitioning to control the rollout.</p> <pre><code>graph TD\n    Trigger[Version Change] --&gt;|Pause| Partition[Set Partition = Replicas]\n    Partition --&gt; Loop{Partition &gt; 0?}\n\n    Loop -- Yes --&gt; Ident[Identify Leader]\n    Ident --&gt;|If Target is Leader| StepDown[Force Step-Down]\n    Ident --&gt;|If Target is Follower| Update[Decrement Partition]\n\n    StepDown --&gt; WaitTransfer[Wait for Leadership Transfer]\n    WaitTransfer --&gt; Update\n\n    Update --&gt; WaitReady[Wait for Pod Ready]\n    WaitReady --&gt; WaitHealth[Wait for Vault Health]\n    WaitHealth --&gt; Loop\n\n    Loop -- No --&gt; Done[Upgrade Complete]\n\n    classDef process fill:transparent,stroke:#9333ea,stroke-width:2px,color:#fff;\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Partition,StepDown,Update write;\n    class Trigger,Ident,WaitReady,WaitHealth read;\n    class Loop,WaitTransfer process;</code></pre> <ol> <li>Partitioning: We pause Kubernetes updates by setting <code>partition</code> equal to <code>replicas</code>.</li> <li>Reverse Ordinal: We update from highest index (e.g., 2) down to 0.</li> <li>Leader Safety: Before updating the node that is currently the Leader, we send <code>PUT /sys/step-down</code> to force a leadership transfer. This prevents the cluster from crashing during the leader's restart.</li> </ol> <p>Goal: Zero-downtime upgrades with instant rollback.</p> <p>Resource Usage</p> <p>Requires 2x Storage capacity during the transition (Blue volume + Green volume).</p> <p>This strategy spawns a parallel \"Green\" cluster and migrates data via Raft.</p> <pre><code>graph TD\n    Start((Start)) --&gt;|v1 -&gt; v2| Deploy[Deploy Green Cluster]\n\n    subgraph Preparation\n        Deploy --&gt;|Wait Ready| Join[Join Raft Mesh]\n        Join -.-&gt;|Non-Voters| Sync[Sync Data]\n        Sync --&gt;|Wait Index| Promote[Promote Green to Voters]\n    end\n\n    subgraph Critical_Section [Traffic Switch]\n        Promote --&gt; Switch[Update Service Selector]\n        Switch --&gt;|Traffic -&gt; Green| Stabilization{Stable?}\n\n        Stabilization -- No --&gt; Rollback[Rollback: Point Service to Blue]\n        Stabilization -- Yes --&gt; Demote[Demote Blue to Non-Voters]\n    end\n\n    subgraph Cleanup\n        Demote --&gt; Remove[Remove Blue from Raft]\n        Remove --&gt; Delete[Delete Blue StatefulSet]\n        Rollback --&gt; RemoveGreen[Remove Green from Raft]\n        RemoveGreen --&gt; DeleteGreen[Delete Green StatefulSet]\n    end\n\n    Delete --&gt; Done((Idle))\n    DeleteGreen --&gt; Done\n\n    classDef process fill:transparent,stroke:#9333ea,stroke-width:2px,color:#fff;\n    classDef critical fill:transparent,stroke:#dc2626,stroke-width:2px,stroke-dasharray: 5 5,color:#fff;\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n\n    class Deploy,Join,Sync,Promote,Demote,Remove,Delete,RemoveGreen,DeleteGreen process;\n    class Switch,Rollback critical;\n    class Start,Done,Stabilization write;</code></pre> <p>Key Phases:</p> Phase Description Deploying Creates the new StatefulSet. Joining Adds new pods as Non-Voters (Read-Only). Promoting Promotes new pods to Voters (Write-Access). Switching Updates Service selector to point to Green. Cleanup Removes Blue pods from the Raft peer list."},{"location":"architecture/upgrade-manager/#2-upgrade-state-machine","title":"2. Upgrade State Machine","text":""},{"location":"architecture/upgrade-manager/#resumability","title":"Resumability","text":"<p>Upgrades are designed to survive Operator restarts. All state is stored in <code>Status</code>:</p> <ul> <li>Rolling: Tracks <code>Status.Upgrade.CurrentPartition</code> and <code>CompletedPods</code>.</li> <li>Blue/Green: Tracks <code>Status.BlueGreen.Phase</code> and <code>JobFailureCount</code>.</li> </ul> <p>If the Operator crashes, it reads the Status on startup and resumes exactly where it left off.</p>"},{"location":"architecture/upgrade-manager/#image-verification","title":"Image Verification","text":"<p>If <code>spec.imageVerification.enabled</code> is <code>true</code>:</p> <ul> <li>Rolling: The main StatefulSet image is pinned to the verified digest.</li> <li>Blue/Green: The Green StatefulSet and all executor Jobs are pinned to the verified digest.</li> </ul>"},{"location":"architecture/upgrade-manager/#3-reconciliation-semantics","title":"3. Reconciliation Semantics","text":"<ul> <li>Idempotency: Re-running a phase multiple times does not cause side effects (e.g., \"Join\" checks if already joined).</li> <li>Safety: The Operator prioritizes Availability over Progress. If a health check fails, the upgrade pauses/retries indefinitely (Rolling) or triggers a rollback (Blue/Green).</li> <li>OwnerReferences: Executor jobs in Blue/Green are owned by the Cluster CR, ensuring easy cleanup.</li> </ul>"},{"location":"architecture/vap-hardening/","title":"VAP Hardening (Drift Detector Removed)","text":"<p>This document captures the rationale and the shipped changes to rely on Validating Admission Policies (VAPs) for drift prevention and remove the drift detector component.</p>"},{"location":"architecture/vap-hardening/#goals","title":"Goals","text":"<ul> <li>Block meaningful drift of OpenBao Operator-managed resources at admission time (GitOps-safe).</li> <li>Preserve compatibility with common cluster add-ons (cloud load balancers, kube controllers, GC).</li> <li>Reduce maintenance burden by removing the drift detector component.</li> <li>Keep the operator usable in both single-tenant and multi-tenant modes.</li> </ul>"},{"location":"architecture/vap-hardening/#non-goals","title":"Non-Goals","text":"<ul> <li>Guarantee drift correction latency of seconds (the drift detector's main value-add).</li> <li>Support every third-party controller mutating operator-managed objects; only Kubernetes-required   flows and widely deployed infrastructure controllers are in scope.</li> <li>Execute CEL expressions in unit tests (we rely on integration/e2e for behavioral validation).</li> </ul> <p>Note</p> <p>The drift detector only triggers on successful mutations that reach etcd. When drift is blocked at admission time, it provides little practical value beyond faster reconciliation.</p>"},{"location":"architecture/vap-hardening/#current-state-summary","title":"Current State (Summary)","text":"<ul> <li>The primary drift-blocking policy is <code>config/policy/lock-managed-resource-mutations.yaml</code>.</li> <li>This policy must be strict enough to deny meaningful drift while allowing required Kubernetes   controller behavior (e.g., Service finalizers/annotations for LoadBalancers).</li> </ul>"},{"location":"architecture/vap-hardening/#compatibility-targets","title":"Compatibility Targets","text":"<p>The policy changes must keep these scenarios working:</p> <ol> <li>Namespace deletion and garbage collection</li> <li>Kubernetes controllers must be able to delete objects and remove finalizers.</li> <li>Service and Ingress/Gateway status management</li> <li>Infrastructure controllers must be able to update <code>*/status</code>.</li> <li>LoadBalancer Service lifecycle</li> <li>Cloud provider controllers may add/remove finalizers and set certain annotations.</li> <li>Pod lifecycle and kubelet deletes</li> <li>Kubelet and controllers must be able to create/delete/update Pods as required.</li> <li>OpenBao self-observation via Pod labels</li> <li>Pods update a small set of \u201cservice registration\u201d labels used for status aggregation.</li> </ol> <p>Warning</p> <p>The most common compatibility pitfall is <code>Service</code> finalizers and controller-added annotations for <code>ServiceType=LoadBalancer</code>. These are not <code>/status</code> writes and will be blocked if not explicitly allowed.</p>"},{"location":"architecture/vap-hardening/#design-narrow-resource-specific-exceptions","title":"Design: Narrow, Resource-Specific Exceptions","text":""},{"location":"architecture/vap-hardening/#guiding-principle","title":"Guiding Principle","text":"<p>Replace broad identity-based bypasses with resource- and field-scoped allowances.</p> <p>At a high level, <code>lock-managed-resource-mutations</code> should enforce:</p> <ul> <li>Default: deny all <code>CREATE</code>/<code>UPDATE</code>/<code>DELETE</code> to operator-managed objects.</li> <li>Allow the OpenBao Operator controller identity to manage its objects.</li> <li>Allow Kubernetes system controllers only for operations they must perform (status, deletes, finalizers).</li> <li>Avoid allowing any non-operator actor to change <code>spec</code> (for resources where <code>spec</code> is meaningful).</li> </ul>"},{"location":"architecture/vap-hardening/#policy-changes-primary","title":"Policy Changes (Primary)","text":"<p>Target: <code>config/policy/lock-managed-resource-mutations.yaml</code>.</p> <ol> <li>Remove blanket bypasses</li> <li>Remove unconditional <code>variables.is_kube_system_controller</code> allow in the main validation.</li> <li> <p>Replace unconditional <code>variables.is_cert_manager</code> allow with a Secrets-only allowance.</p> </li> <li> <p>Allow narrowly-scoped Service metadata updates</p> </li> <li> <p>Permit kube-system identities to update <code>Service</code> metadata while enforcing:</p> <ul> <li><code>spec</code> is unchanged</li> <li>labels and ownerRefs are unchanged</li> <li><code>openbao.org/*</code> annotations are immutable (to avoid escaping operator intent)</li> </ul> </li> <li> <p>Keep existing safe allowances</p> </li> <li>Keep endpoint-related allowances (<code>is_endpoints_controller_request</code>) for system controllers.</li> <li>Keep Pod deletion allowance for kubelet nodes.</li> <li>Keep the pod \u201cservice registration label update\u201d allowance for cluster pods, but validate it remains      strictly label-scoped.</li> </ol>"},{"location":"architecture/vap-hardening/#changes-implemented","title":"Changes Implemented","text":"<ul> <li>Removed drift detector feature (code, CRD fields, manifests, and tests).</li> <li>Tightened <code>lock-managed-resource-mutations</code> to remove broad identity bypasses while allowing required   kube-system Service metadata updates and Secrets-only cert-manager writes.</li> </ul>"},{"location":"architecture/vap-hardening/#rollout-strategy","title":"Rollout Strategy","text":"<ul> <li>Prefer running integration/e2e in representative environments (cloud LB, bare metal, and local Kind)   to validate Service controller behavior.</li> </ul>"},{"location":"architecture/vap-hardening/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>Drift attempts against operator-managed <code>StatefulSet</code>/<code>ConfigMap</code>/<code>Service</code> are denied at admission time.</li> <li>LoadBalancer and service discovery still function in common environments.</li> <li>No drift detector is required to maintain correctness.</li> </ul>"},{"location":"architecture/vap-hardening/#local-verification-pr-equivalent","title":"Local Verification (PR-Equivalent)","text":"<pre><code>make lint-config lint\nmake verify-fmt\nmake verify-tidy\nmake verify-generated\nmake verify-helm\nmake test-ci\n</code></pre>"},{"location":"architecture/lifecycle/","title":"Lifecycle Management Overview","text":"<p>The OpenBao Operator manages the full lifecycle of OpenBao clusters, from initial provisioning (Day 0) to complex upgrades (Day 2) and disaster recovery (Day N).</p> <p>These flows describe manager-level responsibilities. In the code, <code>OpenBaoCluster</code> reconciliation is split across:</p> <ul> <li><code>openbaocluster-workload</code> (certs, infra, init)</li> <li><code>openbaocluster-adminops</code> (upgrade, backup)</li> <li><code>openbaocluster-status</code> (conditions, finalizers)</li> </ul> <ul> <li> <p> Day 0: Provisioning</p> <p>Tenant onboarding, Namespace creation, and RBAC delegation via the Provisioner Controller.</p> </li> <li> <p> Day 1: Creation</p> <p>Cluster initialization strategies (Self-Init vs Standard), PKI bootstrapping, and Unsealing.</p> </li> <li> <p> Day 2: Operations</p> <p>Details on Upgrade strategies (Rolling/Blue-Green), Maintenance Mode, and Version Management.</p> </li> <li> <p> Day N: Backups</p> <p>Backup scheduling, retention policies, and restoration workflows.</p> </li> </ul>"},{"location":"architecture/lifecycle/day0-provisioning/","title":"Day 0: Tenant Provisioning","text":"<p>Day 0 operations focus on onboarding tenants and ensuring they have the necessary permissions and resources to manage their own OpenBao clusters. This phase uses the Provisioner Controller.</p> <p>User Guide</p> <p>For practical instructions on setting up tenants, see the Tenant Onboarding Guide and Multi-Tenancy Guide.</p>"},{"location":"architecture/lifecycle/day0-provisioning/#1-namespace-rbac-creation","title":"1. Namespace &amp; RBAC Creation","text":"<ul> <li>The cluster admin creates a <code>Provisioner</code> CR (or manually configures a Namespace with appropriate RoleBindings).</li> <li>The Provisioner Controller reconciles these resources, granting limited permissions to the tenant's ServiceAccount (e.g., ability to manage <code>OpenBaoCluster</code>, <code>Secret</code>, and <code>ConfigMap</code> within their namespace).</li> </ul>"},{"location":"architecture/lifecycle/day0-provisioning/#2-resource-quotas","title":"2. Resource Quotas","text":"<ul> <li>Default <code>ResourceQuota</code> and <code>LimitRange</code> objects are applied to the tenant namespace to prevent noisy neighbor issues.</li> <li>These limits can be customized by setting <code>spec.quota</code> and <code>spec.limitRange</code> in the <code>OpenBaoTenant</code> CR.</li> </ul>"},{"location":"architecture/lifecycle/day0-provisioning/#3-tenant-onboarding","title":"3. Tenant Onboarding","text":"<ul> <li>The tenant receives their authentication credentials (e.g., Kubeconfig limited to their namespace).</li> <li>The tenant verifies access by listing resources in their namespace.</li> </ul>"},{"location":"architecture/lifecycle/day0-provisioning/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant Admin\n    participant K as Kubernetes API\n    participant Prov as Provisioner Controller\n    participant Tenant\n\n    Admin-&gt;&gt;K: Create Namespace + RoleBinding\n    K--&gt;&gt;Prov: Watch RoleBinding\n    Prov-&gt;&gt;K: Reconcile Permissions (Day 0)\n    Prov-&gt;&gt;K: Apply ResourceQuotas (optional)\n    Admin-&gt;&gt;Tenant: Hand over credentials\n    Tenant-&gt;&gt;K: Create OpenBaoCluster</code></pre>"},{"location":"architecture/lifecycle/day1-creation/","title":"Day 1: Cluster Creation","text":"<p>Day 1 involves the instantiation and initialization of the OpenBao cluster itself.</p> <p>User Guide</p> <p>For details on configuring the cluster, see the Cluster Configuration and Security Fundamentals.</p>"},{"location":"architecture/lifecycle/day1-creation/#initialization-strategies","title":"Initialization Strategies","text":"Self-Initialization (Recommended)Standard Initialization <p>When <code>spec.selfInit.enabled = true</code>, the cluster uses OpenBao's native self-initialization feature:</p> <ol> <li>User creates an <code>OpenBaoCluster</code> CR with <code>spec.selfInit</code> configured.</li> <li>Cert Manager (workload controller) bootstraps PKI (CA + leaf certs).</li> <li>If image verification is enabled, the operator verifies the container image signature before proceeding.</li> <li>Infrastructure Manager (workload controller) ensures a per-cluster auto-unseal configuration (static seal by default, or external KMS if configured).</li> <li>Infrastructure Manager renders <code>config.hcl</code> with TLS, Raft storage, <code>retry_join</code>, <code>service_registration \"kubernetes\"</code>, and the appropriate <code>seal</code> stanza.</li> <li>The init container appends the self-init <code>initialize</code> stanzas (rendered from <code>spec.selfInit.requests[]</code>) to the rendered config for pod-0 only.</li> <li>Infrastructure Manager creates the StatefulSet with 1 replica initially.</li> <li>OpenBao automatically initializes itself on first start using the <code>initialize</code> stanzas:</li> <li>Auto-unseals using the static key.</li> <li>Executes all configured <code>initialize</code> requests (audit, auth, secrets, policies).</li> <li>The root token is NOT returned and is automatically revoked after use.</li> <li>Init Manager detects initialization via Kubernetes service registration labels (preferred) and sets <code>Status.Initialized = true</code>.</li> <li>Infrastructure Manager scales the StatefulSet to the desired <code>spec.replicas</code>.</li> <li>Additional OpenBao pods start, auto-unseal, and join the Raft cluster.</li> </ol> <p>Self-Initialization vs Root Token</p> <p>Self-initialization requires an auto-unseal mechanism (which the Operator provides via static auto-unseal by default, or external KMS if configured). No root token Secret is created when self-init is enabled.</p> <p>Version Requirement</p> <p>The static auto-unseal feature requires OpenBao v2.4.0 or later. Earlier versions do not support the <code>seal \"static\"</code> configuration. External KMS seals may have different version requirements depending on the provider.</p> <p>Sequence Diagram:</p> <pre><code>sequenceDiagram\n    autonumber\n    participant U as User\n    participant K as Kubernetes API\n    participant Op as OpenBao Operator\n    participant Bao as OpenBao Pods\n\n    U-&gt;&gt;K: Create OpenBaoCluster (selfInit.enabled=true)\n    K--&gt;&gt;Op: Watch OpenBaoCluster\n    Op-&gt;&gt;K: Create TLS Secrets (CA, server)\n    Op-&gt;&gt;K: Create config ConfigMap (config.hcl)\n    Op-&gt;&gt;K: Create self-init ConfigMap (initialize blocks)\n    Op-&gt;&gt;K: Create StatefulSet (replicas=1)\n    Bao--&gt;&gt;Bao: Auto-init and run initialize requests\n    Bao--&gt;&gt;Bao: Auto-unseal using static or KMS seal\n    Bao--&gt;&gt;K: Update Pod labels (service_registration)\n    Op-&gt;&gt;K: Observe pod-0 labels (initialized/sealed)\n    Op-&gt;&gt;K: Update OpenBaoCluster.status.initialized=true\n    Op-&gt;&gt;K: Scale StatefulSet to spec.replicas\n    Bao--&gt;&gt;Op: Additional pods auto-unseal and join Raft</code></pre> <ol> <li>User creates an <code>OpenBaoCluster</code> CR in a namespace (without <code>spec.selfInit</code>).</li> <li>Cert Manager (workload controller) bootstraps PKI (CA + leaf certs).</li> <li>If image verification is enabled (<code>spec.imageVerification.enabled</code>), the operator verifies the container image signature using Cosign before proceeding.</li> <li>Infrastructure Manager (workload controller) ensures a per-cluster auto-unseal configuration:</li> <li>If <code>spec.unseal</code> is omitted or <code>spec.unseal.type</code> is <code>\"static\"</code>, creates a static auto-unseal key Secret (<code>&lt;cluster&gt;-unseal-key</code>) if missing.</li> <li>If <code>spec.unseal.type</code> is an external KMS provider (<code>awskms</code>, <code>gcpckms</code>, <code>azurekeyvault</code>, <code>transit</code>), configures the seal with the provided options and credentials (if specified).</li> <li>Infrastructure Manager renders <code>config.hcl</code> with TLS, Raft storage, <code>retry_join</code>, <code>service_registration \"kubernetes\"</code>, and the appropriate <code>seal</code> stanza (static or external KMS).</li> <li>The Operator injects <code>BAO_K8S_NAMESPACE</code> into the OpenBao container environment so service registration can determine the Pod namespace.</li> <li>Infrastructure Manager creates the StatefulSet with 1 replica initially (regardless of <code>spec.replicas</code>), mounting TLS and unseal Secrets (if using static seal) or KMS credentials (if using external KMS).</li> <li>Init Manager waits for pod-0 to be running, then:</li> <li>Prefers Kubernetes service registration Pod labels (<code>openbao-initialized</code>, <code>openbao-sealed</code>) when available.</li> <li>Falls back to the HTTP health endpoint (<code>GET /v1/sys/health</code>) when labels are not yet available.</li> <li>If not, calls the HTTP initialization endpoint (<code>PUT /v1/sys/init</code>) against pod-0 to initialize the cluster.</li> <li>Stores the root token in a per-cluster Secret (<code>&lt;cluster&gt;-root-token</code>).</li> <li>Sets <code>Status.Initialized = true</code>.</li> <li>Once initialized, Infrastructure Manager scales the StatefulSet to the desired <code>spec.replicas</code>.</li> <li>Additional OpenBao pods start, auto-unseal using the static key, join the Raft cluster via <code>retry_join</code>, and become Ready.</li> </ol> <p>Sequence Diagram:</p> <pre><code>sequenceDiagram\n    autonumber\n    participant U as User\n    participant K as Kubernetes API\n    participant Op as OpenBao Operator\n    participant Bao as OpenBao Pods\n\n    U-&gt;&gt;K: Create OpenBaoCluster (no selfInit)\n    K--&gt;&gt;Op: Watch OpenBaoCluster\n    Op-&gt;&gt;K: Create TLS Secrets (CA, server)\n    Op-&gt;&gt;K: Create config ConfigMap (config.hcl)\n    Op-&gt;&gt;K: Create StatefulSet (replicas=1)\n    Bao--&gt;&gt;K: Update Pod labels (service_registration)\n    Op-&gt;&gt;K: Observe pod-0 labels (initialized/sealed)\n    alt labels not yet available\n        Op-&gt;&gt;Bao: Call /v1/sys/health (check initialized)\n    end\n    Op-&gt;&gt;Bao: Call /v1/sys/init (initialize cluster)\n    Op-&gt;&gt;K: Store root token Secret\n    Op-&gt;&gt;K: Update OpenBaoCluster.status.initialized=true\n    Op-&gt;&gt;K: Scale StatefulSet to spec.replicas\n    Bao--&gt;&gt;Op: Pods auto-unseal and join Raft</code></pre>"},{"location":"architecture/lifecycle/day2-operations/","title":"Day 2: Operations &amp; Upgrades","text":"<p>Day 2 operations cover the ongoing management of the cluster, including version upgrades and maintenance.</p> <p>User Guide</p> <p>See the Upgrade Guide for detailed upgrade strategies (Rolling vs Blue/Green).</p>"},{"location":"architecture/lifecycle/day2-operations/#cluster-operations-upgrades","title":"Cluster Operations / Upgrades","text":""},{"location":"architecture/lifecycle/day2-operations/#cluster-operations-upgrades_1","title":"Cluster Operations / Upgrades","text":"Rolling Update (Default)Blue/Green Upgrade <ol> <li>User configures upgrade executor:</li> <li>Set <code>spec.upgrade.executorImage</code> (container image used by upgrade Jobs)</li> <li>Set <code>spec.upgrade.jwtAuthRole</code> and configure the role in OpenBao (binds to <code>&lt;cluster-name&gt;-upgrade-serviceaccount</code>, automatically created by operator)</li> <li>User updates <code>OpenBaoCluster.Spec.Version</code> and/or <code>Spec.Image</code>.</li> <li>Upgrade Manager (adminops controller) detects version drift and performs pre-upgrade validation:</li> <li>Validates semantic versioning (blocks downgrades by default).</li> <li>Verifies all pods are Ready and quorum is healthy.</li> <li>Optionally triggers a pre-upgrade backup if <code>spec.upgrade.preUpgradeSnapshot</code> is enabled.</li> <li>Upgrade Manager orchestrates Raft-aware rolling updates:</li> <li>Locks StatefulSet updates using partitioning.</li> <li>Iterates pods in reverse ordinal order.</li> <li>Runs an upgrade Job to perform leader step-down before updating the leader pod.</li> <li>Waits for pod Ready, OpenBao health, and Raft sync after each update.</li> <li>Upgrade progress is persisted in <code>Status.Upgrade</code>, allowing resumption after Operator restart.</li> <li>On completion, <code>Status.CurrentVersion</code> is updated and <code>Status.Upgrade</code> is cleared.</li> </ol> <p>Upgrade Policy</p> <p>Upgrades are designed to be safe and resumable. Downgrades are blocked by default. If an upgrade fails, it halts and sets <code>Degraded=True</code>; automated rollback is not supported. Root tokens are not used for upgrade operations.</p> <p>Blue/Green upgrades provide zero-downtime updates by creating a parallel \"Green\" standby cluster.</p> <ol> <li>Drift Detection: User updates <code>OpenBaoCluster</code> spec with a new version or image, using the Blue/Green strategy.</li> <li>Green Creation: The operator creates a new \"Green\" StatefulSet with the new version.</li> <li>Join &amp; Standby: Green pods start and join the existing \"Blue\" Raft cluster as non-voters (or voters, depending on strategy). They replicate data but do not serve traffic.</li> <li>Health Check: Operator verifies the Green cluster is healthy and fully replicated.</li> <li>Cutover: Operator updates the Service selector to point to the Green pods. Traffic switches instantly.</li> <li>Cleanup: After a verification period or manual confirmation, the old \"Blue\" StatefulSet is scaled down and terminated.</li> </ol>"},{"location":"architecture/lifecycle/day2-operations/#sequence-diagram-rolling-updates","title":"Sequence Diagram (Rolling Updates)","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant U as User\n    participant K as Kubernetes API\n    participant Op as OpenBao Operator\n    participant Bao as OpenBao Pods\n\n    U-&gt;&gt;K: Patch OpenBaoCluster.spec.version\n    K--&gt;&gt;Op: Watch OpenBaoCluster (version drift)\n    Op-&gt;&gt;Op: Validate versions, health, optional pre-upgrade backup\n    Op-&gt;&gt;K: Patch StatefulSet updateStrategy (lock with partition)\n    loop per pod (highest ordinal -&gt; 0)\n        Op-&gt;&gt;Bao: /v1/sys/health on target pod\n        alt pod is leader\n            Op-&gt;&gt;Bao: /v1/sys/step-down\n        end\n        Op-&gt;&gt;K: Decrement StatefulSet.partition to update pod\n        K--&gt;&gt;Bao: Roll new pod\n        Bao--&gt;&gt;Op: Pod Ready + OpenBao health OK\n    end\n    Op-&gt;&gt;K: Update OpenBaoCluster.status.currentVersion\n    Op-&gt;&gt;K: Clear OpenBaoCluster.status.upgrade</code></pre>"},{"location":"architecture/lifecycle/day2-operations/#sequence-diagram-bluegreen","title":"Sequence Diagram (Blue/Green)","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant U as User\n    participant K as Kubernetes API\n    participant Op as OpenBao Operator\n    participant Blue as Blue Pods (v1)\n    participant Green as Green Pods (v2)\n\n    U-&gt;&gt;K: Update Image to v2 (BlueGreen Strategy)\n    K--&gt;&gt;Op: Watch OpenBaoCluster\n    Op-&gt;&gt;K: Create Green StatefulSet (v2)\n    K--&gt;&gt;Green: Start Green Pods\n    Green-&gt;&gt;Blue: Join Raft Cluster (Standby)\n    Op-&gt;&gt;Green: Wait for Healthy\n    Op-&gt;&gt;K: Switch Service Selector to Green\n    Op-&gt;&gt;Blue: Scale Down / Terminate</code></pre>"},{"location":"architecture/lifecycle/day2-operations/#maintenance-manual-recovery","title":"Maintenance / Manual Recovery","text":"<ol> <li>User sets <code>OpenBaoCluster.Spec.Paused = true</code> to enter maintenance mode.</li> <li>All reconcilers for that cluster short-circuit and stop mutating resources, allowing manual actions (e.g., manual restore from snapshot).</li> <li>If an upgrade was in progress, it is paused but state is preserved in <code>Status.Upgrade</code>.</li> <li>After maintenance, user sets <code>Paused = false</code> to resume normal reconciliation (including any paused upgrade).</li> </ol>"},{"location":"architecture/lifecycle/dayN-backups/","title":"Day N: Backups &amp; Disaster Recovery","text":"<p>Day N operations ensure data durability through regular backups and disaster recovery procedures.</p> <p>User Guide</p> <p>See Backups and Restore for configuration examples.</p>"},{"location":"architecture/lifecycle/dayN-backups/#backups","title":"Backups","text":"<ol> <li>User configures backup schedule (<code>spec.backup.schedule</code>) and target object storage in the <code>OpenBaoCluster</code> spec.</li> <li>User configures authentication method:</li> <li>JWT Auth (Preferred): Set <code>spec.backup.jwtAuthRole</code> and configure the role in OpenBao</li> <li>Static Token (Fallback): For all clusters, set <code>spec.backup.tokenSecretRef</code> pointing to a backup token Secret (root tokens are not used)</li> <li>Backup Manager (adminops controller) schedules backups using cron expressions (e.g., <code>\"0 3 * * *\"</code> for daily at 3 AM).</li> <li>On schedule, Backup Manager:</li> <li>Creates a Kubernetes Job with the backup executor container</li> <li>Job uses <code>&lt;cluster-name&gt;-backup-serviceaccount</code> (automatically created by operator)</li> <li>Backup executor:<ul> <li>Authenticates to OpenBao using JWT Auth (via projected ServiceAccount token) or static token</li> <li>Discovers the current Raft leader via OpenBao API</li> <li>Streams <code>GET /v1/sys/storage/raft/snapshot</code> directly to object storage (no disk buffering)</li> <li>Names backups predictably: <code>&lt;prefix&gt;/&lt;namespace&gt;/&lt;cluster&gt;/&lt;timestamp&gt;-&lt;uuid&gt;.snap</code></li> <li>Verifies upload completion</li> </ul> </li> <li>Backup status is recorded in <code>Status.Backup</code>:</li> <li><code>LastBackupTime</code>, <code>NextScheduledBackup</code> for visibility</li> <li><code>ConsecutiveFailures</code> for alerting</li> <li>Optional retention policies (<code>spec.backup.retention</code>) automatically delete old backups:</li> <li><code>MaxCount</code>: Keep only the N most recent backups</li> <li><code>MaxAge</code>: Delete backups older than a specified duration</li> </ol> <p>Backup Limitations</p> <p>Backups are skipped during upgrades to avoid inconsistent snapshots. Backups are optional for all clusters. If backups are enabled, either <code>jwtAuthRole</code> or <code>tokenSecretRef</code> must be configured. Root tokens are not used for backup operations.</p>"},{"location":"architecture/lifecycle/dayN-backups/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant U as User\n    participant K as Kubernetes API\n    participant Op as OpenBao Operator\n    participant Job as Backup Job Pod\n    participant Bao as OpenBao API\n    participant S3 as Object Storage\n\n    U-&gt;&gt;K: Configure backup schedule and target in OpenBaoCluster\n    K--&gt;&gt;Op: Watch OpenBaoCluster (backup spec)\n    Op-&gt;&gt;Op: Schedule backup via cron\n    Op-&gt;&gt;K: Create Job/&lt;cluster&gt;-backup\n    K--&gt;&gt;Job: Start backup executor Pod\n    Job-&gt;&gt;Bao: Authenticate (JWT or token)\n    Job-&gt;&gt;Bao: GET /v1/sys/storage/raft/snapshot\n    Job-&gt;&gt;S3: Stream snapshot to object storage\n    Job--&gt;&gt;Op: Exit status (success/failure)\n    Op-&gt;&gt;K: Update OpenBaoCluster.status.backup (last backup, failures)\n    Op-&gt;&gt;S3: Apply retention policies (via backup manager, if configured)</code></pre>"},{"location":"contributing/","title":"Contributing to OpenBao Operator","text":"<p>Welcome! We are glad you want to contribute to the OpenBao Operator. Whether you're fixing a bug, adding a feature, or improving documentation, this guide will help you get started.</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<ul> <li> <p> Getting Started</p> <p>New to the project? Start here to set up your environment and make your first contribution.</p> <p> Start Here</p> </li> <li> <p> Coding Standards</p> <p>Review our conventions for Go code, Kubernetes patterns, and error handling.</p> <p> View Standards</p> </li> <li> <p> Testing Guide</p> <p>Learn how to run our unit tests, integration tests, and E2E suites.</p> <p> Run Tests</p> </li> </ul>"},{"location":"contributing/#ai-assisted-contributions","title":"AI-Assisted Contributions","text":"<p>Policy on AI Tools</p> <p>We welcome contributions that leverage AI tools! However, you must verify all generated code.</p> <ul> <li>Responsibility: You are responsible for the code you submit. \"The AI wrote it\" is not a valid excuse for bugs or security issues.</li> <li>Context: Use our <code>.agent/rules/</code> to ensure your AI assistant follows our project conventions.</li> <li>Quality: PRs that appear to be low-effort AI dumps (\"slop PRs\") without proper understanding or testing will be closed.</li> </ul>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<p>Follow this checklist to ensure your Pull Request is ready for review:</p> <ol> <li>Code: Conform to the Coding Standards.</li> <li>Tests: Add unit tests for logic and E2E tests for features.</li> <li> <p>Validation: Run the local verification suite:</p> <pre><code># 1. Linting &amp; Formatting\nmake lint-config lint verify-fmt verify-tidy\n\n# 2. Generated Artifacts\nmake verify-generated verify-helm\n\n# 3. Tests\nmake test-ci\n</code></pre> </li> <li> <p>Commits: Write clear, descriptive commit messages.</p> </li> <li>Documentation: Update relevant docs if behavior changed.</li> </ol>"},{"location":"contributing/#documentation-guides","title":"Documentation Guides","text":"<ul> <li> <p>CI/CD Pipeline</p> <p>Understanding our GitHub Actions workflows.</p> <p> CI Guide</p> </li> <li> <p>Release Management</p> <p>How we version and release the operator.</p> <p> Release Process</p> </li> <li> <p>SDLC Overview</p> <p>Our development lifecycle and philosophy.</p> <p> Read SDLC</p> </li> <li> <p>Docs Style Guide</p> <p>Writing documentation for the project.</p> <p> Style Guide</p> </li> </ul>"},{"location":"contributing/#quick-links","title":"Quick Links","text":"<ul> <li>Go Style \u2014 Formatting, naming, idioms</li> <li>Error Handling \u2014 Error patterns</li> <li>Kubernetes Patterns \u2014 Operator best practices</li> <li>Security Practices \u2014 Secure coding</li> <li>Project Conventions \u2014 Metrics, logging, testing</li> </ul>"},{"location":"contributing/ci/","title":"Continuous Integration","text":"<p>We use GitHub Actions for all CI checks. The pipeline is designed to be deterministic and reproducible locally.</p>"},{"location":"contributing/ci/#1-ci-pipeline","title":"1. CI Pipeline","text":"<p>The pipeline runs on every PR and <code>main</code> push.</p> <pre><code>graph TD\n    PR([PR Created]) --&gt; Static\n    PR --&gt; Build\n    PR --&gt; Unit\n\n    subgraph Static [Static Analysis]\n        Lint[Lint &amp; Tidy]\n        Gen[Verify Generated]\n        Helm[Verify Helm]\n        Sec[Security Scan]\n    end\n\n    subgraph Build [Build Artifacts]\n        Docs[Build Docs]\n        Chart[Lint Chart]\n    end\n\n    subgraph Unit [Verification]\n        Sanity[Unit Tests]\n        Compat[OpenBao Compat]\n    end\n\n    Static --&gt; E2E{E2E Tests}\n    Build --&gt; E2E\n    Unit --&gt; E2E\n\n    E2E --&gt; Smoke[\"Smoke Tests\"]\n    E2E --&gt; Full[\"Full Matrix (Nightly)\"]\n\n    classDef process fill:transparent,stroke:#9333ea,stroke-width:2px;\n    classDef check fill:transparent,stroke:#60a5fa,stroke-width:2px;\n\n    class PR process;\n    class Lint,Gen,Helm,Sec,Docs,Chart,Sanity,Compat,Smoke,Full check;</code></pre>"},{"location":"contributing/ci/#2-ci-vs-local-commands-the-rosetta-stone","title":"2. CI vs Local Commands (\"The Rosetta Stone\")","text":"<p>Run these locally to debug CI failures.</p> CI Job Local Command Description Lint Check <code>make lint</code> Runs <code>golangci-lint</code> Formatting <code>make verify-fmt</code> Checks <code>gofmt</code> compliance Dependencies <code>make verify-tidy</code> Ensures <code>go.mod</code> is clean Generators <code>make verify-generated</code> Checks for drift in CRDs/RBAC Helm Sync <code>make verify-helm</code> Checks drift in <code>charts/</code> Unit Tests <code>make test-ci</code> Runs unit + integration tests Compatibility <code>make verify-openbao-config-compat</code> Checks HCL against upstream OpenBao"},{"location":"contributing/ci/#3-end-to-end-testing","title":"3. End-to-End Testing","text":"<p>We use Kind for E2E tests.</p>"},{"location":"contributing/ci/#prerequisites","title":"Prerequisites","text":"<ul> <li> Docker running</li> <li> <code>kubectl</code> installed</li> <li> 4 CPU / 8GB RAM recommended</li> </ul>"},{"location":"contributing/ci/#running-tests","title":"Running Tests","text":"Smoke Test (Fast)\ud83e\uddea Full Suite (Thorough)\ud83d\udc1b Debug Mode <p>Runs a subset of critical tests. Best for quick feedback.</p> <pre><code>make test-e2e-ci \\\n  KIND_NODE_IMAGE=kindest/node:v1.34.3 \\\n  E2E_LABEL_FILTER=smoke \\\n  E2E_PARALLEL_NODES=1\n</code></pre> <p>Runs the entire test matrix (Upgrade, Backup, Restore, etc).</p> <pre><code>make test-e2e-ci KIND_NODE_IMAGE=kindest/node:v1.34.3\n</code></pre> <p>Keeps the cluster alive after failure for inspection.</p> <pre><code>make test-e2e-ci E2E_SKIP_CLEANUP=true\n</code></pre>"},{"location":"contributing/ci/#4-security-checks","title":"4. Security Checks","text":"<p>We run vulnerability scanning on every PR.</p> GovulncheckTrivy <p>Detects known vulnerabilities in Go dependencies.</p> <pre><code>go install golang.org/x/vuln/cmd/govulncheck@latest\ngovulncheck -test ./...\n</code></pre> <p>Scans the operator image for OS vulnerabilities.</p> <pre><code>make security-scan IMG=ghcr.io/dc-tec/openbao-operator:latest\n</code></pre> <p>Expected RBAC findings (skipped in Trivy FS)</p> <p>Trivy's Kubernetes misconfiguration rules flag several intentionally privileged RBAC manifests/templates (e.g. tenant template roles, single-tenant mode, and provisioner cleanup permissions). We skip these specific files in CI and in <code>make security-scan</code> using Trivy's <code>--skip-files</code> flags.</p> <p>If you modify RBAC under <code>config/rbac/</code>, <code>dist/install.yaml</code>, or the chart RBAC templates, and Trivy starts failing, update the skip list in:</p> <ul> <li><code>.github/workflows/ci.yml</code> (Trivy FS step)</li> <li><code>.github/workflows/nightly.yml</code> (Trivy FS step)</li> <li><code>Makefile</code> (<code>security-scan</code> target)</li> </ul>"},{"location":"contributing/ci/#5-documentation-build","title":"5. Documentation Build","text":"<p>Docs are built with MkDocs and Material.</p> <pre><code># Local preview\nmake docs-serve\n\n# Build distribution (checks internal links)\nmake docs-build\n</code></pre> <p>Preview Deployment</p> <p>Every PR deploys a temporary preview environment URL directly in the GitHub comment.</p>"},{"location":"contributing/docs-style-guide/","title":"OpenBao Operator Documentation Style Guide","text":"<p>This guide ensures a consistent voice, structure, and appearance across all OpenBao Operator documentation.</p>"},{"location":"contributing/docs-style-guide/#1-general-principles","title":"1. General Principles","text":"<ul> <li>Voice: Professional, concise, and direct. Use the imperative mood (e.g., \"Run the command\").</li> <li>Audience: DevOps engineers, SREs, and Platform engineers.</li> <li>Terminology:<ul> <li>OpenBao Operator (capitalized).</li> <li>OpenBao (software).</li> <li>Kubernetes or K8s.</li> </ul> </li> </ul>"},{"location":"contributing/docs-style-guide/#2-structure-layout","title":"2. Structure &amp; Layout","text":""},{"location":"contributing/docs-style-guide/#21-grids-cards","title":"2.1 Grids &amp; Cards","text":"<p>Use <code>grid</code> layouts for landing pages or grouping related navigation items.</p> <ul> <li> <p> Quick Start</p> <p>Get up and running in minutes.</p> <p> Start</p> </li> <li> <p> Concepts</p> <p>Deep dive into architecture.</p> <p> Learn</p> </li> </ul>"},{"location":"contributing/docs-style-guide/#22-content-tabs","title":"2.2 Content Tabs","text":"<p>Use tabs to group alternative configurations (e.g., Helm vs YAML) or operating modes.</p> HelmYAML <pre><code>helm install openbao openbao/openbao\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\n</code></pre>"},{"location":"contributing/docs-style-guide/#3-advanced-components","title":"3. Advanced Components","text":""},{"location":"contributing/docs-style-guide/#31-admonitions","title":"3.1 Admonitions","text":"<p>Use the correct type for the severity of the message.</p> Type Usage <code>!!! note</code> General context or \"good to know\". <code>!!! tip</code> Shortcuts or best practices. <code>!!! success</code> Confirming an action or \"Best Practice\". <code>!!! warning</code> Potential pitfalls (recoverable). <code>!!! failure</code> Operational failures (troubleshooting). <code>!!! danger</code> Irreversible data loss or security risks."},{"location":"contributing/docs-style-guide/#32-annotations","title":"3.2 Annotations","text":"<p>Add callouts to code blocks to explain specific lines.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret # (1)!\n</code></pre> <ol> <li>This name must match the <code>OpenBaoCluster</code> reference.</li> </ol>"},{"location":"contributing/docs-style-guide/#33-icons-buttons","title":"3.3 Icons &amp; Buttons","text":"<ul> <li>Icons: Use Material Design icons: <code>:material-check-circle:</code>.</li> <li>Buttons:     [Click Me]{ .md-button .md-button--primary }</li> </ul>"},{"location":"contributing/docs-style-guide/#4-diagrams-mermaid","title":"4. Diagrams (Mermaid)","text":"<p>All diagrams must be Theme Neutral (work in Light and Dark mode).</p>"},{"location":"contributing/docs-style-guide/#41-color-palette","title":"4.1 Color Palette","text":"<p>Use <code>fill:transparent</code> generally, and distinct stroke colors for semantic meaning.</p> Class Meaning Stroke Color CSS <code>write</code> Create, Update, Success Green <code>stroke:#22c55e</code> <code>read</code> Read, Observe, Info Blue <code>stroke:#60a5fa</code> <code>security</code> RBAC, Policies, Danger Red <code>stroke:#dc2626</code> <code>process</code> Jobs, Workflows Purple <code>stroke:#9333ea</code> <code>git</code> GitOps, Source control Pink <code>stroke:#f472b6</code>"},{"location":"contributing/docs-style-guide/#42-standard-definition-block","title":"4.2 Standard Definition Block","text":"<p>Include this at the end of every Mermaid diagram:</p> <pre><code>graph TD\n    A --&gt; B\n\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n    classDef security fill:transparent,stroke:#dc2626,stroke-width:2px,color:#fff;\n\n    class A write;\n    class B read;</code></pre>"},{"location":"contributing/docs-style-guide/#5-user-guide-structure","title":"5. User Guide Structure","text":"<ol> <li>Title: Noun-based (e.g., \"Backup Configuration\").</li> <li>Introduction: What does this feature do?</li> <li>Prerequisites: What is needed?</li> <li>Configuration: YAML examples.</li> <li>Operations: How to use/monitor.</li> </ol>"},{"location":"contributing/docs-style-guide/#6-previewing-docs","title":"6. Previewing Docs","text":"<pre><code>make docs-serve # http://localhost:8000\nmake docs-build # ./site/\n</code></pre>"},{"location":"contributing/release-management/","title":"Release Management","text":"<p>We follow a strict \"Build Once, Promote Everywhere\" philosophy. Releases are automated, signed, and provenanced.</p>"},{"location":"contributing/release-management/#1-release-architecture","title":"1. Release Architecture","text":"<p>Our pipeline ensures that the artifacts we test in E2E are the exact same bits that are published (bit-for-bit identical).</p> <pre><code>graph TD\n    Start([Manual Trigger]) --&gt; Build\n\n    subgraph Build [Build Phase]\n        Img[Build Images]\n        Tag[Tag: build-sha]\n        Scan[Security Scan]\n    end\n\n    subgraph Gate [Quality Gate]\n        Vuln[Govulncheck]\n        Test[E2E Matrix]\n        Compat[Config Compat]\n    end\n\n    subgraph Promote [Promotion Phase]\n        Retag[Retag by Digest]\n        Sign[Cosign Signing]\n        Chart[Package OCI Chart]\n        SBOM[Generate SBOM]\n    end\n\n    subgraph Publish [Public Release]\n        Git[Git Tag vX.Y.Z]\n        GH[GitHub Release]\n        Asset[Upload Assets]\n    end\n\n    Build --&gt; Gate\n    Gate --&gt; Promote\n    Promote --&gt; Publish\n\n    classDef phase fill:transparent,stroke:#9333ea,stroke-width:2px;\n    classDef step fill:transparent,stroke:#60a5fa,stroke-width:2px;\n\n    class Build,Promote,Publish phase;\n    class Img,Tag,Scan,Vuln,Test,Compat,Retag,Sign,Chart,SBOM,Git,GH,Asset step;</code></pre>"},{"location":"contributing/release-management/#2-release-checklist","title":"2. Release Checklist","text":"<p>For Release Managers.</p>"},{"location":"contributing/release-management/#pre-flight-checks","title":"Pre-Flight Checks","text":"<ul> <li> Changelog: Regenerate <code>CHANGELOG.md</code> locally from git history:</li> <li><code>make changelog</code> (Unreleased since last tag)</li> <li><code>make changelog-all</code> (all tags + Unreleased)</li> <li> Docs: Ensure documentation is consistent with new features.</li> <li> Compatibility: Verify <code>docs/reference/compatibility.md</code> covers the supported versions.</li> <li> Clean CI: Ensure the latest commit on main is green.</li> </ul>"},{"location":"contributing/release-management/#triggering-the-release","title":"Triggering the Release","text":"<ol> <li>Go to Actions -&gt; Release Workflow.</li> <li>Click Run workflow.</li> <li>Version: Enter the semver tag (e.g., <code>v0.1.0</code>).</li> <li>Ref: Leave blank for <code>main</code>.</li> </ol>"},{"location":"contributing/release-management/#post-release","title":"Post-Release","text":"<ul> <li> Verify: Check that the GitHub Release exists and assets are valid.</li> <li> Announce: Post in relevant community channels.</li> </ul>"},{"location":"contributing/release-management/#3-verifying-artifacts","title":"3. Verifying Artifacts","text":"<p>All artifacts are signed using Sigstore (Keyless).</p>  Verify Image Signature Verify Attestation Verify Helm Chart <p>Using <code>cosign</code> to verify the image was built by our release workflow.</p> <pre><code>cosign verify \\\n  --certificate-identity-regexp \"https://github.com/dc-tec/openbao-operator/.github/workflows/release.yml\" \\\n  --certificate-oidc-issuer \"https://token.actions.githubusercontent.com\" \\\n  ghcr.io/dc-tec/openbao-operator:v0.1.0\n</code></pre> <p>Using GitHub CLI to verify build provenance.</p> <pre><code>gh attestation verify \\\n  oci://ghcr.io/dc-tec/openbao-operator:v0.1.0 \\\n  --owner dc-tec\n</code></pre> <p>Verify the OCI Helm Chart signature.</p> <pre><code>cosign verify \\\n  --certificate-identity-regexp \"https://github.com/dc-tec/openbao-operator/.github/workflows/release.yml\" \\\n  --certificate-oidc-issuer \"https://token.actions.githubusercontent.com\" \\\n  ghcr.io/dc-tec/charts/openbao-operator:0.1.0\n</code></pre>"},{"location":"contributing/release-management/#4-helm-chart-maintenance","title":"4. Helm Chart Maintenance","text":"<p>The Helm chart is a first-class citizen but heavily automated.</p> <p>Generated Sources</p> <p>The chart lives in <code>charts/openbao-operator</code>. However, CRDs and the Installer Template are generated from source.</p>"},{"location":"contributing/release-management/#updating-the-chart","title":"Updating the Chart","text":"<p>If you change <code>api/</code>, <code>config/</code>, or <code>dist/install.yaml</code>, you must sync the chart.</p> <pre><code># Syncs CRDs and templates from upstream sources\nmake helm-sync\n</code></pre>"},{"location":"contributing/release-management/#validation","title":"Validation","text":"<p>CI ensures the chart is always in sync.</p> <pre><code>make verify-helm\n</code></pre>"},{"location":"contributing/sdlc/","title":"Software Development Lifecycle (SDLC)","text":"<p>The OpenBao Operator follows a secure-by-default SDLC, integrating security checks, automated verification, and provenance at every stage.</p>"},{"location":"contributing/sdlc/#1-lifecycle-overview","title":"1. Lifecycle Overview","text":"<pre><code>graph TD\n    Plan --&gt; Design\n    Design --&gt; Code\n    Code --&gt; Secure\n    Secure --&gt; Verify\n    Verify --&gt; Release\n    Release --&gt; Deploy\n    Deploy --&gt; Operate\n    Operate --&gt; Plan\n\n    classDef phase fill:transparent,stroke:#9333ea,stroke-width:2px;\n    class Plan,Design,Code,Secure,Verify,Release,Deploy,Operate phase;</code></pre>"},{"location":"contributing/sdlc/#2-phase-detail","title":"2. Phase Detail","text":"<p>The lifecycle maps directly to our documentation and toolchain.</p> <ul> <li> <p> Plan &amp; Design</p> <p>Define requirements and architecture.</p> <ul> <li> Compatibility Policy</li> <li> Architecture Overview</li> <li> Components</li> </ul> </li> <li> <p> Code &amp; Implement</p> <p>Write code adhering to strict standards.</p> <ul> <li> Coding Standards</li> <li> Dev Setup</li> <li> Project Conventions</li> </ul> </li> <li> <p> Secure &amp; Verify</p> <p>Automated gates ensure quality and safety.</p> <ul> <li> Security Practices</li> <li> CI Pipeline</li> <li> Threat Model</li> </ul> </li> <li> <p> Release &amp; Deploy</p> <p>Build once, sign, and promote.</p> <ul> <li> Release Process</li> <li> Artifact Verification</li> <li> Helm Charts</li> </ul> </li> <li> <p> Operate &amp; Monitor</p> <p>Run reliably in production.</p> <ul> <li> User Guides</li> <li> Security Posture</li> <li> Troubleshooting</li> </ul> </li> </ul>"},{"location":"contributing/sdlc/#3-secure-by-design","title":"3. Secure by Design","text":"<p>Security is not a separate phase; it is injected into every step of the process.</p> Phase Tooling Check Code <code>golangci-lint</code> Static analysis for bugs and style Deps <code>dependabot</code> Automated dependency updates Verify <code>govulncheck</code> Known vulnerability scanning Build <code>trivy</code> Container filesystem scanning Release <code>cosign</code> Keyless signing of images and charts Publish <code>gh attestation</code> Build provenance trails"},{"location":"contributing/testing/","title":"Testing Strategy","text":"<p>To ensure correctness and avoid regressions, we adopt a Layered Testing Strategy. We trade off speed and isolation (Unit) for fidelity and coverage (E2E).</p> <pre><code>graph BT\n    Unit[\"Unit Tests (Go)&lt;br/&gt;&lt;i&gt;Business Logic, HCL Gen&lt;/i&gt;\"]\n    Integration[\"Integration (EnvTest)&lt;br/&gt;&lt;i&gt;Controller Logic, K8s API&lt;/i&gt;\"]\n    E2E[\"End-to-End (Kind)&lt;br/&gt;&lt;i&gt;Full Lifecycle, Upgrades&lt;/i&gt;\"]\n    Manual[\"Manual / Exploratory&lt;br/&gt;&lt;i&gt;Chaos, DR, Hardware Failures&lt;/i&gt;\"]\n\n    Unit --&gt; Integration\n    Integration --&gt; E2E\n    E2E --&gt; Manual\n\n    classDef manual fill:#f472b6,stroke:#831843,stroke-width:2px,color:white\n    classDef e2e fill:#a78bfa,stroke:#4c1d95,stroke-width:2px,color:white\n    classDef integration fill:#60a5fa,stroke:#1e3a8a,stroke-width:2px,color:white\n    classDef unit fill:#22c55e,stroke:#14532d,stroke-width:2px,color:white\n\n    class Manual manual\n    class E2E e2e\n    class Integration integration\n    class Unit unit</code></pre>"},{"location":"contributing/testing/#quick-start","title":"Quick Start","text":"<ul> <li> <p> Unit Tests</p> <p>Fast, in-process logic checks.</p> <p><code>make test</code></p> </li> <li> <p> Integration</p> <p>Controller logic against real K8s API.</p> <p><code>make test-integration</code></p> </li> <li> <p> End-to-End</p> <p>Full cluster lifecycle in Kind.</p> <p><code>make test-e2e</code></p> </li> <li> <p> Manual</p> <p>Disaster recovery &amp; chaos.</p> <p>See Scenarios</p> </li> </ul>"},{"location":"contributing/testing/#test-layers","title":"Test Layers","text":"1. Unit Tests 2. Integration (EnvTest) 3. End-to-End (E2E) 4. Manual / Exploratory <p>Unit tests focus on deterministic, in-process logic with no external I/O.</p> <p>Targets:</p> <ul> <li>HCL Config Generation (<code>internal/config</code>)</li> <li>PKI Helpers (Cert rotation math)</li> <li>State Machine Logic (Upgrade paths)</li> </ul> <p>Integration tests validate reconciliation behavior using <code>controller-runtime</code>'s <code>envtest</code>.</p> <p>Environment:</p> <ul> <li>Local <code>etcd</code> and <code>kube-apiserver</code> (managed by <code>setup-envtest</code>).</li> <li>No container runtime (no Pods are scheduled).</li> <li>Controllers run as goroutines in the test process.</li> </ul> <p>Scope:</p> <ul> <li>CRUD Operations: Ensuring Objects are created/updated correctly.</li> <li>Status Updates: Verifying <code>Status.Phase</code> transitions.</li> <li>Finalizers: Testing deletion interception.</li> <li>Admission Policies: Verifying VAP-enforced invariants and safety checks.</li> </ul> <p>Speed vs Fidelity</p> <p>Since no Pods run, you cannot test network connectivity, volume mounting, or OpenBao startup. Use E2E tests for those.</p> <p>E2E tests validate real-system behavior in a <code>kind</code> cluster.</p> <p>Environment:</p> <ul> <li>Kind Cluster: Runs actual Kubernetes nodes in Docker.</li> <li>Real Images: Builds and loads the Operator and OpenBao images.</li> <li>Helpers: automated <code>backup-executor</code>, <code>upgrade-executor</code>.</li> </ul> <p>Core Scenarios:</p> <ol> <li>Lifecycle: Provision -&gt; Init -&gt; Scale -&gt; Delete.</li> <li>Upgrades: Rolling Updates (with pre-upgrade backups) and Blue/Green.</li> <li>Backups: Streaming Raft snapshots to MinIO.</li> <li>Security: RBAC and admission policy enforcement.</li> <li>Multi-Tenancy: Namespace isolation verification.</li> </ol> <p>Filtering Tests:</p> <p>Use <code>E2E_LABEL_FILTER</code> to focus on specific domains.</p> <pre><code># Run only critical smoke tests\nmake test-e2e E2E_LABEL_FILTER='smoke &amp;&amp; critical'\n\n# Run only upgrade scenarios\nmake test-e2e E2E_LABEL_FILTER='upgrade'\n</code></pre> <p>Some scenarios are difficult or impossible to automate reliably in CI. These must be tested manually before major releases (Minor/Major versions).</p> <p>Scenarios:</p> Category Scenario Procedure Infrastructure Node Failure Hard power-off a Kind node (<code>docker stop &lt;container&gt;</code>) and verify Pod rescheduling and Raft recovery. Consensus Split Brain Isolate network traffic between nodes using <code>iptables</code> and verify leader election behavior. Data Safety Corrupt Storage Manually corrupt <code>raft.db</code> on a PVC and verify the operator does not nuke the cluster. Disaster Recovery Region Failover (If applicable) Simulate region loss by deleting all resources in one zone. Operations Manual Unseal Boot a cluster in <code>mode: \"manual\"</code> and perform the Shamir unseal flow by hand. <p>Production Safety</p> <p>Never run exploratory chaos tests against a production cluster. Use a dedicated staging environment.</p>"},{"location":"contributing/testing/#table-driven-pattern","title":"Table-Driven Pattern","text":"<p>We strictly enforce table-driven tests for business logic.</p> <pre><code>func TestRenderConfig(t *testing.T) {\n    tests := []struct {\n        name      string\n        spec      OpenBaoClusterSpec\n        wantErr   bool\n        wantMatch string\n    }{\n        {\n            name:      \"minimal config\",\n            spec:      minimalSpec(),\n            wantErr:   false,\n            wantMatch: `listener \"tcp\"`,\n        },\n        {\n            name:    \"protected stanza override rejected\",\n            spec:    specWithForbiddenConfig(),\n            wantErr: true,\n        },\n    }\n\n    for _, tt := range tests {\n        t.Run(tt.name, func(t *testing.T) {\n            // ... assertion logic ...\n        })\n    }\n}\n</code></pre>"},{"location":"contributing/testing/#golden-files","title":"Golden Files","text":"<p>For complex output like HCL generation, we use Golden Files.</p> <ul> <li>Location: <code>internal/config/testdata/</code></li> <li>Update Command: <code>make test-update-golden</code></li> </ul> <p>Golden File Review</p> <p>Always carefully review the <code>git diff</code> of golden files. They represent the exact configuration that will be deployed to user clusters.</p>"},{"location":"contributing/testing/#code-quality-standards","title":"Code Quality Standards","text":"<p>We enforce strict quality gates via <code>make lint</code>.</p> <ul> <li> <p>GolangCI-Lint</p> <p>Static analysis for bugs and anti-patterns.</p> <p><code>make lint</code></p> </li> <li> <p>Formatting</p> <p>Standard <code>gofmt</code> style.</p> <p><code>make fmt</code></p> </li> <li> <p>Code Generation</p> <p>DeepCopy and CRD manifest generation.</p> <p><code>make generate manifests</code></p> </li> </ul>"},{"location":"contributing/getting-started/","title":"Getting Started with Contributing","text":"<p>Welcome! We are excited that you are interested in contributing to the OpenBao Operator. This section will guide you through setting up your environment and making your first contribution.</p>"},{"location":"contributing/getting-started/#quick-start","title":"Quick Start","text":"<ul> <li> <p> 1. Fork &amp; Clone</p> <p>Fork the repository on GitHub and clone it locally to start working.</p> <pre><code>git clone https://github.com/YOUR-USERNAME/openbao-operator.git\ncd openbao-operator\n</code></pre> </li> <li> <p> 2. Setup Environment</p> <p>Install the necessary tools (Go, Docker, Kind) to run the operator locally.</p> <p> Setup Guide</p> </li> <li> <p> 3. Create Branch</p> <p>Create a new feature branch from <code>main</code> for your changes.</p> <pre><code>git checkour -b feature/my-awesome-feature\n</code></pre> </li> <li> <p> 4. Test &amp; Submit</p> <p>Run the test suite and open a Pull Request.</p> <p> Testing Guide</p> </li> </ul>"},{"location":"contributing/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following tools installed before starting:</p> <ul> <li> <p> Go 1.25+</p> <p>The core language for the operator.</p> </li> <li> <p> Docker</p> <p>Required for building container images.</p> </li> <li> <p> Kind</p> <p>(Kubernetes in Docker) for running local E2E tests.</p> </li> <li> <p> kubectl</p> <p>CLI for interacting with the cluster.</p> </li> </ul>"},{"location":"contributing/getting-started/#ways-to-contribute","title":"Ways to Contribute","text":"<p>We welcome many types of contributions beyond just code:</p> Type Description Bug Fixes Fix issues found in existing functionality. Check for the <code>bug</code> label. Features Add new capabilities to the operator. Discuss large changes in an Issue first. Documentation Improve guides, fix typos, or add examples to help other users. Tests Improve test coverage or add new E2E scenarios. Refactoring Improve code quality and maintainability without changing behavior. <p>First Time?</p> <p>Look for issues labeled with <code>good first issue</code> on our GitHub Issues page.</p>"},{"location":"contributing/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Development Setup</p> <p>Step-by-step guide to configuring your local dev environment.</p> <p> Go to Setup</p> </li> <li> <p>Coding Standards</p> <p>Learn about our code style, linting rules, and conventions.</p> <p> View Standards</p> </li> </ul>"},{"location":"contributing/getting-started/development/","title":"Setting Up Your Environment","text":"<p>This guide covers everything you need to build, run, and test the OpenBao Operator locally.</p>"},{"location":"contributing/getting-started/development/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed before starting:</p> <p>Required Tools</p> <ul> <li> Go v1.25.5+ (<code>go version</code>)</li> <li> Docker v28.3.3+ (<code>docker version</code>)</li> <li> kubectl v1.33+ (<code>kubectl version --client</code>)</li> <li> Kubernetes Cluster v1.33+ (Kind, Minikube, or Cloud)</li> </ul> <p>Optional but Recommended</p> <ul> <li> Kind for running local E2E tests (<code>kind version</code>)</li> <li> k9s for inspecting the cluster state</li> <li> golangci-lint for local linting</li> </ul>"},{"location":"contributing/getting-started/development/#development-workflow","title":"Development Workflow","text":"<p>We support two main development workflows. Choose the one that fits your current task.</p>  Local Development (Fast Loop) Cluster Deployment (Integration Loop) <p>Best for rapid iteration on logic. The operator runs as a generic Go binary on your laptop and connects to the cluster via your kubeconfig.</p> <ol> <li> <p>Clone the Repo: <pre><code>git clone https://github.com/dc-tec/openbao-operator.git\ncd openbao-operator\n</code></pre></p> </li> <li> <p>Install CRDs:     Apply the Custom Resource Definitions to your target cluster.     <pre><code>make install\n</code></pre></p> </li> <li> <p>Run Operator:     Start the controller locally. It will use your <code>~/.kube/config</code>.     <pre><code>make run\n</code></pre></p> </li> </ol> <p>Limitations</p> <ul> <li>Webhooks may not work locally without tunneling (ngrok).</li> <li>NetworkPolicies cannot be tested this way.</li> </ul> <p>Best for testing full lifecycle, webhooks, and RBAC permissions. The operator runs as a Pod inside the cluster.</p> <ol> <li> <p>Start Kind Cluster: <pre><code>kind create cluster --name openbao-dev\n</code></pre></p> </li> <li> <p>Build &amp; Load Image:     Build the docker image and load it directly into Kind (no registry needed).     <pre><code>make docker-build IMG=openbao-operator:dev\nkind load docker-image openbao-operator:dev --name openbao-dev\n</code></pre></p> </li> <li> <p>Deploy:     Install CRDs and deploy the operator manifests.     <pre><code>make deploy IMG=openbao-operator:dev\n</code></pre></p> </li> <li> <p>Verify: <pre><code>kubectl get pods -n openbao-operator-system\n</code></pre></p> </li> </ol>"},{"location":"contributing/getting-started/development/#common-make-targets","title":"Common Make Targets","text":"<p>Use <code>make help</code> to see all available commands, or refer to this cheatsheet:</p> Category Target Description Build <code>make build</code> Compile the binary to <code>bin/manager</code>. <code>make docker-build</code> Build the container image. Deploy <code>make install</code> / <code>uninstall</code> Install/Remove CRDs. <code>make deploy</code> / <code>undeploy</code> Deploy/Remove Operator &amp; RBAC. Verify <code>make lint</code> Run code linters. <code>make test</code> Run unit tests. <code>make test-integration</code> Run integration tests (envtest). Generate <code>make manifests</code> Regenerate CRD YAMLs and RBAC. <code>make generate</code> Regenerate <code>deepcopy</code> code."},{"location":"contributing/getting-started/development/#troubleshooting","title":"Troubleshooting","text":"RBAC Errors during <code>make deploy</code>? <p>Ensure your current kubeconfig user has <code>cluster-admin</code> privileges. <pre><code>kubectl create clusterrolebinding my-admin --clusterrole=cluster-admin --user=$(gcloud config get-value account)\n</code></pre></p> Webhooks failing locally? <p>Validating Webhooks require the K8s API server to reach the operator. When running <code>make run</code>, this is difficult. Use the Cluster Deployment method to test webhooks.</p>"},{"location":"contributing/standards/","title":"Coding Standards","text":"<p>These coding standards ensure consistency and quality across the OpenBao Operator codebase. All contributors\u2014human or AI-assisted\u2014must follow these guidelines.</p> <p>Guiding Principle: \"Clear is better than clever.\" We prioritize readability, maintainability, and explicit error handling over terse or \"magical\" code.</p>"},{"location":"contributing/standards/#standards-directory","title":"Standards Directory","text":"<ul> <li> <p> Go Style</p> <p>Formatting, naming conventions, and idiomatic Go usage.</p> <p> Read Guide</p> </li> <li> <p> Error Handling</p> <p>Proper error wrapping, checking, and defining well-known errors.</p> <p> Read Guide</p> </li> <li> <p> Generated Artifacts</p> <p>Handling auto-generated code (CRDs, DeepCopy, RBAC).</p> <p> Read Guide</p> </li> <li> <p> K8s Patterns</p> <p>Operator best practices: idempotency, context, and status updates.</p> <p> Read Guide</p> </li> <li> <p> Security Practices</p> <p>Secure coding, input validation, and secrets handling.</p> <p> Read Guide</p> </li> <li> <p> Conventions</p> <p>Project-specific rules for metrics, logging, and extensive testing.</p> <p> Read Guide</p> </li> <li> <p> Conventional Commits</p> <p>Standardized commit messages for automated changelogs.</p> <p> Read Guide</p> </li> </ul>"},{"location":"contributing/standards/#quick-reference","title":"Quick Reference","text":""},{"location":"contributing/standards/#the-golden-rules","title":"The Golden Rules","text":"<p>Must Do</p> <ul> <li> Format Code: Always run <code>gofmt</code> or <code>goimports</code>.</li> <li> Linting: Pass <code>golangci-lint</code> with the default configuration.</li> <li> Wrap Errors: Use <code>fmt.Errorf(\"...: %w\", err)</code> to preserve context.</li> <li> Structured Logs: Use <code>log.Info(\"msg\", \"key\", \"value\")</code> instead of <code>Printf</code>.</li> <li> Test Logic: Write table-driven unit tests for all business logic.</li> <li> Verify: Run the full check suite:     <code>make lint verify-fmt verify-tidy verify-generated verify-helm test-ci</code></li> </ul> <p>Must NOT Do</p> <ul> <li> No <code>interface{}</code>: Avoid <code>any</code> types without rigorous justification.</li> <li> No Secret Logs: Never log keys, tokens, or passwords.</li> <li> No Blockers: Do NOT use <code>time.Sleep()</code> in reconcilers. Use <code>RequeueAfter</code>.</li> <li> No RBAC Annotations: Do NOT use <code>+kubebuilder:rbac</code> on the Cluster controller.</li> <li> No Shelling Out: Do NOT exec out to <code>kubectl</code> or CLI tools; use Go libraries.</li> </ul>"},{"location":"contributing/standards/#see-also","title":"See Also","text":"<ul> <li>Testing Guide \u2014 Detailed test requirements</li> <li>Documentation Style Guide \u2014 Writing docs</li> </ul>"},{"location":"contributing/standards/conventional-commits/","title":"Conventional Commits","text":"<p>We follow the Conventional Commits specification to ensure a consistent commit history and to enable automated changelog generation and semantic versioning.</p>"},{"location":"contributing/standards/conventional-commits/#format","title":"Format","text":"<p>Each commit message consists of a header, a body, and a footer.</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>Good Commit Message</p> <pre><code>feat(backup): add support for S3-compatible storage\n\nIntroduces a new `s3` stanza to the Backup configuration to allow\nstreaming snapshots to MinIO and AWS S3.\n\nCloses #123\n</code></pre>"},{"location":"contributing/standards/conventional-commits/#commit-types","title":"Commit Types","text":"Type Description SemVer Bump <code>feat</code> A new feature Minor <code>fix</code> A bug fix Patch <code>docs</code> Documentation only changes Patch <code>style</code> Formatting, missing semi-colons, etc; no code change Patch <code>refactor</code> A code change that neither fixes a bug nor adds a feature Patch <code>perf</code> A code change that improves performance Patch <code>test</code> Adding missing tests or correcting existing tests Patch <code>build</code> Changes that affect the build system or external dependencies Patch <code>ci</code> Changes to our CI configuration files and scripts Patch <code>chore</code> Other changes that don't modify src or test files Patch <code>revert</code> Reverts a previous commit Patch"},{"location":"contributing/standards/conventional-commits/#scopes","title":"Scopes","text":"<p>We enforce specific scopes to categorize changes by component.</p>"},{"location":"contributing/standards/conventional-commits/#core-components","title":"Core Components","text":"Scope Area <code>core</code> General core logic, main package <code>api</code> CRD definitions and API types <code>controller</code> Reconciler logic <code>infra</code> Infrastructure management (StatefulSet, Service) <code>config</code> HCL configuration generation <code>security</code> TLS, encryption, security profiles <code>rbac</code> RBAC generation and management"},{"location":"contributing/standards/conventional-commits/#features","title":"Features","text":"Scope Area <code>backup</code> Backup Manager and snapshot logic <code>restore</code> Restore Manager and recovery logic <code>upgrade</code> Upgrade Manager and version transitions <code>bluegreen</code> Blue/Green upgrade strategy"},{"location":"contributing/standards/conventional-commits/#testing","title":"Testing","text":"Scope Area <code>test(unit)</code> Unit tests (<code>_test.go</code>) <code>test(integration)</code> EnvTest integration tests <code>test(e2e)</code> End-to-End Kind tests"},{"location":"contributing/standards/conventional-commits/#operations-meta","title":"Operations &amp; Meta","text":"Scope Area <code>charts</code> Helm chart changes <code>manifests</code> Generated YAML manifests <code>deps</code> Dependency updates (<code>go.mod</code>) <code>ci</code> GitHub Actions workflow changes <code>build</code> Makefiles, Dockerfiles <code>docs</code> Documentation changes <code>ai</code> AI instructions and rules"},{"location":"contributing/standards/conventional-commits/#breaking-changes","title":"Breaking Changes","text":"<p>To indicate a breaking change, append <code>!</code> after the type/scope, or include <code>BREAKING CHANGE:</code> in the footer.</p> <pre><code>feat(api)!: remove deprecated v1alpha1 fields\n</code></pre> <p>OR</p> <pre><code>feat(api): remove deprecated v1alpha1 fields\n\nBREAKING CHANGE: The `autoUnseal` field has been removed in favor of `unsealConfig`.\n</code></pre>"},{"location":"contributing/standards/error-handling/","title":"Error Handling","text":"<p>Error handling is critical for the stability and debuggability of the OpenBao Operator. We follow strict patterns to ensure errors are propagated with context and handled gracefully.</p>"},{"location":"contributing/standards/error-handling/#1-wrapping-errors","title":"1. Wrapping Errors","text":"<p>Always wrap errors when passing them up the stack. This preserves the original error type (for checking) while adding valuable context (for debugging).</p>  Good Pattern Bad Pattern <pre><code>// Adds context and preserves the chain\nreturn fmt.Errorf(\"failed to create secret: %w\", err)\n</code></pre> <pre><code>// Loses the \"where\" and \"why\"\nreturn err\n</code></pre>"},{"location":"contributing/standards/error-handling/#2-guard-clauses","title":"2. Guard Clauses","text":"<p>Handle errors immediately. Check for failure first, return early, and keep the \"happy path\" unindented.</p>  Good Pattern Bad Pattern <pre><code>if err := r.Client.Get(ctx, key, obj); err != nil {\n    return fmt.Errorf(\"failed to fetch object: %w\", err)\n}\n\n// Do work on obj...\n</code></pre> <pre><code>if err := r.Client.Get(ctx, key, obj); err == nil {\n    // Do work on obj...\n} else {\n    return err\n}\n</code></pre>"},{"location":"contributing/standards/error-handling/#3-kubernetes-errors","title":"3. Kubernetes Errors","text":"<p>When interacting with the Kubernetes API, check for specific error types using <code>apierrors</code>.</p>  Good Pattern <pre><code>import apierrors \"k8s.io/apimachinery/pkg/api/errors\"\n\nif err := r.Client.Get(ctx, key, secret); err != nil {\n    if apierrors.IsNotFound(err) {\n        // Resource missing is expected here, create it\n        return r.createSecret(ctx, ...)\n    }\n    // Unexpected error, bubble it up\n    return fmt.Errorf(\"failed to get secret: %w\", err)\n}\n</code></pre>"},{"location":"contributing/standards/error-handling/#4-checkable-errors","title":"4. Checkable Errors","text":"<p>Define exported, well-known errors for implementation-specific conditions that callers might need to handle.</p> <pre><code>var (\n    // ErrClusterLocked indicates the cluster is in a frozen state.\n    ErrClusterLocked = errors.New(\"cluster is locked\")\n\n    // ErrNoLeader indicates the Raft cluster has lost quorum.\n    ErrNoLeader      = errors.New(\"no leader available\")\n)\n</code></pre> <p>Usage:</p> <pre><code>if errors.Is(err, ErrClusterLocked) {\n    // Handle lock specifically\n}\n</code></pre>"},{"location":"contributing/standards/error-handling/#5-panics","title":"5. Panics","text":"<p>No Panics Allowed</p> <p>NEVER panic in controllers or internal logic packages.</p> <p>A panic in a single reconciler thread can crash the entire operator process, taking down management for ALL clusters.</p>  Good Pattern Bad Pattern <pre><code>if config == nil {\n    return fmt.Errorf(\"internal error: config is nil\")\n}\n</code></pre> <pre><code>if config == nil {\n    panic(\"config is nil\") // CRASHES THE OPERATOR\n}\n</code></pre>"},{"location":"contributing/standards/generated-artifacts/","title":"Generated Artifacts","text":"<p>This project contains several artifacts that are automatically generated from source code or templates.</p> <p>Do Not Edit Manually</p> <p>Never edit generated files directly. Your changes will be overwritten by the next build. Always edit the Source file and run the appropriate Make Command.</p>"},{"location":"contributing/standards/generated-artifacts/#quick-reference","title":"Quick Reference","text":"If you modified... Run this command Verified by CI target <code>api/v1alpha1/*.go</code> <code>make manifests generate</code> <code>verify-generated</code> <code>internal/provisioner</code> <code>make rbac-sync</code> <code>verify-rbac-sync</code> <code>dist/install.yaml</code> <code>make helm-sync</code> <code>verify-helm</code> <code>internal/config/*.go</code> <code>make test-update-golden</code> <code>test</code> (fails if mismatch) I don't know <code>make generate manifests helm-sync</code> <code>verify-generated</code>"},{"location":"contributing/standards/generated-artifacts/#artifact-details","title":"Artifact details","text":""},{"location":"contributing/standards/generated-artifacts/#1-kubernetes-crds-deepcopy","title":"1. Kubernetes CRDs &amp; DeepCopy","text":"<p>Standard Kubebuilder artifacts generated from Go types.</p> <ul> <li>Source: <code>api/v1alpha1/*.go</code> (structs + kubebuilder markers)</li> <li>Output:</li> <li><code>config/crd/bases/</code> (CRD manifests)</li> <li><code>api/v1alpha1/zz_generated.deepcopy.go</code> (Go deepcopy methods)</li> <li>Command: <code>make manifests generate</code></li> </ul>"},{"location":"contributing/standards/generated-artifacts/#2-helm-chart-sync","title":"2. Helm Chart Sync","text":"<p>We maintain a standalone Helm chart that must stay in sync with our core manifests.</p> <ul> <li>Source: <code>config/crd/bases/</code> + <code>dist/install.yaml</code></li> <li>Output:</li> <li><code>charts/openbao-operator/crds/</code> (Synced CRDs)</li> <li><code>charts/openbao-operator/files/install.yaml.tpl</code> (Synced installer)</li> <li>Command: <code>make helm-sync</code></li> </ul>"},{"location":"contributing/standards/generated-artifacts/#3-provisioner-rbac","title":"3. Provisioner RBAC","text":"<p>The Provisioner Delegate's permissions are strictly defined in Go code to ensure security.</p> <ul> <li>Source: <code>internal/provisioner/rbac.go</code> (Go struct definitions)</li> <li>Output: <code>config/rbac/provisioner_delegate_clusterrole.yaml</code></li> <li>Command: <code>make rbac-sync</code></li> </ul>"},{"location":"contributing/standards/generated-artifacts/#4-golden-test-files","title":"4. Golden Test Files","text":"<p>We use \"Golden Files\" to verify complex HCL configuration generation reliability.</p> <ul> <li>Source: <code>internal/config/builder.go</code> logic changes</li> <li>Output: <code>internal/config/testdata/*.golden.hcl</code></li> <li>Command: <code>make test-update-golden</code></li> </ul>"},{"location":"contributing/standards/generated-artifacts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributing/standards/generated-artifacts/#ci-failure-diff-found-in-generated-files","title":"CI Failure: \"Diff found in generated files\"","text":"<p>If the <code>verify-generated</code> or <code>verify-helm</code> job fails in CI, it means you forgot to check in the updated artifacts.</p> <p>Fix:</p> <ol> <li>Run the suggested command locally (e.g., <code>make manifests generate helm-sync</code>).</li> <li>Verify you see changes in <code>config/</code> or <code>charts/</code>.</li> <li>Commit and push those changes.</li> </ol>"},{"location":"contributing/standards/go-style/","title":"Go Style Guide","text":"<p>General Go coding style and conventions for the OpenBao Operator.</p>"},{"location":"contributing/standards/go-style/#1-naming-conventions","title":"1. Naming Conventions","text":""},{"location":"contributing/standards/go-style/#acronyms","title":"Acronyms","text":"<p>Keep acronyms consistent in casing. Avoid \"Java-style\" mixed capitalization for acronyms.</p>  Good Pattern Bad Pattern <pre><code>// Keep acronyms all-caps\nServeHTTP\nNewUUID\nParseURL\nuserID\n</code></pre> <pre><code>// Do not mix case in acronyms\nServeHttp\nNewUuid\nParseUrl\nuserId\n</code></pre>"},{"location":"contributing/standards/go-style/#getters","title":"Getters","text":"<p>Go prefers direct naming for getters. Do NOT prefix with <code>Get</code>.</p>  Good Pattern Bad Pattern <pre><code>func (c *Cluster) Name() string\nfunc (c *Cluster) Status() string\n</code></pre> <pre><code>func (c *Cluster) GetName() string\nfunc (c *Cluster) GetStatus() string\n</code></pre>"},{"location":"contributing/standards/go-style/#interfaces","title":"Interfaces","text":"<p>Interfaces with a single method should end in <code>-er</code>.</p>  Good Pattern <pre><code>type Reader interface { Read(p []byte) (n int, err error) }\ntype CertRotator interface { Rotate(ctx context.Context) error }\n</code></pre>"},{"location":"contributing/standards/go-style/#2-error-handling","title":"2. Error Handling","text":""},{"location":"contributing/standards/go-style/#wrapping","title":"Wrapping","text":"<p>Always wrap errors to preserve context using <code>%w</code>.</p>  Good Pattern Bad Pattern <pre><code>if err != nil {\n    return fmt.Errorf(\"failed to sync secret: %w\", err)\n}\n</code></pre> <pre><code>if err != nil {\n    // Context is lost\n    return err\n}\n</code></pre>"},{"location":"contributing/standards/go-style/#checkable-errors","title":"Checkable Errors","text":"<p>Define exported, well-known errors for conditions callers might need to check.</p> <pre><code>var (\n    ErrClusterNotReady = errors.New(\"cluster not ready\")\n    ErrSecretNotFound  = errors.New(\"secret not found\")\n)\n</code></pre>"},{"location":"contributing/standards/go-style/#3-structured-logging","title":"3. Structured Logging","text":"<p>Use <code>logr</code> with key-value pairs. Never use <code>fmt.Printf</code> or <code>log.Println</code>.</p>  Good Pattern Bad Pattern <pre><code>log.Info(\"Reconciling Cluster\",\n    \"namespace\", req.Namespace,\n    \"name\", req.Name,\n)\n</code></pre> <pre><code>// Unstructured and lacks context\nlog.Info(fmt.Sprintf(\"Reconciling Cluster %s/%s\", req.Namespace, req.Name))\n\n// Forbidden\nfmt.Printf(\"Reconciling %s\\n\", req.Name)\n</code></pre> <p>Security Warning</p> <p>NEVER log secrets, tokens, keys, or passwords. Even in debug mode.</p>"},{"location":"contributing/standards/go-style/#4-concurrency-reconcilers","title":"4. Concurrency &amp; Reconcilers","text":""},{"location":"contributing/standards/go-style/#no-goroutines-in-reconcile","title":"No Goroutines in Reconcile","text":"<p>The <code>Reconcile</code> loop is already concurrent (if configured). Do not spawn unmanaged goroutines.</p>  Good Pattern Bad Pattern <pre><code>// Do work synchronously\nif err := r.ensurePods(ctx, cluster); err != nil {\n    return ctrl.Result{}, err\n}\n</code></pre> <pre><code>// Fails silently, loses context, risks race conditions\ngo func() {\n    r.ensurePods(ctx, cluster)\n}()\n</code></pre>"},{"location":"contributing/standards/go-style/#no-timesleep","title":"No <code>time.Sleep</code>","text":"<p>Blocking a reconciler thread degrades the entire controller's performance.</p>  Good Pattern Bad Pattern <pre><code>// Requeue nicely\nreturn ctrl.Result{RequeueAfter: 5 * time.Second}, nil\n</code></pre> <pre><code>// Blocks the worker thread\ntime.Sleep(5 * time.Second)\n</code></pre>"},{"location":"contributing/standards/go-style/#5-imports","title":"5. Imports","text":"<p>Group imports into three blocks separated by newlines:</p> <ol> <li>Standard Library</li> <li>Third-party (e.g., K8s, Controller Runtime)</li> <li>Local (<code>github.com/dc-tec/openbao-operator/...</code>)</li> </ol> <pre><code>import (\n    \"context\"\n    \"fmt\"\n\n    appsv1 \"k8s.io/api/apps/v1\"\n    \"sigs.k8s.io/controller-runtime/pkg/client\"\n\n    \"github.com/dc-tec/openbao-operator/internal/config\"\n)\n</code></pre>"},{"location":"contributing/standards/go-style/#6-constants","title":"6. Constants","text":"<p>Avoid \"Magic Numbers\" or raw strings in logic.</p>  Good Pattern Bad Pattern <pre><code>const (\n    DefaultReplicas   = 3\n    TLSSecretSuffix   = \"-tls\"\n    ReconcileInterval = 10 * time.Second\n)\n</code></pre> <pre><code>// What does \"3\" mean here?\nif cluster.Spec.Replicas &lt; 3 { ... }\n\n// Hardcoded strings are prone to typos\nsecretName := cluster.Name + \"-tls\"\n</code></pre>"},{"location":"contributing/standards/kubernetes-patterns/","title":"Kubernetes Operator Patterns","text":"<p>This guide outlines the core design patterns used in the OpenBao Operator. Understanding these is essential for writing robust, \"Kubernetes-native\" controllers.</p>"},{"location":"contributing/standards/kubernetes-patterns/#1-the-reconcile-loop","title":"1. The Reconcile Loop","text":"<p>Kubernetes controllers are Level-Triggered, not Edge-Triggered. We reconcile the current state of the world with the desired state defined in the Custom Resource (CR).</p> <pre><code>graph TD\n    Trigger[Trigger: Event or Interval] --&gt; Fetch[Fetch CR]\n    Fetch --&gt; Check{Exists?}\n    Check -- No --&gt; Stop([Stop: Deleted])\n    Check -- Yes --&gt; Child[Fetch Child Resources]\n    Child --&gt; Diff{Diff State}\n    Diff -- \"Drift Detected\" --&gt; Act[Create / Update]\n    Diff -- \"Synced\" --&gt; Status[Update Status]\n    Act --&gt; Status\n    Status --&gt; End([End: Requeue if needed])\n\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Act,Status write;\n    class Trigger,Fetch,Child,Diff read;</code></pre>"},{"location":"contributing/standards/kubernetes-patterns/#2-idempotency","title":"2. Idempotency","text":"<p>The <code>Reconcile</code> function may be called 10 times in a row for the same change. Your logic must be idempotent: it should produce the same result regardless of how many times it runs.</p>"},{"location":"contributing/standards/kubernetes-patterns/#the-fetch-check-act-pattern","title":"The \"Fetch-Check-Act\" Pattern","text":"<p>Always check if a resource exists before creating it. Check if it matches spec before updating it.</p>  Good Pattern Bad Pattern <pre><code>// 1. Define Expected Object\nexpected := &amp;corev1.Secret{\n    ObjectMeta: metav1.ObjectMeta{\n        Name:      cluster.Name + \"-tls\",\n        Namespace: cluster.Namespace,\n    },\n    Data: derivedData,\n}\n\n// 2. Fetch Existing\nexisting := &amp;corev1.Secret{}\nerr := r.Client.Get(ctx, client.ObjectKeyFromObject(expected), existing)\n\n// 3. Act based on state\nif apierrors.IsNotFound(err) {\n    // DOES NOT EXIST -&gt; CREATE\n    return r.Client.Create(ctx, expected)\n} else if err != nil {\n    return err\n}\n\n// 4. Update if drifted (simplified)\nif !reflect.DeepEqual(existing.Data, expected.Data) {\n    existing.Data = expected.Data\n    return r.Client.Update(ctx, existing)\n}\n</code></pre> <pre><code>// BLIND CREATE - Will fail on 2nd run\nsecret := &amp;corev1.Secret{...}\nif err := r.Client.Create(ctx, secret); err != nil {\n    return err\n}\n</code></pre>"},{"location":"contributing/standards/kubernetes-patterns/#3-concurrency-context","title":"3. Concurrency &amp; Context","text":""},{"location":"contributing/standards/kubernetes-patterns/#no-unmanaged-goroutines","title":"No Unmanaged Goroutines","text":"<p>The Controller Runtime manages concurrency for you. Do not spawn goroutines inside a Reconciler.</p>  Good Pattern Bad Pattern <pre><code>if err := r.heavyOperation(ctx); err != nil {\n    return ctrl.Result{}, err\n}\n</code></pre> <pre><code>go func() {\n    // Race conditions, lost errors, potential crash\n    r.heavyOperation(context.Background())\n}()\n</code></pre>"},{"location":"contributing/standards/kubernetes-patterns/#no-blocking","title":"No Blocking","text":"<p>Never block the reconcile thread with <code>time.Sleep</code>. This starves other resources from being reconciled.</p>  Good Pattern Bad Pattern <pre><code>// Re-run this loop in 10 seconds\nreturn ctrl.Result{RequeueAfter: 10 * time.Second}, nil\n</code></pre> <pre><code>// Blocks the thread completely\ntime.Sleep(10 * time.Second)\n</code></pre>"},{"location":"contributing/standards/kubernetes-patterns/#4-structured-logging","title":"4. Structured Logging","text":"<p>Use the context-aware logger. It automatically attaches <code>reconcile_id</code> and other metadata.</p>"},{"location":"contributing/standards/kubernetes-patterns/#standard-fields","title":"Standard Fields","text":"Field Description <code>cluster_namespace</code> Namespace of the target CR <code>cluster_name</code> Name of the target CR <code>phase</code> Current status phase of the operation"},{"location":"contributing/standards/kubernetes-patterns/#usage","title":"Usage","text":"Good Pattern Bad Pattern <pre><code>log := log.FromContext(ctx)\n\nlog.Info(\"Reconciling connection\", \n    \"cluster_name\", req.Name,\n    \"mode\", \"production\",\n)\n</code></pre> <pre><code>// Losing context\nfmt.Printf(\"Reconciling %s\\n\", req.Name)\n</code></pre> <p>Security</p> <p>NEVER log secrets, passwords, token content, or private keys.</p>"},{"location":"contributing/standards/project-conventions/","title":"Project Conventions","text":"<p>Specific conventions for the OpenBao Operator codebase that go beyond standard Go idioms.</p>"},{"location":"contributing/standards/project-conventions/#1-type-safety","title":"1. Type Safety","text":"<p>We leverage Go's strong typing to prevent runtime errors.</p>"},{"location":"contributing/standards/project-conventions/#strict-prohibitions","title":"Strict Prohibitions","text":"<p>No <code>any</code> or <code>interface{}</code></p> <p>The use of <code>interface{}</code> or <code>any</code> is strictly prohibited in core logic. It defeats compile-time safety and requires runtime type assertions.</p> <p>Exception: Interaction with external libraries that require it (e.g., <code>json.Unmarshal</code>).</p>  Bad Pattern Good Pattern <pre><code>func process(data any) error {\n    // Runtime crash risk!\n    return data.(string)\n}\n</code></pre> <pre><code>func process(data ConfigType) error {\n    // Compile-time safe\n    return nil\n}\n</code></pre>"},{"location":"contributing/standards/project-conventions/#enum-constants","title":"Enum Constants","text":"<p>Avoid \"stringly-typed\" code. Use defined types for status fields.</p> <pre><code>type Phase string\n\nconst (\n    PhaseRunning Phase = \"Running\"\n    PhaseFailed  Phase = \"Failed\"\n)\n</code></pre>"},{"location":"contributing/standards/project-conventions/#2-code-organization-dry","title":"2. Code Organization (DRY)","text":""},{"location":"contributing/standards/project-conventions/#the-rule-of-three","title":"The Rule of Three","text":"<p>Don't Abstract Too Early</p> <p>Do not create a helper function or shared package until logic is repeated three times.</p> <ol> <li>First time: Write it inline.</li> <li>Second time: Copy-paste (yes, really).</li> <li>Third time: Refactor into a shared helper.</li> </ol>"},{"location":"contributing/standards/project-conventions/#package-naming","title":"Package Naming","text":"<p>Avoid generic names that become \"junk drawers\".</p>  Avoid  Prefer <code>util</code>, <code>common</code>, <code>shared</code> <code>slice</code>, <code>pointer</code>, <code>k8sutil</code> <code>types</code>, <code>models</code> <code>api</code>, <code>config</code>, <code>schema</code>"},{"location":"contributing/standards/project-conventions/#3-review-hygiene","title":"3. Review Hygiene","text":"<p>To keep code reviews high-signal, adhere to these scoping rules:</p> <ul> <li> One Theme: A PR should fix a bug OR add a feature OR refactor. Not all three.</li> <li> No Drive-By Changes: Do not reformat unrelated files.</li> <li> Update Generated Files: If you change <code>api/</code>, run <code>make manifests generate</code>.</li> </ul>"},{"location":"contributing/standards/project-conventions/#4-observability-standards","title":"4. Observability Standards","text":""},{"location":"contributing/standards/project-conventions/#metrics","title":"Metrics","text":"<p>Must use the <code>openbao_</code> prefix and standard labels.</p> Type Name Labels Gauge <code>openbao_cluster_replicas</code> <code>namespace</code>, <code>name</code> Counter <code>openbao_reconcile_errors_total</code> <code>controller</code>, <code>type</code>"},{"location":"contributing/standards/project-conventions/#logging","title":"Logging","text":"<p>See Kubernetes Patterns for detailed logging rules.</p>"},{"location":"contributing/standards/project-conventions/#5-testing-requirements","title":"5. Testing Requirements","text":"<p>Every change requires verification.</p> Change Type Required Test Command Business Logic Unit Test (Table-driven) <code>make test</code> HCL Config Golden File Update <code>make test-update-golden</code> Controller Logic EnvTest Integration <code>make test-integration</code> Critical Path End-to-End Test <code>make test-e2e</code> <p>Golden Files</p> <p>Changes to <code>internal/config/builder.go</code> will often break tests. Run <code>make test-update-golden</code> to update the expected output in <code>internal/config/testdata/</code>.</p>"},{"location":"contributing/standards/project-conventions/#6-crd-evolution","title":"6. CRD Evolution","text":"<p>Breaking changes to <code>OpenBaoCluster</code> are expensive.</p> <ol> <li>Prefer Additive Changes: Add new optional fields; do not rename or remove existing ones.</li> <li>Versioning: Significant changes require a new API version (e.g., <code>v1beta1</code>).</li> <li>Migration: You must document how to migrate from the old version.</li> </ol>"},{"location":"contributing/standards/security-practices/","title":"Security Practices","text":"<p>Security is paramount when handling sensitive credentials like Unseal Keys and TLS certificates. Follow these guidelines strictly.</p>"},{"location":"contributing/standards/security-practices/#1-file-permissions","title":"1. File Permissions","text":"<p>When creating files, use the most restrictive permissions possible.</p> File Type Octal Meaning Secrets / Keys <code>0600</code> Read/Write by Owner ONLY Config / Public <code>0644</code> Read All, Write Owner Directories <code>0755</code> Execute/Read All, Write Owner  Good Pattern Bad Pattern <pre><code>// Private Key - 0600\nif err := os.WriteFile(keyPath, keyData, 0600); err != nil {\n    return err\n}\n</code></pre> <pre><code>// Too permissive!\nos.WriteFile(keyPath, keyData, 0777)\n</code></pre>"},{"location":"contributing/standards/security-practices/#2-cryptography","title":"2. Cryptography","text":""},{"location":"contributing/standards/security-practices/#randomness","title":"Randomness","text":"<p>Always use <code>crypto/rand</code> for security-sensitive operations (tokens, keys, passwords).</p>  Good Pattern Bad Pattern <pre><code>import \"crypto/rand\"\n\ntoken := make([]byte, 32)\nif _, err := rand.Read(token); err != nil {\n    return err\n}\n</code></pre> <pre><code>import \"math/rand\"\n\n// Not cryptographically secure!\ntoken := rand.Int63()\n</code></pre>"},{"location":"contributing/standards/security-practices/#certificates","title":"Certificates","text":"<p>Do not implement custom certificate logic. Use the <code>internal/pki</code> package, which defaults to safe algorithms (ECDSA P-256 or RSA 2048+).</p>"},{"location":"contributing/standards/security-practices/#3-no-shelling-out","title":"3. No Shelling Out","text":"<p>Forbidden</p> <p>Controllers and internal packages MUST NOT execute external binaries (<code>kubectl</code>, <code>helm</code>, <code>bao</code>, <code>vault</code>).</p> <p>Shelling out introduces injection vulnerabilities, dependency requirements, and performance overhead.</p>  Good Pattern Bad Pattern <pre><code>// Use the Go client\nvar pods corev1.PodList\nif err := r.Client.List(ctx, &amp;pods, client.InNamespace(ns)); err != nil {\n    return err\n}\n</code></pre> <pre><code>// Shell Injection Vulnerability!\ncmd := exec.Command(\"kubectl\", \"get\", \"pods\", \"-n\", ns)\n</code></pre>"},{"location":"contributing/standards/security-practices/#4-input-validation","title":"4. Input Validation","text":"<p>Validate all user input from Custom Resources before using it.</p> Path TraversalNumeric Ranges <pre><code>// Clean and Verify\ncleanPath := filepath.Clean(filepath.Join(baseDir, userInput))\nif !strings.HasPrefix(cleanPath, baseDir) {\n    return fmt.Errorf(\"invalid path: %s\", userInput)\n}\n</code></pre> <pre><code>if spec.Replicas &lt; 1 || spec.Replicas &gt; 9 {\n    return fmt.Errorf(\"replicas must be between 1 and 9\")\n}\n</code></pre>"},{"location":"contributing/standards/security-practices/#5-secrets-handling","title":"5. Secrets Handling","text":""},{"location":"contributing/standards/security-practices/#no-logging","title":"No Logging","text":"<p>Do Not Log Secrets</p> <p>NEVER log the content of secrets, tokens, or unseal keys. Be careful with <code>fmt.Sprintf(\"%v\", obj)</code>, which might print struct fields.</p>  Good Pattern Bad Pattern <pre><code>log.Info(\"Secret loaded\", \"name\", secret.Name, \"len\", len(secret.Data))\n</code></pre> <pre><code>log.Info(\"Got secret\", \"data\", secret.Data)\n</code></pre>"},{"location":"contributing/standards/security-practices/#memory-scrubbing","title":"Memory Scrubbing","text":"<p>Minimize the exposure window of sensitive data in memory.</p> <pre><code>func handleKeys(keys []byte) {\n    // Zero out memory when done\n    defer func() {\n        for i := range keys {\n            keys[i] = 0\n        }\n    }()\n\n    // ... use keys ...\n}\n</code></pre>"},{"location":"reference/compatibility/","title":"Compatibility Matrix","text":"<p>This document defines the supported Kubernetes and OpenBao versions for the OpenBao Operator.</p> <p>Support Policy</p> <p>We aim to support the latest 3 minor versions of Kubernetes and the latest major version of OpenBao.</p>"},{"location":"reference/compatibility/#1-kubernetes-versions","title":"1. Kubernetes Versions","text":"<p>We run full E2E tests against these versions.</p> Version Status Notes v1.35 Supported Primary CI target v1.34 Supported Verified in Nightly v1.33 Supported Minimum supported version v1.32 End of Life No longer tested"},{"location":"reference/compatibility/#2-openbao-versions","title":"2. OpenBao Versions","text":"Version Status Notes 2.4.x Supported Primary target 2.3.x Deprecated Best effort support"},{"location":"reference/compatibility/#3-ci-validation-matrix","title":"3. CI Validation Matrix","text":"<p>We treat our CI configuration as the source of truth for compatibility.</p> Workflow Scope Versions Tested PR Gate Logic &amp; Config K8s 1.35 + OpenBao 2.4.0 Nightly Full Coverage K8s 1.33, 1.34, 1.35 + OpenBao 2.4.x <p>Production Upgrade</p> <p>Always validate new Kubernetes or OpenBao versions in a staging environment before upgrading production, even if they are listed as \"Supported\".</p>"},{"location":"security/","title":"Security: OpenBao Operator","text":"<p>This section provides a comprehensive security overview for the OpenBao Operator, covering the security model, RBAC architecture, and threat analysis.</p>"},{"location":"security/#security-model-overview","title":"Security Model Overview","text":"<p>The security model relies on a Supervisor Pattern, where the operator orchestrates security-critical configuration (TLS, unseal keys, network policies) from the outside, while delegating data plane security to OpenBao itself.</p>"},{"location":"security/#secure-by-default","title":"Secure by Default","text":"<p>The Operator enforces a \"Secure by Default\" posture:</p> <ul> <li>Non-Root Execution: Operator and OpenBao pods run as non-root users</li> <li>Read-Only Filesystem: OpenBao pods use read-only root filesystem</li> <li>Network Isolation: Automatic NetworkPolicies enforce default-deny ingress</li> <li>Least-Privilege RBAC: Split-controller design with minimal permissions</li> <li>Supply Chain Security: Optional Cosign image verification</li> </ul>"},{"location":"security/#tenancy-security-models","title":"Tenancy Security Models","text":"<ul> <li>Multi-Tenant (Zero Trust): The Controller is untrusted. It cannot read Secrets and must request permissions via the Provisioner. This creates a hard security boundary between tenants.</li> <li>Single-Tenant (Direct Admin): The Controller is fully trusted within its namespace. It has <code>ClusterRole</code> permissions bound to that specific namespace, simplifying operations but removing the Zero Trust isolation.</li> </ul>"},{"location":"security/#security-topics","title":"Security Topics","text":"<ul> <li> <p> Fundamentals</p> <p>Threat model, profiles, and secrets management.</p> <p> Threat Model</p> <p> Profiles</p> <p> Secrets</p> </li> <li> <p> Infrastructure</p> <p>RBAC, Admission Policies, and Network Security.</p> <p> RBAC</p> <p> Policies</p> <p> Networking</p> </li> <li> <p> Workload</p> <p>Pod security, TLS, and Supply Chain.</p> <p> Pod Security</p> <p> TLS</p> <p> Supply Chain</p> </li> <li> <p> Multi-Tenancy</p> <p>Namespace isolation and tenant boundaries.</p> <p> Tenant Isolation</p> </li> </ul>"},{"location":"security/#see-also","title":"See Also","text":"<ul> <li>User guide: Security Profiles</li> <li>User guide: Security Considerations</li> <li>User guide: Multi-Tenancy</li> <li>User guide: Production Checklist</li> </ul>"},{"location":"security/fundamentals/","title":"Security Fundamentals","text":"<p>Core Concepts</p> <p>This section defines the foundational security models and mechanisms of the OpenBao Operator, establishing the baseline for secure operations.</p>"},{"location":"security/fundamentals/#security-model","title":"Security Model","text":"<p>The Operator implements a Defense-in-Depth strategy, ensuring security at multiple layers:</p> <ol> <li>Threat Modeling: Proactive identification of attack vectors and mitigations.</li> <li>Profiles: Pre-configured security postures (Development vs. Hardened).</li> <li>Secrets: Secure lifecycle management for root tokens and auto-unseal keys.</li> </ol>"},{"location":"security/fundamentals/#topics","title":"Topics","text":"<ul> <li> <p> Threat Model</p> <p>Detailed analysis of trust boundaries, potential threats, and architectural mitigations.</p> <p> Read Analysis</p> </li> <li> <p> Security Profiles</p> <p>Comparison of <code>development</code> versus <code>hardened</code> profiles and their impact on cluster configuration.</p> <p> Compare Profiles</p> </li> <li> <p> Secrets Management</p> <p>How the Operator generates, encrypts, and rotates sensitive credentials like Root Tokens and Recovery Keys.</p> <p> Manage Secrets</p> </li> </ul>"},{"location":"security/fundamentals/#see-also","title":"See Also","text":"<ul> <li> Infrastructure Security \u2014 RBAC and Network Policies.</li> <li> Workload Security \u2014 Pod Security and TLS.</li> </ul>"},{"location":"security/fundamentals/profiles/","title":"Security Profiles","text":"<p>Concept</p> <p>OpenBao Operator supports two distinct security profiles via <code>spec.profile</code>. These profiles enforce different validation rules and default behaviors to match the environment's risk level.</p>"},{"location":"security/fundamentals/profiles/#profile-comparison","title":"Profile Comparison","text":"Feature  Hardened (Production)  Development (Testing) Root Token Never Generated Stored in Secret Unseal Keys External KMS Required Stored in Secret TLS External / ACME Required Operator Managed Allowed Replicas Minimum 3 (HA Required) Any (1+) Self-Init Required (<code>enabled=true</code>) Optional Admission Check Strict Validation Relaxed Validation Use Case Production Proof of Concept, Local Dev"},{"location":"security/fundamentals/profiles/#detailed-configuration","title":"Detailed Configuration","text":"Hardened Profile Development Profile <p>Production Ready</p> <p>The <code>hardened</code> profile is MANDATORY for all production deployments. It enforces a \"Secure by Default\" posture that eliminates Root Tokens and ensures strong encryption.</p> <p>To use this profile, your <code>OpenBaoCluster</code> must meet these requirements:</p> <ol> <li>High Availability: You must set <code>spec.replicas</code> to at least <code>3</code> for Raft quorum.</li> <li>External KMS: You must provide a KMS key (AWS, GCP, Azure, or Vault Transit) for auto-unseal.</li> <li>Valid TLS: You must provide valid TLS certificates (via <code>cert-manager</code> or external secret); <code>tlsSkipVerify</code> is rejected.</li> <li>Self-Initialization: The Operator must drive the initialization process to ensure no humans handle initial secrets.</li> </ol> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: production-cluster\nspec:\n  profile: hardened\n  replicas: 3  # Minimum 3 for HA\n  selfInit:\n    enabled: true\n  unseal:\n    mode: awskms # or gcpckms, azurekeyvault\n</code></pre> <p>Non-Production Only</p> <p>The <code>development</code> profile creates significant security risks by storing the Root Token in a Kubernetes Secret. This allows any user with Secret read permissions to take full control of the cluster.</p> <p>This profile is useful for:</p> <ul> <li>Local testing (Minikube/Kind).</li> <li>CI/CD integration tests.</li> <li>Rapid prototyping where long-term security is not required.</li> </ul> <p>Key Behaviors:</p> <ul> <li>Root Token: Generated and stored in <code>&lt;cluster-name&gt;-root-token</code>.</li> <li>Unseal Keys: Generated and stored in <code>&lt;cluster-name&gt;-unseal-key</code> (unless KMS is configured).</li> <li>Status Warning: The Operator sets <code>ConditionSecurityRisk=True</code> on the cluster status.</li> </ul>"},{"location":"security/fundamentals/profiles/#guidance","title":"Guidance","text":"<p>Migration Path</p> <p>Teams often start with Development for initial exploration. When moving to Staging or Production, you should create a new cluster with the Hardened profile rather than trying to converting an existing Development cluster. Trust roots established in Development are typically not secure enough for Production.</p>"},{"location":"security/fundamentals/profiles/#see-also","title":"See Also","text":"<ul> <li> Infrastructure Security</li> <li>Server Configuration</li> </ul>"},{"location":"security/fundamentals/secrets-management/","title":"Secrets Management","text":"<p>The Operator manages the lifecycle of several critical secrets, from the Root Token to internal PKI certificates.</p>"},{"location":"security/fundamentals/secrets-management/#managed-secrets-matrix","title":"Managed Secrets Matrix","text":"Secret Name Config Default Name Rotation Policy Risk Level Root Token <code>&lt;cluster&gt;-root-token</code> Manual Revocation  Critical Unseal Keys <code>&lt;cluster&gt;-unseal-key</code> Manual Rotation  Critical Cluster CA <code>&lt;cluster&gt;-ca</code> 1 Year (Auto)  High TLS Certs <code>&lt;cluster&gt;-tls</code> 90 Days (Auto)  High Backup Creds User Defined User Managed  High"},{"location":"security/fundamentals/secrets-management/#auto-unseal-configuration","title":"Auto-Unseal Configuration","text":"Static (Default)External KMS (Recommended) <p>Static Keys</p> <p>This mode generates a static 32-byte key stored in a Kubernetes Secret. This key becomes the root of trust for your OpenBao data encryption. If <code>etcd</code> is not encrypted at rest, this key is vulnerable.</p> <p>Behavior:</p> <ol> <li>Generation: Operator generates a random 32-byte key.</li> <li>Storage: Stored in <code>&lt;cluster&gt;-unseal-key</code>.</li> <li>Mounting: Mounted at <code>/etc/bao/unseal/key</code>.</li> <li>Condition: Sets <code>ConditionEtcdEncryptionWarning=True</code> if etcd encryption is not verified.</li> </ol> <p>Enhanced Security</p> <p>Using an external KMS (AWS, GCP, Azure, or Vault Transit) shifts the root of trust away from the Kubernetes cluster, significantly improving security.</p> <p>Behavior:</p> <ol> <li>No Operator Key: The Operator does NOT generate or manage unseal keys.</li> <li>Configuration: Configure <code>spec.unseal</code> with your provider details.</li> <li>Authentication:<ul> <li>Workload Identity (Recommended): Use IRSA (AWS) or Workload Identity (GCP) to authenticate without static credentials.</li> <li>Credentials Secret: Mount static credentials via <code>spec.unseal.credentialsSecretRef</code>.</li> </ul> </li> </ol>"},{"location":"security/fundamentals/secrets-management/#root-token-lifecycle","title":"Root Token Lifecycle","text":"<p>Root Token Security</p> <p>In Development profile, the Root Token is stored in a Secret. This grants Full Administrative Access to anyone who can read Secrets in the namespace.</p> <p>Recommendation: Immediately revoke the root token after initial setup or use Self-Initialization with the Hardened profile, which avoids storing the root token entirely.</p>"},{"location":"security/fundamentals/secrets-management/#jwt-authentication-oidc","title":"JWT Authentication &amp; OIDC","text":"<p>The Operator uses Kubernetes OIDC to securely authenticate Backup and Upgrade executor jobs without managing static long-lived tokens.</p>"},{"location":"security/fundamentals/secrets-management/#workflow","title":"Workflow","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant Op as Operator\n    participant K8s as Kubernetes API\n    participant Bao as OpenBao\n    participant Job as Executor Job\n\n    Note over Op, K8s: 1. Discovery\n    Op-&gt;&gt;K8s: Discover OIDC Issuer &amp; JWKS\n\n    Note over Op, Bao: 2. Bootstrap\n    Op-&gt;&gt;Bao: Enable JWT Auth\n    Op-&gt;&gt;Bao: Create \"openbao-operator\" Policy/Role\n\n    Note over Op, Job: 3. Execution\n    Op-&gt;&gt;K8s: Create Job (Mounts Projected Token)\n    K8s--&gt;&gt;Job: Start with Token (aud=openbao-internal)\n\n    Job-&gt;&gt;Bao: Login (JWT)\n    Bao--&gt;&gt;Job: OpenBao Token\n    Job-&gt;&gt;Bao: Perform Snapshot/Upgrade</code></pre>"},{"location":"security/fundamentals/secrets-management/#benefits","title":"Benefits","text":"<ol> <li>Short-Lived: Projected tokens expire automatically (default 1 hour).</li> <li>Rotated: Kubernetes rotates the tokens automatically.</li> <li>Audience Bound: Tokens are valid only for the <code>openbao-internal</code> audience, preventing replay attacks against other services.</li> </ol>"},{"location":"security/fundamentals/secrets-management/#see-also","title":"See Also","text":"<ul> <li> External Access</li> <li> Security Profiles</li> <li> Backups Integration</li> </ul>"},{"location":"security/fundamentals/threat-model/","title":"Threat Model","text":"<p>Scope</p> <p>This document analyzes the security boundaries, assets, and potential threats to the OpenBao Operator using the STRIDE framework.</p>"},{"location":"security/fundamentals/threat-model/#1-trust-boundaries","title":"1. Trust Boundaries","text":"<p>The system is divided into three major trust zones.</p> <pre><code>graph TD\n    subgraph Operator_Zone [Trust Zone: Operator]\n        Op[Operator Controller]\n        OpSA[ServiceAccount: Operator]\n    end\n\n    subgraph Tenant_Zone [Trust Zone: Tenant Namespace]\n        Bao[OpenBao Cluster]\n        Secret[Root Token / Unseal Key]\n        PVC[Raft Storage PVC]\n    end\n\n    subgraph Cloud_Zone [Trust Zone: External]\n        S3[Object Storage]\n        K8sAPI[Kubernetes API]\n    end\n\n    Op -- Reconciles --&gt; Bao\n    Op -- Minimal Access --&gt; K8sAPI\n    Op -.-&gt;|Blind Write| Secret\n    Bao -- Streams Snapshots --&gt; S3\n\n    linkStyle 0 stroke:#22c55e,stroke-width:2px;\n    linkStyle 2 stroke:#ef4444,stroke-width:2px;</code></pre>"},{"location":"security/fundamentals/threat-model/#2-asset-identification","title":"2. Asset Identification","text":"Asset Risk Level Location Description Root Token Critical <code>Secret</code> Grants full administrative access to OpenBao. Unseal Keys High <code>Secret</code> Static keys used to decrypt the vault master key. Raft Data High <code>PVC</code> Encrypted persistent storage containing all vault data. Snapshots High <code>S3/GCS</code> Encrypted backups of the Raft data. CA Key High <code>Secret</code> Private key for the Cluster Root CA. Configuration  Medium <code>ConfigMap</code> HCL configuration files."},{"location":"security/fundamentals/threat-model/#3-stride-analysis","title":"3. STRIDE Analysis","text":"Spoofing (Identity) <p>Threat: A rogue pod attempts to join the Raft cluster.</p> <p>Mitigation: mTLS</p> <p>Only pods with a valid certificate signed by the Operator CA (mounted via Secret) can join the mesh.</p> <p>Threat: An attacker spoofs external endpoints.</p> <p>Mitigation: Network Policy</p> <p>Default-deny ingress policies enforce cluster isolation. TLS required for all external traffic.</p> Tampering (Data Integrity) <p>Threat: User manually edits the StatefulSet (e.g., changes image).</p> <p>Mitigation: Reconciliation</p> <p>The Operator watches for changes and immediately reverts drift to the desired state defined in the CRD.</p> <p>Threat: Malicious tenant points backups to unauthorized storage.</p> <p>Mitigation: Policy</p> <p>Use Admission Policies (ValidatingAdmissionPolicy) to restrict allowed backup targets.</p> Repudiation (Audit Logs) <p>Threat: Lack of audit trail for critical actions (step-down, backup).</p> <p>Mitigation: Structured Auditing</p> <p>The Operator emits structured JSON logs with <code>audit=true</code> for all control plane actions.</p> Information Disclosure (Privacy) <p>Threat: TLS keys or tokens exposed in logs.</p> <p>Mitigation: Redaction</p> <p>Strict policy against logging secrets. Use <code>crypto/rand</code> for generation. OpenBao telemetry is used instead of debug logs.</p> Denial of Service (Availability) <p>Threat: Misconfigured CR causing hot reconcile loops.</p> <p>Mitigation: Rate Limiting</p> <p>The controller utilizes <code>MaxConcurrentReconciles</code> and exponential backoff to preventing API saturation.</p> Elevation of Privilege (Authorization) <p>Threat: Attacker compromises the Operator Pod.</p> <p>Mitigation: Least Privilege</p> <ul> <li>Non-Root: Operator runs as non-root.</li> <li>Blind Writes: Operator can create Secrets but cannot list/read them back.</li> <li>Split RBAC: Separated ServiceAccounts for Provisioning vs. Management.</li> </ul>"},{"location":"security/fundamentals/threat-model/#4-secrets-management","title":"4. Secrets Management","text":"<p>Critical Requirements</p> <ol> <li>Never Log Secrets: Root tokens, unseal keys, and CA keys must NEVER appear in stdout/stderr.</li> <li>Unique Names: Secrets must include the cluster name to prevent collisions.</li> <li>Strict RBAC: Only Cluster Admins should have access to the Root Token Secret.</li> </ol>"},{"location":"security/infrastructure/","title":"Infrastructure Security","text":"<p>Platform Controls</p> <p>Infrastructure security focuses on the Kubernetes platform layer: protecting the Operator's control plane, isolating tenant namespaces, and enforcing policy compliance before workloads even start.</p>"},{"location":"security/infrastructure/#overview","title":"Overview","text":"<p>The OpenBao Operator leverages native Kubernetes security primitives to create a Zero Trust environment:</p> <ol> <li>RBAC: A precise, split-controller model that grants permissions only where needed (Provisioning vs. Management).</li> <li>Admission Policies: Guardrails that prevent insecure configurations (like disabling TLS) from being applied.</li> <li>Network Security: Isolation layers that restrict traffic flow between tenants and the internet.</li> </ol>"},{"location":"security/infrastructure/#topics","title":"Topics","text":"<ul> <li> <p> RBAC Architecture</p> <p>Deep dive into the Provisioner and Controller role separation and the \"Blind Write\" pattern.</p> <p> Explore RBAC</p> </li> <li> <p>:material-policy: Admission Policies</p> <p>Using <code>ValidatingAdmissionPolicy</code> (CEL) to enforce security standards without webhooks.</p> <p> View Policies</p> </li> <li> <p> Network Security</p> <p>Default-deny <code>NetworkPolicies</code> and controlling Egress traffic for backups and upgrades.</p> <p> Network Controls</p> </li> </ul>"},{"location":"security/infrastructure/#prerequisites","title":"Prerequisites","text":"<p>Cluster Requirements</p> <ul> <li>Kubernetes v1.30+: Required for <code>ValidatingAdmissionPolicy</code> (GA in 1.30).</li> <li>CNI Plugin: A CNI that enforces <code>NetworkPolicy</code> (e.g., Cilium, Calico, Antrea) is required for isolation features to work.</li> </ul>"},{"location":"security/infrastructure/#see-also","title":"See Also","text":"<ul> <li> Security Fundamentals</li> <li> Workload Security</li> </ul>"},{"location":"security/infrastructure/admission-policies/","title":"Admission Policies","text":"<p>Concept</p> <p>The Operator uses Kubernetes <code>ValidatingAdmissionPolicy</code> (CEL) to enforce security invariants at the API level. This provides Defense-in-Depth by rejecting invalid or insecure configurations before they are persisted to etcd, supplementing the Operator's runtime reconciliation loops.</p>"},{"location":"security/infrastructure/admission-policies/#enforcement-flow","title":"Enforcement Flow","text":"<p>The following diagram illustrates how the Operator's policies intercept GitOps syncs:</p> <pre><code>graph LR\n    User[\"GitOps Pipeline\"]\n    API[\"Kubernetes API\"]\n    VAP[\"ValidatingAdmissionPolicy&lt;br/&gt;(lock-managed-resource-mutations)\"]\n    Res[\"Managed Resource&lt;br/&gt;(StatefulSet)\"]\n\n    User --\"Apply Change\"--&gt; API\n    API --\"Validate\"--&gt; VAP\n    VAP --\"Deny\"--&gt; API\n    API -.-x|\"Reject\"| User\n\n    API --\"Pass\"--&gt; Res\n\n    %% Style Guide Compliant\n    classDef git fill:transparent,stroke:#f472b6,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n    classDef security fill:transparent,stroke:#dc2626,stroke-width:2px,color:#fff;\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n\n    class User git;\n    class API read;\n    class VAP security;\n    class Res write;</code></pre>"},{"location":"security/infrastructure/admission-policies/#policy-inventory","title":"Policy Inventory","text":"<p>The Operator ships with a suite of policies to enforce \"Least Privilege\" and \"GitOps Safety\":</p> Policy Name Target Enforcement Description <code>lock-managed-resource-mutations</code> <code>StatefulSet</code>, <code>Service</code>, <code>Secret</code> Block Prevents users/GitOps from modifying resources managed by the Operator (labeled <code>app.kubernetes.io/managed-by=openbao-operator</code>). <code>lock-controller-statefulset-mutations</code> <code>StatefulSet</code> (Controller) Block Self-protection: prevents the Controller from modifying its own sensitive fields (volumes, args). <code>validate-openbaocluster</code> <code>OpenBaoCluster</code> Validate Enforces spec invariants (e.g., Hardened profile requirements, TLS configs). <code>restrict-provisioner-delegate</code> <code>Role</code>, <code>RoleBinding</code> Restrict Limits the Provisioner Delegate to creating only specific, pre-approved RBAC roles."},{"location":"security/infrastructure/admission-policies/#provisioner-delegate-hardening","title":"Provisioner Delegate Hardening","text":"<p>The <code>restrict-provisioner-delegate</code> policy is a defense-in-depth control that applies to RBAC mutations performed by the impersonated Provisioner Delegate identity.</p> <p>Key guarantees:</p> <ul> <li>Only specific Role/RoleBinding names are allowed (tenant + secrets allowlist roles).</li> <li>RoleBindings are restricted to known ServiceAccount subjects (prevents backdoor bindings).</li> <li>Dangerous verbs and wildcards are denied (<code>impersonate</code>, <code>bind</code>, <code>escalate</code>, <code>*</code>).</li> <li>Secret permissions are only allowed via the dedicated secrets allowlist Roles, and those Roles must be name-scoped (<code>resourceNames</code>) and non-enumerating (no <code>list</code>/<code>watch</code>).</li> </ul>"},{"location":"security/infrastructure/admission-policies/#configuration-ownership","title":"Configuration Ownership","text":"<p>The Operator ensures that user intent (<code>spec.configuration</code>) is respected while enforcing mandatory platform settings.</p>  Operator Owned User Tunable <p>These stanzas are Always Overwritten by the Operator to ensure correctness and security:</p> <ul> <li><code>listener \"tcp\"</code>: TLS settings are mandatory based on <code>spec.tls</code>.</li> <li><code>storage \"raft\"</code>: Peer discovery is managed by the Operator.</li> <li><code>seal</code>: Auto-unseal configuration is derived from <code>spec.unseal</code>.</li> <li><code>api_addr</code>, <code>cluster_addr</code>: Networking identity is fixed.</li> </ul> <p>These areas are safe for user customization via <code>spec.configuration</code>:</p> <ul> <li><code>telemetry</code>: Metrics and tracing.</li> <li><code>log_level</code>: Observability tuning.</li> <li><code>plugin_directory</code>: Custom plugin paths.</li> <li><code>ui</code>: Dashboard enablement.</li> </ul>"},{"location":"security/infrastructure/admission-policies/#see-also","title":"See Also","text":"<ul> <li> RBAC Architecture</li> <li> Security Profiles</li> </ul>"},{"location":"security/infrastructure/network-security/","title":"Network Security","text":"<p>Core Concept</p> <p>The Operator enforces a Default Deny network posture for every OpenBao cluster. This means the OpenBao pods start in total isolation, and the Operator explicitly whitelists only the traffic required for clustering, monitoring, and management.</p>"},{"location":"security/infrastructure/network-security/#network-perimeter","title":"Network Perimeter","text":"<p>The following diagram illustrates the trusted communication paths allowed through the NetworkPolicy firewall:</p> <pre><code>flowchart TB\n    %% External Actors\n    Operator[\"Operator Pod\"]\n    K8sAPI[\"Kubernetes API\"]\n    DNS[\"CoreDNS\"]\n    Peer[\"Raft Peers\"]\n\n    %% The OpenBao Cluster\n    subgraph Cluster [\"OpenBao Perimeter (Default Deny)\"]\n        Yield[\"Active Node\"]\n    end\n\n    %% Ingress Rules\n    Operator --\"Ingress (8200)\"--&gt; Yield\n    Peer --\"Ingress (8200/8201)\"--&gt; Yield\n\n    %% Egress Rules\n    Yield --\"Egress (443)\"--&gt; K8sAPI\n    Yield --\"Egress (53)\"--&gt; DNS\n    Yield --\"Egress (8201)\"--&gt; Peer\n\n    %% Styling\n    classDef external fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n    classDef secure fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef blocked fill:transparent,stroke:#dc2626,stroke-width:2px,color:#fff;\n\n    class Operator,K8sAPI,DNS,Peer external;\n    class Yield secure;</code></pre>"},{"location":"security/infrastructure/network-security/#traffic-rules","title":"Traffic Rules","text":"Ingress (Incoming) Egress (Outgoing) <p>By default, all incoming traffic is blocked. The following exceptions are made to allow the cluster to function:</p> Source Port Reason Operator <code>8200</code> (HTTP) Required for liveness probes, initialization checks, and status updates. Raft Peers <code>8201</code> (TCP) Required for Raft consensus and replication between pods in the StatefulSet. Gateway <code>8200</code> (HTTP) (Optional) Allowed only if <code>spec.gateway</code> is enabled, permitting traffic from the Gateway API namespace. Kube System Any Required for some CNI/DNS health checks from the system namespace. <p>Custom Ingress</p> <p>You can allow additional traffic (e.g., from your application namespaces) via <code>spec.network.ingressRules</code>. These rules are additive and cannot disable the core operator rules.</p> <p>Egress is strictly limited to prevent data exfiltration and restrict the blast radius of a compromised path:</p> Destination Port Reason Kubernetes API <code>443</code> Required for Service Registration (updating Pod labels) and Peer Discovery. CoreDNS <code>53</code> (UDP/TCP) Required for resolving external services and peer addresses. Raft Peers <code>8201</code> (TCP) Required for replication traffic. <p>Backup Jobs</p> <p>Backup and Restore jobs are excluded from this restrictive policy. They run as separate pods that need broad access to reach external object storage (S3, GCS, Azure).</p>"},{"location":"security/infrastructure/network-security/#controller-network-security","title":"Controller Network Security","text":"<p>The OpenBao Operator Controller itself runs in a highly restricted network environment to minimize its attack surface.</p>  Ingress Egress <p>Default Deny: All incoming traffic to the controller is blocked by default.</p> Source Port Reason Monitoring <code>8080</code> / <code>8443</code> Metrics endpoint (Prometheus). Kubelet <code>8081</code> healthz/readyz probes. Webhook <code>9443</code> Kubernetes API Server admission (validating/mutating) webhook requests. <p>The controller is permitted to initiate connections only to essential services:</p> Destination Reason Kubernetes API Watching and reconciling resources. Webhooks Self-calls for admission webhooks (if applicable)."},{"location":"security/infrastructure/network-security/#see-also","title":"See Also","text":"<ul> <li> Network Configuration</li> <li>:material-policy: Admission Policies</li> </ul>"},{"location":"security/infrastructure/rbac/","title":"RBAC Architecture","text":"<p>Core Concept</p> <p>The Operator implements a Zero Trust security model by splitting responsibilities between two distinct ServiceAccounts: a Provisioner (cluster-wide permission manager) and a Controller (namespace-scoped workload manager). This ensures that a compromise of the workload controller does not grant cluster-wide administrative access.</p>"},{"location":"security/infrastructure/rbac/#architecture-diagram","title":"Architecture Diagram","text":"<p>The \"Split-Controller Model\" ensures that broad permissions are never held by the long-running controller process.</p> <pre><code>flowchart TB\n    subgraph OperatorNS [\"Operator Namespace\"]\n        Prov[\"Provisioner SA\"]\n        Ctrl[\"Controller SA\"]\n        Tmpl[\"Delegate Template\"]\n    end\n\n    subgraph TenantNS [\"Tenant Namespace\"]\n        TRole[\"Tenant Role\"]\n        Workload[\"StatefulSet / Pods\"]\n    end\n\n    %% Provisioner Flow\n    Prov -.-&gt;|\"Impersonate\"| Tmpl\n    Tmpl --\"Create/Update\"--&gt; TRole\n\n    %% Controller Flow\n    Ctrl --\"Bind\"--&gt; TRole\n    TRole --\"Manage\"--&gt; Workload\n\n    %% Styling\n    classDef security fill:transparent,stroke:#dc2626,stroke-width:2px,color:#fff;\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Prov,Tmpl,TRole security;\n    class Ctrl,Workload write;</code></pre>"},{"location":"security/infrastructure/rbac/#serviceaccount-permissions","title":"ServiceAccount Permissions","text":"Provisioner Delegate (Impersonated) Controller <p>The Provisioner is responsible for \"Day 1\" setup. It provisions tenant RBAC by impersonating a dedicated Delegate ServiceAccount.</p> <p>Blind Write Pattern</p> <p>The Provisioner creates Roles in tenant namespaces but does not grant itself permission to use them. It delegates these permissions to the Controller. This prevents the Provisioner from inspecting tenant data.</p> Resource Verbs Rationale <code>Namespace</code> <code>get</code>, <code>update</code>, <code>patch</code> Manage namespace labels/annotations. No <code>list</code> (prevents discovery). <code>OpenBaoTenant</code> <code>get</code>, <code>list</code>, <code>watch</code> Watch for new tenant requests. <code>Role / RoleBinding</code> (none directly) RBAC objects are created/updated/deleted via the Delegate ServiceAccount through impersonation. <code>ServiceAccount</code> <code>impersonate</code> Use the \"Delegate\" to safely elevate privileges for Role creation. <p>The Delegate ServiceAccount is never used directly by a controller. It exists only to provide a tightly scoped identity that the Provisioner can impersonate for RBAC management.</p> <p>Defense In Depth</p> <p>The Delegate is additionally constrained by the <code>restrict-provisioner-delegate</code> ValidatingAdmissionPolicy, which limits the RBAC objects it can create/update and enforces strict Secret allowlist Roles.</p> <p>The Controller is responsible for \"Day 2\" operations. It has high privileges within tenant namespaces but zero privileges outside them.</p> <p>Isolation</p> <p>The Controller cannot even list namespaces. It is entirely dependent on the Provisioner to \"introduce\" it to a tenant namespace via a RoleBinding.</p> <p>Cluster Scope:</p> Resource Verbs Rationale <code>OpenBaoCluster</code> <code>get</code>, <code>list</code>, <code>watch</code> Global watch for CRD events. <code>TokenReview</code> <code>create</code> Authenticate metrics requests. <code>ValidatingAdmissionPolicy</code> <code>get</code> Verify security policy existence. <p>Tenant Scope (via RoleBinding):</p> Resource Verbs Rationale <code>StatefulSet</code> <code>*</code> Manage OpenBao pods. <code>Service</code>, <code>Ingress</code> <code>*</code> Manage network access. <code>Secret</code> (allowlisted) Secret access is limited by name (dedicated reader/writer Roles). No <code>list</code>/<code>watch</code>. <code>ConfigMap</code> <code>*</code> Manage configuration and TLS metadata. <code>Job</code> <code>*</code> Run snapshots and upgrades. <code>Gateway</code> ... <code>*</code> (Optional) Manage Gateway API resources if enabled."},{"location":"security/infrastructure/rbac/#security-guarantees","title":"Security Guarantees","text":"<ol> <li>No Secret Enumeration: Neither ServiceAccount has <code>list</code> permissions on Secrets cluster-wide.</li> <li>No Topology Discovery: Neither ServiceAccount has <code>list</code> permissions on Namespaces (Provisioner knows only what you tell it via CRs).</li> <li>Privilege Separation: The account that writes the permissions (Provisioner) cannot use them, and the account that uses them (Controller) cannot change them.</li> <li>Name-Scoped Secrets: Tenant Secret access is restricted to explicit Secret name allowlists and enforced by admission policy (no Secrets wildcards or enumeration).</li> </ol>"},{"location":"security/infrastructure/rbac/#see-also","title":"See Also","text":"<ul> <li>:material-policy: Admission Policies</li> <li> Network Security</li> </ul>"},{"location":"security/multi-tenancy/","title":"Multi-Tenancy Security","text":"<p>Shared Platform, Isolated Tenants</p> <p>OpenBao Operator is designed for Hard Multi-Tenancy. It allows multiple independent teams to share a single Kubernetes cluster and Operator installation while maintaining strict cryptographic, network, and identity isolation.</p>"},{"location":"security/multi-tenancy/#security-pillars","title":"Security Pillars","text":"<ul> <li> <p> Tenant Isolation</p> <p>How the \"Provisioner\" controller enforces strict namespace boundaries and prevents cross-tenant access.</p> <p> Isolation Model</p> </li> <li> <p> RBAC Boundaries</p> <p>The \"Zero Trust\" split-controller architecture that ensures no single credential has total cluster control.</p> <p> RBAC Architecture</p> </li> <li> <p> Network Isolation</p> <p>Default Deny NetworkPolicies that prevent tenants from discovering or accessing each other's pods.</p> <p> Network Security</p> </li> </ul>"},{"location":"security/multi-tenancy/#the-split-controller-model","title":"The Split-Controller Model","text":"<p>To achieve secure multi-tenancy, the Operator splits responsibilities between two distinct controllers:</p> <ol> <li> <p>The Provisioner:</p> <ul> <li>Scope: Cluster-wide.</li> <li>Power: Can create Roles/RoleBindings but cannot read Secrets or manage Workloads.</li> <li>Role: The \"Landlord\" who hands out keys but can't enter apartments.</li> </ul> </li> <li> <p>The Controller:</p> <ul> <li>Scope: Namespace-restricted (per tenant).</li> <li>Power: Can manage Workloads/Secrets but only in namespaces where the Provisioner issued a key.</li> <li>Role: The \"Tenant\" who manages their own apartment.</li> </ul> </li> </ol>"},{"location":"security/multi-tenancy/#see-also","title":"See Also","text":"<ul> <li> User Guide: Multi-Tenancy</li> <li> User Guide: Onboarding</li> </ul>"},{"location":"security/multi-tenancy/tenant-isolation/","title":"Tenant Isolation (Namespace Provisioner)","text":"<p>Governance Engine</p> <p>The Provisioner controller is the enforcement engine for multi-tenancy. It watches for <code>OpenBaoTenant</code> requests and responsibly \"onboards\" namespaces by creating strict, limited RBAC bindings for the workload controller.</p>"},{"location":"security/multi-tenancy/tenant-isolation/#governance-models","title":"Governance Models","text":"<p>The Operator supports two distinct onboarding models depending on your organization's trust level:</p>  Self-Service (Recommended) Centralized Admin <p>Ideal for autonomous platform teams where reliability is the main goal.</p> <ul> <li>Mechanism: Namespace admins create an <code>OpenBaoTenant</code> CR in their own namespace.</li> <li>Constraint: <code>spec.targetNamespace</code> MUST match <code>metadata.namespace</code>.</li> <li>Security: Prevents users from provisioning RBAC in namespaces they do not own.</li> </ul> <pre><code>apiVersion: operator.openbao.org/v1alpha1\nkind: OpenBaoTenant\nmetadata:\n  name: my-tenant\n  namespace: team-a # &lt;--- Source\nspec:\n  targetNamespace: team-a # &lt;--- MUST Match Source\n</code></pre> <p>Ideal for strict compliance environments where only a central platform team can vend database-as-a-service.</p> <ul> <li>Mechanism: Cluster admins create an <code>OpenBaoTenant</code> CR in the Operator's namespace.</li> <li>Capability: Can target any namespace.</li> <li>Security: Rely on Kubernetes RBAC to prevent normal users from creating CRs in the operator namespace.</li> </ul> <pre><code>apiVersion: operator.openbao.org/v1alpha1\nkind: OpenBaoTenant\nmetadata:\n  name: team-a-onboarding\n  namespace: openbao-operator-system # &lt;--- Admin Namespace\nspec:\n  targetNamespace: team-a # &lt;--- Target Any Namespace\n</code></pre>"},{"location":"security/multi-tenancy/tenant-isolation/#provisioning-flow","title":"Provisioning Flow","text":"<p>This diagram illustrates how the Provisioner (Cluster Scope) safely grants permission to the Controller (Namespace Scope) without ever possessing those permissions itself.</p> <pre><code>flowchart TD\n    User(\"User / Admin\")\n\n    subgraph OperatorNS[\"Operator Namespace\"]\n        OBT[\"OpenBaoTenant CR\"]\n        Prov{{\"Provisioner\"}}\n        Ctrl[[\"Controller SA\"]]\n    end\n\n    subgraph TenantNS[\"Tenant Namespace\"]\n        TRole[(\"Tenant Role\")]\n        TRB[(\"RoleBinding\")]\n        Workload[\"OpenBao Cluster\"]\n    end\n\n    %% Flow\n    %% Flow\n    User -- 1. Request --&gt; OBT\n    OBT -- 2. Trigger --&gt; Prov\n    Prov -- 3. Create --&gt; TRole\n    Prov -- 4. Bind --&gt; TRB\n    TRB -.-&gt;|5. Grant Access| Ctrl\n    Ctrl ==&gt;|6. Reconcile| Workload</code></pre> <ol> <li>Request: User creates <code>OpenBaoTenant</code>.</li> <li>Verify: Provisioner checks if the request is valid (Self-Service constraint).</li> <li>Create: Provisioner creates a <code>Role</code> in the target namespace (permissions to manage StatefulSets, Services, etc.).</li> <li>Bind: Provisioner creates a <code>RoleBinding</code>, connecting the Controller ServiceAccount to that Role.<ul> <li>Note: The Provisioner does not grant itself access. It performs a Blind Write.</li> </ul> </li> <li>Reconcile: The Controller now has permission to manage resources in that namespace.</li> </ol>"},{"location":"security/multi-tenancy/tenant-isolation/#security-guarantees","title":"Security Guarantees","text":"<p>The architecture provides the following immutable security properties:</p> Property Description No Topology Discovery The Controller cannot <code>list</code> namespaces. It literally does not know other tenants exist. No Cross-Talk Namespace A's RoleBinding does not grant access to Namespace B. Privilege Separation The component that grants access (Provisioner) cannot use access. The component that uses access (Controller) cannot grant access. Namespace Hardening The Provisioner automatically labels tenant namespaces with <code>pod-security: restricted</code>, preventing insecure workloads."},{"location":"security/multi-tenancy/tenant-isolation/#threat-mitigation-checklist","title":"Threat Mitigation Checklist","text":"Threat Mitigation Strategy Control Tenant A reads Tenant B's Keys RBAC Scoping Controller has no cluster-wide Secret access. Tenant A DoS attack on Node Resource Quotas Namespace Quotas (Configurable via <code>OpenBaoTenant</code>) + Controller Rate Limiting. Tenant A attacks Tenant B's Pods Network Isolation Default Deny NetworkPolicy. Tenant A spoofs Admin Role Restrictions Self-Service mode enforces <code>targetNamespace == namespace</code>."},{"location":"security/workload/","title":"Workload Security","text":"<p>Runtime Protection</p> <p>Workload security focuses on the runtime aspect of the OpenBao deployment: securing the Pods, Containers, and Images that make up the service.</p>"},{"location":"security/workload/#overview","title":"Overview","text":"<p>The Operator enforces a Secure-by-Default runtime environment:</p> <ol> <li>Pod Security: Strict non-root, read-only filesystem containers compliant with <code>Restricted</code> PSS.</li> <li>TLS: Automated certificate management for end-to-end encryption.</li> <li>Supply Chain: Cryptographic verification of container images to prevent tampering.</li> </ol>"},{"location":"security/workload/#topics","title":"Topics","text":"<ul> <li> <p> Pod Security</p> <p>Deep dive into SecurityContexts, ServiceAccount tokens, and Resource Limits.</p> <p> Container Hardening</p> </li> <li> <p> TLS Management</p> <p>Managing server TLS, mutual TLS (mTLS) for peers, and integration with Cert-Manager.</p> <p> TLS Configuration</p> </li> <li> <p> Supply Chain</p> <p>Verifying image signatures with Cosign and enforcing digest pinning.</p> <p> Image Verification</p> </li> </ul>"},{"location":"security/workload/#default-security-posture","title":"Default Security Posture","text":"<p>All OpenBao Pods are deployed with the following non-negotiable settings:</p> <pre><code>securityContext:\n  runAsUser: 1000\n  runAsGroup: 1000\n  runAsNonRoot: true\n  readOnlyRootFilesystem: true\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop: [\"ALL\"]\n  seccompProfile:\n    type: RuntimeDefault\n</code></pre>"},{"location":"security/workload/#see-also","title":"See Also","text":"<ul> <li> Security Favorites</li> <li> Infrastructure Security</li> </ul>"},{"location":"security/workload/supply-chain/","title":"Supply Chain Security","text":"<p>Immutable Assurance</p> <p>The Operator implements container image signature verification to protect against compromised registries, man-in-the-middle attacks, and TOCTOU (Time-of-Check to Time-of-Use) vulnerabilities.</p>"},{"location":"security/workload/supply-chain/#verification-flow","title":"Verification Flow","text":"<p>The Operator intercepts Pod creation requests and verifies the image signature against a trusted public key and (optionally) the Rekor transparency log.</p> <pre><code>flowchart LR\n    Registry[(Container Registry)]\n    Operator{{OpenBao Operator}}\n    Rekor[Rekor Log]\n    Cluster[Kubernetes Cluster]\n\n    Registry --\"1. Pull Image\"--&gt; Operator\n    Operator --\"2. Verify Signature\"--&gt; Operator\n    Operator -.-&gt;|\"3. Verify Log (Optional)\"| Rekor\n\n    Operator --\"4. Mutate to Digest\"--&gt; Cluster\n\n    %% Styling\n    classDef storage fill:transparent,stroke:#f59e0b,stroke-width:2px,color:#fff;\n    classDef logic fill:transparent,stroke:#9333ea,stroke-width:2px,color:#fff;\n    classDef external fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Registry,Cluster storage;\n    class Operator logic;\n    class Rekor external;</code></pre>"},{"location":"security/workload/supply-chain/#configuration","title":"Configuration","text":"Image Verification Digest Pinning <p>The Operator uses Cosign to verify signatures.</p> <ul> <li>Public Key: You provide the public key; the Operator uses it to verify signatures found in the registry.</li> <li>Private Registry: Supports <code>imagePullSecrets</code> for authenticated verification.</li> <li>Caching: Results are cached in-memory by digest to prevent performance impact.</li> </ul> <p>Private Keys</p> <p>The Operator never requires your private key. Signing happens in your CI/CD pipeline; the Operator only needs the public key for verification.</p> <pre><code>spec:\n  imageVerification:\n    enabled: true\n    publicKey: |\n      -----BEGIN PUBLIC KEY-----\n      ...\n      -----END PUBLIC KEY-----\n</code></pre> <p>To prevent TOCTOU (Time-of-Check to Time-of-Use) attacks, the Operator mutates image tags to immutable digests.</p> <ul> <li>Attack Vector: An attacker pushes a malicious image to <code>v1.2.3</code> after the admission controller checks it but before the Kubelet pulls it.</li> <li>Mitigation: The Operator resolves <code>openbao:v1.2.3</code> to <code>openbao@sha256:abc...</code> during verification and forces the Pod to use the digest.</li> </ul> <p>Immutability</p> <p>This ensures that the exact bits that were verified are the ones that run in your cluster.</p>"},{"location":"security/workload/supply-chain/#rekor-transparency-log","title":"Rekor Transparency Log","text":"<p>By default, the Operator verifies signatures against the Sigstore Rekor transparency log.</p> <ul> <li>Non-Repudiation: Ensures that the signature was actually created by the signer at a specific time.</li> <li>Auditability: Publicly meaningful event log of all signing activity.</li> </ul> <p>Air-Gapped Environments</p> <p>In disconnected environments where reaching the public Rekor log is impossible, you can disable this check: <pre><code>spec:\n  imageVerification:\n    ignoreTlog: true\n</code></pre></p>"},{"location":"security/workload/supply-chain/#failure-policies","title":"Failure Policies","text":"Policy Behavior Use Case Block (Default) Prevents the Pod from starting. Sets <code>ConditionDegraded=True</code>. Production environments requiring strict security. Warn Logs an error but allows the Pod to start using the original tag. Testing or during initial rollout of signing infrastructure."},{"location":"security/workload/supply-chain/#verified-workloads","title":"Verified Workloads","text":"<p>Verification applies to all images managed by the Operator:</p> Image Config Field Description OpenBao Server <code>spec.imageVerification</code> The main OpenBao binary Init Container <code>spec.operatorImageVerification</code> Helper for config rendering Backup/Restore Jobs <code>spec.operatorImageVerification</code> Snapshot executors Upgrade Jobs <code>spec.operatorImageVerification</code> Raft membership jobs"},{"location":"security/workload/supply-chain/#separate-signers-for-openbao-and-operator-images","title":"Separate Signers for OpenBao and Operator Images","text":"<p>The OpenBao main image (<code>openbao/openbao</code>) is signed by the OpenBao project, while helper images (init container, backup/restore executors, upgrade jobs) are signed by the operator project. Use <code>operatorImageVerification</code> to specify different signing credentials:</p> <pre><code>spec:\n  # Main OpenBao image (signed by openbao/openbao)\n  image: \"openbao/openbao:2.4.4\"\n  imageVerification:\n    enabled: true\n    issuer: \"https://token.actions.githubusercontent.com\"\n    subject: \"https://github.com/openbao/openbao/.github/workflows/release.yml@refs/tags/v2.4.4\"\n    failurePolicy: Block\n\n  # Operator images (signed by dc-tec/openbao-operator)\n  operatorImageVerification:\n    enabled: true\n    issuer: \"https://token.actions.githubusercontent.com\"\n    subject: \"https://github.com/dc-tec/openbao-operator/.github/workflows/release.yml@refs/tags/v1.2.4\"\n    failurePolicy: Block\n\n  initContainer:\n    image: \"ghcr.io/dc-tec/openbao-config-init:1.2.4\"\n  backup:\n    executorImage: \"ghcr.io/dc-tec/openbao-backup:1.1.0\"\n</code></pre> <p>No Fallback Behavior</p> <p><code>operatorImageVerification</code> and <code>imageVerification</code> are completely independent configurations. If <code>operatorImageVerification</code> is not configured, helper images are not verified (even if <code>imageVerification</code> is set). This prevents confusing failures when the main image and helper images have different signers.</p>"},{"location":"security/workload/tls/","title":"TLS &amp; Identity","text":"<p>Encryption in Transit</p> <p>The Operator ensures that all internal and external communication is encrypted via TLS. It supports three distinct modes of operation to fit different security architectures: Operator Managed, External, and ACME.</p>"},{"location":"security/workload/tls/#certificate-rotation-flow","title":"Certificate Rotation Flow","text":"<p>In the default Operator Managed mode, the operator handles the full lifecycle of the certificates, including rotation and hot-reloading.</p> <pre><code>sequenceDiagram\n    participant Time as Timer\n    participant Operator\n    participant Secret as K8s Secret\n    participant Pod as OpenBao Pod\n\n    Note over Time,Pod: Rotation Period Reached (e.g. 24h)\n\n    Time-&gt;&gt;Operator: Trigger Rotation\n    Operator-&gt;&gt;Operator: Generate New Cert + Key\n    Operator-&gt;&gt;Secret: Update TLS Secret\n\n    Note over Secret,Pod: Volume Watch Trigger\n\n    Secret-&gt;&gt;Pod: ConfigMap/Secret Update\n    Pod-&gt;&gt;Pod: Hot Reload (SIGHUP or Watcher)\n\n    Note right of Pod: New Cert Active (Zero Downtime)</code></pre>"},{"location":"security/workload/tls/#tls-modes","title":"TLS Modes","text":"Operator Managed (Default) External Provider ACME (Native) <p>This is the \"batteries included\" mode. The Operator acts as an internal Certificate Authority (CA).</p> <ul> <li>Automated PKI: Generates a self-signed Root CA and ephemeral leaf certificates.</li> <li>Strict Identity: Certificates use strict SANs (Subject Alternative Names) matching the Service and Pod DNS.</li> <li>Rotation: Automatically rotates certificates before expiry (configurable via <code>spec.tls.rotationPeriod</code>).</li> <li>Gateway Support: Automatically manages a CA ConfigMap for ingress controllers.</li> </ul> <pre><code>spec:\n  tls:\n    mode: OperatorManaged\n    rotationPeriod: 24h\n</code></pre> <p>In this mode, the Operator delegates certificate management to an external system, such as cert-manager or a corporate PKI.</p> <ul> <li>BYO-PKI: Integrates with existing infrastructure.</li> <li>Expectation: The Operator expects Secrets named <code>&lt;cluster&gt;-tls-ca</code> and <code>&lt;cluster&gt;-tls-server</code> to exist in the namespace.</li> <li>Hot Reload: The Operator monitors these Secrets and triggers hot-reloads when the external provider updates them.</li> </ul> <p>Cert-Manager Integration</p> <p>You can use <code>cert-manager</code> to issue certificates signed by Let's Encrypt or Vault, and the Operator will consume them seamlessly.</p> <pre><code>spec:\n  tls:\n    mode: External\n</code></pre> <p>OpenBao uses its built-in ACME client to fetch certificates directly from a provider like Let's Encrypt.</p> <ul> <li>Zero Trust: The Operator never sees the private key. It is generated in-memory by the OpenBao process.</li> <li>No Secrets: No Kubernetes Secrets are created for the server certificate.</li> <li>Automatic Rotation: OpenBao handles its own rotation via the ACME protocol.</li> </ul> <pre><code>spec:\n  tls:\n    mode: ACME\n    acme:\n      email: \"admin@example.com\"\n      domain: \"bao.example.com\"\n      directoryURL: \"https://acme-v02.api.letsencrypt.org/directory\"\n</code></pre>"},{"location":"security/workload/tls/#comparison-matrix","title":"Comparison Matrix","text":"Feature Operator Managed External Provider ACME (Native) Generator Operator (Internal CA) External (e.g., cert-manager) OpenBao (Built-in) Rotation Automatic External responsibility Automatic Private Key Kubernetes Secret Kubernetes Secret In-Memory (Secure) Best For Development, Simple Prod Enterprise PKI Integration Zero Trust, Public Facing"},{"location":"security/workload/tls/#see-also","title":"See Also","text":"<ul> <li> Pod Security</li> <li> Supply Chain</li> </ul>"},{"location":"security/workload/workload-security/","title":"Workload Security","text":"<p>Core Concept</p> <p>The Operator ensures that OpenBao pods run with Restricted Privileges by default. This minimizes the blast radius of a container escape and enforces isolation at the runtime level.</p>"},{"location":"security/workload/workload-security/#pod-security-context","title":"Pod Security Context","text":"<p>The <code>StatefulSet</code> creates pods with a hardened security context compliant with the Restricted Pod Security Standard.</p> Setting Value Purpose Run As User/Group <code>1000</code> Ensures non-root execution. Read-Only Root FS <code>true</code> Prevents filesystem tampering and immutable infrastructure violations. Capabilities <code>ALL</code> dropped Minimizes privilege escalation risks. Seccomp Profile <code>RuntimeDefault</code> Restricts available syscalls to the kernel. Privilege Escalation <code>false</code> Prevents setuid binaries from gaining root. <p>Volume Mounts</p> <p>Since the root filesystem is read-only, all mutable data (logs, storage, tmp) is written to explicit, size-limited volume mounts.</p>"},{"location":"security/workload/workload-security/#resource-guardrails","title":"Resource Guardrails","text":"<p>The Operator places default resource limits on ephemeral jobs to protect the node from \"noisy neighbor\" resource exhaustion.</p> <p>Default Job Limits</p> <p>These defaults ensure that a stuck backup job or an aggressive snapshot process doesn't starve the actual OpenBao pods (or other tenants) on the same node.</p> Job Type Resource Request Limit Backup / Restore CPU <code>100m</code> <code>500m</code> Memory <code>128Mi</code> <code>512Mi</code>"},{"location":"security/workload/workload-security/#serviceaccount-token-handling","title":"ServiceAccount Token Handling","text":"<p>The Operator minimizes the attack surface of the Kubernetes JWT token:</p> <ol> <li>No Automounting: <code>automountServiceAccountToken: false</code> is set on the Pod spec.</li> <li>Projected Volume: A short-lived, audience-bound token is projected only into the OpenBao container (not init containers).</li> </ol> <p>Token Usage:</p> <ul> <li>Peering: Used by <code>discover-k8s</code> to find other Raft peers.</li> <li>Registration: Used to update Pod labels (<code>openbao-active</code>, <code>openbao-sealed</code>) for service handling.</li> </ul>"},{"location":"security/workload/workload-security/#init-containers","title":"Init Containers","text":"<p>An init container (<code>bao-config-init</code>) is used to render the OpenBao configuration (<code>config.hcl</code>) at runtime.</p> <ul> <li>Purpose: Injects dynamic environment variables (Pod IP, Hostname) into the config template securely.</li> <li>Security: Runs with the exact same non-root restrictions (<code>1000:1000</code>) as the main container. It has no network access.</li> </ul>"},{"location":"security/workload/workload-security/#pod-security-standards-pss","title":"Pod Security Standards (PSS)","text":"<p>The Provisioner automatically applies PSS labels to any Tenant namespace it creates:</p> Label Value Enforcement <code>pod-security.kubernetes.io/enforce</code> <code>restricted</code> Hard Block <code>pod-security.kubernetes.io/audit</code> <code>restricted</code> Audit Log <code>pod-security.kubernetes.io/warn</code> <code>restricted</code> User Warning <p>Impact: Any workload deployed into a Tenant namespace (by the user or operator) MUST meet these strict standards or the API server will reject it. This prevents users from accidentally deploying insecure \"sidecar\" workloads alongside OpenBao.</p>"},{"location":"user-guide/","title":"OpenBao Operator \u2013 User Guide","text":"<p>This guide covers everything you need to know to successfully operate OpenBao on Kubernetes, from installation to Day 2 operations.</p> <ul> <li> <p> 1. Install Operator</p> <p>Deploy the operator using Helm or manifests. Supports Multi-Tenant (Default) and Single-Tenant modes.</p> <p> Installation  Single-Tenant Mode</p> </li> <li> <p> 2. Onboard Tenants</p> <p>Provision namespaces, RBAC, and quotas for teams using the <code>OpenBaoTenant</code> resource.</p> <p> Tenant Onboarding</p> </li> <li> <p> 3. Deploy Cluster</p> <p>Create and configure highly available OpenBao clusters with the <code>OpenBaoCluster</code> resource.</p> <p> Create Cluster</p> </li> <li> <p> 4. Operate &amp; Restore</p> <p>Manage backups, upgrades, resizing, and disaster recovery operations.</p> <p> Upgrades</p> <p> Backup Operations</p> <p> Restore Operations</p> </li> </ul>"},{"location":"user-guide/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Architecture \u2013 Understand the internal controller design.</li> <li>Security Model \u2013 Learn about the zero-trust security model.</li> <li>Troubleshooting \u2013 Production readiness and common issues.</li> </ul>"},{"location":"user-guide/openbaocluster/getting-started/","title":"Basic Cluster Creation","text":"<p>This guide walks you through creating your first OpenBaoCluster. Choose the path that matches your use case.</p>"},{"location":"user-guide/openbaocluster/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenBao Operator: Installed and running (see Installation)</li> <li>Storage Class: Default storage class configured in the cluster</li> </ul>"},{"location":"user-guide/openbaocluster/getting-started/#choose-your-path","title":"Choose Your Path","text":"Development (Local Testing)Production <p>For local development and testing. Not suitable for production.</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: dev-cluster\n  namespace: default\nspec:\n  version: \"2.4.4\"\n  image: \"openbao/openbao:2.4.4\"\n  replicas: 3\n  profile: Development\n  tls:\n    enabled: true\n    mode: OperatorManaged\n    rotationPeriod: \"720h\"\n  storage:\n    size: \"10Gi\"\n</code></pre> <p>Development Profile</p> <p>The <code>Development</code> profile uses static auto-unseal and stores sensitive  material in Kubernetes Secrets. This is convenient for testing but  insecure for production use.</p> <p>For production deployments with hardened security.</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: prod-cluster\n  namespace: openbao\nspec:\n  version: \"2.4.4\"\n  image: \"openbao/openbao:2.4.4\"\n  replicas: 3\n  profile: Hardened\n  tls:\n    enabled: true\n    mode: External\n  storage:\n    size: \"50Gi\"\n  selfInit:\n    enabled: true\n  unseal:\n    type: awskms\n    awskms:\n      region: us-east-1\n      kmsKeyID: alias/openbao-unseal\n</code></pre> <p>Production Checklist</p> <p>Before deploying to production, complete the Production Checklist  to ensure proper security configuration.</p>"},{"location":"user-guide/openbaocluster/getting-started/#apply-the-configuration","title":"Apply the Configuration","text":"<pre><code>kubectl apply -f cluster.yaml\n</code></pre>"},{"location":"user-guide/openbaocluster/getting-started/#verify-deployment","title":"Verify Deployment","text":"<p>Check the cluster status:</p> <pre><code>kubectl get openbaocluster &lt;name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Watch pods come up:</p> <pre><code>kubectl get pods -l openbao.org/cluster=&lt;name&gt; -n &lt;namespace&gt; -w\n</code></pre>"},{"location":"user-guide/openbaocluster/getting-started/#check-status-conditions","title":"Check Status Conditions","text":"<pre><code>kubectl describe openbaocluster &lt;name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Look for:</p> <ul> <li><code>status.phase</code> \u2014 Current lifecycle phase</li> <li><code>status.readyReplicas</code> \u2014 Number of ready replicas</li> <li><code>status.initialized</code> \u2014 <code>true</code> after cluster initialization</li> <li><code>status.conditions</code>:</li> <li><code>Available</code> \u2014 Cluster is serving requests</li> <li><code>TLSReady</code> \u2014 TLS certificates are valid</li> <li><code>ProductionReady</code> \u2014 Security requirements met (Hardened only)</li> <li><code>Degraded</code> \u2014 Issues detected</li> </ul>"},{"location":"user-guide/openbaocluster/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>External Access \u2014 Expose your cluster</li> <li>Security Profiles \u2014 Understand profile differences</li> <li>Backups \u2014 Configure disaster recovery</li> </ul>"},{"location":"user-guide/openbaocluster/next-steps/","title":"Next Steps","text":"<p>Once you have basic cluster creation working:</p> <ul> <li>Configure OpenBao server settings via <code>spec.configuration</code> for structured configuration (UI, logging, plugin, lease/TTL, cache, advanced features, listener, raft, ACME CA root).</li> <li>Configure audit devices via <code>spec.audit</code> for declarative audit logging.</li> <li>Configure plugins via <code>spec.plugins</code> for OCI-based plugin management.</li> <li>Configure telemetry via <code>spec.telemetry</code> for metrics and observability.</li> <li>Configure <code>spec.backup</code> and object storage to enable snapshot streaming (see the TDD for details).</li> <li>Integrate Operator metrics with your monitoring stack using the manifests under <code>config/prometheus</code>.</li> </ul> <p>For deeper architectural details and the full roadmap (upgrades, backups, multi-tenancy), refer to Architecture and Security.</p>"},{"location":"user-guide/openbaocluster/overview/","title":"OpenBaoCluster","text":"<p><code>OpenBaoCluster</code> is the primary Custom Resource Definition (CRD) that declaratively defines a production-ready OpenBao cluster on Kubernetes.</p> <p>It acts as a high-level abstraction over complex stateful infrastructure, managing the entire lifecycle of the cluster.</p>"},{"location":"user-guide/openbaocluster/overview/#capabilities","title":"Capabilities","text":"<ul> <li> <p> Secure Defaults</p> <p> Automatic TLS certificate management</p> <p> Secure-by-default configurations</p> <p> Security Profiles for hardening</p> </li> <li> <p> infrastructure</p> <p> Managed StatefulSets and Services</p> <p> Configurable Storage and PVCs</p> <p> Automated resizing (Vertical Scaling)</p> </li> <li> <p> Day 2 Operations</p> <p> Automated Upgrades (Rolling &amp; Blue/Green)</p> <p> Automated Backups to S3/GCS/Azure</p> <p> Breakdown/Recovery automation</p> </li> </ul>"},{"location":"user-guide/openbaocluster/overview/#gitops-architecture","title":"GitOps Architecture","text":"<p>The Operator follows a strict GitOps contract. Your Git repository is the source of truth for the <code>spec</code> (Desired State), while the Operator reports the <code>status</code> (Observed State).</p> <pre><code>flowchart LR\n    Git[\"fa:fa-code-branch Git (ArgoCD/Flux)\"] --&gt;|Sync| Spec[\"OpenBaoCluster.spec\\n(Desired State)\"]\n    Spec --&gt;|Reconcile| Controller[\"fa:fa-gears Operator Controller\"]\n\n    subgraph Cluster [\"Kubernetes Cluster\"]\n        Controller --&gt;|Manage| Infra[\"StatefulSet\\nServices\\nConfigMaps\\nSecrets\"]\n        Infra -.-&gt;|Report| Status[\"OpenBaoCluster.status\\n(Observed State)\"]\n    end\n\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n    classDef git fill:transparent,stroke:#f472b6,stroke-width:2px,color:#fff;\n\n    class Spec read;\n    class Status,Infra write;\n    class Git git;</code></pre>"},{"location":"user-guide/openbaocluster/overview/#configuration-examples","title":"Configuration Examples","text":"Minimal (Dev) Production (HA) <p>Start small for local development or testing.</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: dev-cluster\n  namespace: dev\nspec:\n  version: \"2.0.0\"\n  replicas: 1\n  description: \"Local dev cluster\"\n</code></pre> <p>A standard 3-node HA cluster with TLS and storage.</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: prod-cluster\n  namespace: security\nspec:\n  version: \"2.0.0\"\n  replicas: 3\n  description: \"Production HA Cluster\"\n\n  resources:\n    requests:\n      memory: \"1Gi\"\n      cpu: \"500m\"\n\n  storage:\n    size: \"10Gi\"\n    storageClass: \"gp3\"\n\n  tls:\n    enabled: true\n</code></pre>"},{"location":"user-guide/openbaocluster/overview/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Configuration</p> <p>Deep dive into customization options.</p> <p> Security Profiles</p> <p> Self-Initialization</p> </li> <li> <p> Operations</p> <p>Manage upgrades and disaster recovery.</p> <p> Upgrades</p> <p> Backups</p> <p> Recovery</p> </li> </ul>"},{"location":"user-guide/openbaocluster/security-considerations/","title":"Security Considerations","text":"<p>Securing an OpenBao cluster involves careful management of initialization tokens, unseal keys, and container integrity. This guide outlines critical configurations for a production-hardened deployment.</p>"},{"location":"user-guide/openbaocluster/security-considerations/#root-token-management","title":"Root Token Management","text":"<p>During cluster initialization (bootstrap), OpenBao generates an initial Root Token with unlimited privileges. Handling this token securely is critical.</p> <p>Root Token Risk</p> <p>By default, the Operator stores the root token in a Kubernetes Secret named <code>&lt;cluster&gt;-root-token</code>. This is convenient for development but risky for production.</p>"},{"location":"user-guide/openbaocluster/security-considerations/#recommended-self-initialization","title":"Recommended: Self-Initialization","text":"<p>For production environments, we strongly recommend enabling Self-Initialization.</p> <ul> <li>How it works: The Operator injects a one-time configuration to set up auth methods and policies immediately after initialization.</li> <li>Benefit: The root token is automatically revoked by OpenBao itself after setup is complete. It never persists in a Secret.</li> </ul> <p>Learn more about Self-Initialization</p>"},{"location":"user-guide/openbaocluster/security-considerations/#auto-unseal-configuration","title":"Auto-Unseal Configuration","text":"<p>OpenBao requires an \"unseal key\" to decrypt its master key on startup. You must choose a strategy for managing this key.</p> Cloud KMSOn-Prem / HybridDevelopment (Static) <p>Offload key management to a trusted cloud provider. This is the most secure option for cloud deployments.</p> AWS KMSGCP Cloud KMSAzure Key VaultOCI KMS <pre><code>spec:\n  unseal:\n    type: awskms\n    awskms:\n      kmsKeyID: \"arn:aws:kms:us-east-1:123456789012:key/...\"\n      region: \"us-east-1\"\n      # Optional: Use specific credentials if not using IRSA\n      # accessKey: \"...\"\n      # secretKey: \"...\"\n</code></pre> <pre><code>spec:\n  unseal:\n    type: gcpckms\n    gcpCloudKMS:\n      project: \"my-project\"\n      region: \"us-central1\"\n      keyRing: \"openbao-ring\"\n      cryptoKey: \"openbao-key\"\n      # Optional: Use specific credentials file if not using Workload Identity\n      # credentials: \"JSON_STRING_OR_PATH\"\n</code></pre> <pre><code>spec:\n  unseal:\n    type: azurekeyvault\n    azureKeyVault:\n      vaultName: \"my-vault\"\n      keyName: \"openbao-key\"\n      # Optional: Specific tenant/client config\n      # tenantID: \"...\"\n      # clientID: \"...\"\n</code></pre> <pre><code>spec:\n  unseal:\n    type: ocikms\n    ocikms:\n      keyID: \"ocid1.key.oc1...\"\n      cryptoEndpoint: \"https://&lt;unique&gt;.crypto.objectstorage.&lt;region&gt;.oci.customer-oci.com\"\n      managementEndpoint: \"https://&lt;unique&gt;.management.objectstorage.&lt;region&gt;.oci.customer-oci.com\"\n      authType: \"instance_principal\" # or \"user_principal\"\n</code></pre> <p>Use existing hardware security modules or a central OpenBao/Vault cluster.</p> Transit (Recommended)PKCS#11 (HSM)KMIP <p>Use another OpenBao cluster (the \"provider\") to unseal this cluster (the \"dependent\"). Ideally suited for multi-cluster management.</p> <pre><code>spec:\n  unseal:\n    type: transit\n    transit:\n      address: \"https://central-openbao.example.com\"\n      token: \"hvs.CAES...\"  # Token with 'update' on transit/encrypt/key\n      keyName: \"tenant-1-key\"\n      mountPath: \"transit\"\n      # Optional: TLS verification\n      # tlsSkipVerify: false\n</code></pre> <p>Connect to a physical Hardware Security Module (HSM).</p> <pre><code>spec:\n  unseal:\n    type: pkcs11\n    pkcs11:\n      lib: \"/usr/lib/libnotHSM.so\" # Path to vendor library\n      slot: \"0\"\n      pin: \"1234\"                  # User PIN\n      keyLabel: \"openbao-hsm-key\"\n      hmacKeyLabel: \"openbao-hsm-hmac-key\"\n      generateKey: true            # Generate if missing\n      mechanism: \"0x0000\"          # Optional specific mechanism\n</code></pre> <p>Connect to an enterprise Key Management Interoperability Protocol server.</p> <pre><code>spec:\n  unseal:\n    type: kmip\n    kmip:\n      address: \"10.0.0.5:5696\"\n      certificate: \"/etc/openbao/kmip/cert.pem\"\n      key: \"/etc/openbao/kmip/key.pem\"\n      caCert: \"/etc/openbao/kmip/ca.pem\"\n</code></pre> <p>Store the unseal key in a Kubernetes Secret.</p> <p>Production Risk</p> <p>This method stores the decryption key (<code>&lt;cluster&gt;-unseal-key</code>) alongside the encrypted data. If an attacker gains access to etcd or the namespace Secrets, they can decrypt the entire cluster.</p> <p>Requirements for safety: 1. Enable Etcd Encryption in your Kubernetes cluster. 2. Strictly limit RBAC access to Secrets.</p> <pre><code>spec:\n  unseal:\n    type: static  # Default\n</code></pre>"},{"location":"user-guide/openbaocluster/security-considerations/#supply-chain-security","title":"Supply Chain Security","text":"<p>To protect against compromised container registries, the Operator includes native support for Cosign image verification.</p> <p>Secure by Default</p> <p>The Operator verifies all images against the Rekor transparency log unless explicitly disabled.</p>"},{"location":"user-guide/openbaocluster/security-considerations/#enabling-verification","title":"Enabling Verification","text":"<p>Add the <code>imageVerification</code> block to your <code>OpenBaoCluster</code>. The Operator will block the startup of any Pods if the image signature cannot be verified against the public key.</p> <pre><code>spec:\n  imageVerification:\n    enabled: true\n    failurePolicy: Block  # \"Block\" or \"Warn\"\n    publicKey: |\n      -----BEGIN PUBLIC KEY-----\n      MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE...\n      ...\n      -----END PUBLIC KEY-----\n</code></pre>"},{"location":"user-guide/openbaocluster/security-considerations/#private-registries","title":"Private Registries","text":"<p>If your images are in a private registry, provide the necessary pull secrets:</p> <pre><code>spec:\n  imageVerification:\n    enabled: true\n    publicKey: | ... |\n    imagePullSecrets:\n      - name: my-registry-creds\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/external-access/","title":"External Access","text":"<p>OpenBao clusters can be exposed using Gateway API (Recommended), Ingress, or standard LoadBalancer services.</p>"},{"location":"user-guide/openbaocluster/configuration/external-access/#access-methods","title":"Access Methods","text":"Gateway API (Recommended)IngressService (L4) <p>The Operator provides first-class support for Kubernetes Gateway API, offering advanced routing, portability, and cleaner multi-tenancy.</p> <p>Full Guide</p> <p>See the Gateway API Guide for complete configuration details, including TLS Passthrough and backend policies.</p> <pre><code>spec:\n  gateway:\n    enabled: true\n    hostname: bao.example.com\n    gatewayRef:\n      name: main-gateway\n      namespace: gateway-system\n</code></pre> <p>Standard Kubernetes Ingress support.</p> <pre><code>spec:\n  ingress:\n    enabled: true\n    host: \"bao.example.com\"\n    annotations:\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n</code></pre> <p>Traefik v3</p> <p>Traefik v3 requires a <code>ServersTransport</code> to trust the internal CA. See the Traefik v3 Configuration section below.</p> <p>Expose the cluster directly via a LoadBalancer or NodePort service.</p> <pre><code>spec:\n  service:\n    type: LoadBalancer\n    annotations:\n      service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/external-access/#tls-configuration","title":"TLS Configuration","text":"<p>Secure your cluster using one of the following TLS modes.</p> ACME (Let's Encrypt)External PKIOperator Managed <p>Zero-Trust: OpenBao acts as a native ACME client (e.g., Let's Encrypt), managing its own certificates without mounting Secrets.</p> <pre><code>spec:\n  tls:\n    enabled: true\n    mode: ACME\n    acme:\n      directoryURL: \"https://acme-v02.api.letsencrypt.org/directory\"\n      domain: \"bao.example.com\"\n      email: \"admin@example.com\"\n</code></pre> <p>BYO-Cert: Integrate with <code>cert-manager</code> or corporate PKI. You provide the Secrets; the Operator uses them.</p> <pre><code>spec:\n  tls:\n    enabled: true\n    mode: External\n</code></pre> <p>Requirements: - Secret <code>&lt;name&gt;-tls-ca</code>: Keys <code>ca.crt</code> (optional <code>ca.key</code>) - Secret <code>&lt;name&gt;-tls-server</code>: Keys <code>tls.crt</code>, <code>tls.key</code>, <code>ca.crt</code></p> <p>Default: The Operator manages an internal CA and rotates certificates automatically. Good for internal use or testing.</p> <pre><code>spec:\n  tls:\n    enabled: true\n    # mode defaults to OperatorManaged\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/external-access/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"user-guide/openbaocluster/configuration/external-access/#traefik-v3-configuration","title":"Traefik v3 Configuration","text":"<p>Traefik v3 enforces potential CA validation for backends. The Operator creates a Secret named <code>&lt;cluster&gt;-tls-ca</code> which Traefik can reference directly in a <code>ServersTransport</code>.</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: ServersTransport\nmetadata:\n  name: openbao-tls-transport\nspec:\n  rootCAsSecrets:\n    - my-cluster-tls-ca\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/gateway-api/","title":"Gateway API Support","text":"<p>The Operator provides first-class support for Kubernetes Gateway API, enabling standardized, portable, and expressive external access.</p>"},{"location":"user-guide/openbaocluster/configuration/gateway-api/#architecture","title":"Architecture","text":"<p>The Operator supports two primary modes: Termination (HTTPS at Gateway) and Passthrough (End-to-End Encryption).</p> <pre><code>flowchart TB\n    subgraph Term[\"TLS Termination&lt;br&gt;(HTTPRoute)\"]\n        direction TB\n        Ext1[Client] --&gt;|\"HTTPS (443)\"| GW1[Gateway]\n        GW1 -- \"Re-encrypted HTTPS\" --&gt; Bao1[OpenBao]\n    end\n\n    subgraph Pass[\"TLS Passthrough&lt;br&gt;(TLSRoute)\"]\n        direction TB\n        Ext2[Client] --&gt;|\"HTTPS (SNI)\"| GW2[Gateway]\n        GW2 -- \"Encrypted TLS\" --&gt; Bao2[OpenBao]\n    end\n\n    style Term fill:transparent,stroke:#00e676,stroke-width:2px,color:#fff\n    style Pass fill:transparent,stroke:#2979ff,stroke-width:2px,color:#fff\n    style GW1 fill:transparent,stroke:#ffa726\n    style GW2 fill:transparent,stroke:#ffa726\n    style Bao1 fill:transparent,stroke:#ab47bc\n    style Bao2 fill:transparent,stroke:#ab47bc</code></pre>"},{"location":"user-guide/openbaocluster/configuration/gateway-api/#configuration","title":"Configuration","text":"<p>Choose your deployment mode.</p> TLS Termination (Default)TLS Passthrough <p>Best for: Standard web traffic, WAF integration, Certificate management at Gateway.</p> <p>The Gateway terminates TLS, and the Operator (optionally) configures a secure link to the backend.</p> <pre><code>spec:\n  gateway:\n    enabled: true\n    hostname: bao.example.com\n    gatewayRef:\n      name: main-gateway\n      namespace: gateway-system\n</code></pre> <p>What happens: 1. Operator creates an <code>HTTPRoute</code> referencing the Gateway. 2. Operator creates a <code>BackendTLSPolicy</code> to encrypt traffic between the Gateway and OpenBao (re-encryption).</p> Generated BackendTLSPolicy <p>The operator automatically creates a policy to validate the OpenBao backend certificate:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: BackendTLSPolicy\nmetadata:\n  name: my-cluster-backend-tls\nspec:\n  targetRefs:\n    - kind: Service\n      name: my-cluster-public\n  validation:\n    caCertificateRefs:\n      - kind: ConfigMap\n        name: my-cluster-tls-ca\n    hostname: my-cluster-public.default.svc\n</code></pre> <p>Best for: Zero Trust, End-to-End Encryption, Compliance, Client Cert Auth.</p> <p>The Gateway routes traffic based on SNI without decrypting it. OpenBao terminates TLS.</p> <pre><code>spec:\n  gateway:\n    enabled: true\n    tlsPassthrough: true  # Enables TLSRoute\n    hostname: bao.example.com\n    gatewayRef:\n      name: main-gateway\n      namespace: gateway-system\n</code></pre> <p>Requirements: - Gateway Listener must be in <code>Passthrough</code> mode. - <code>TLSRoute</code> CRD must be installed (often Experimental channel).</p>"},{"location":"user-guide/openbaocluster/configuration/gateway-api/#comparison-reference","title":"Comparison Reference","text":"Feature Ingress Gateway API (HTTPRoute) Gateway API (TLSRoute) Routing Path/Host Header, Path, Method, Query SNI (Host) TLS Terminate Only Terminate Terminate or Passthrough Multi-Tenancy Weak Strong (Namespace-scoped routes) Strong Resource <code>Ingress</code> <code>HTTPRoute</code> <code>TLSRoute</code>"},{"location":"user-guide/openbaocluster/configuration/gateway-api/#advanced-options","title":"Advanced Options","text":"Field Description Default <code>gateway.backendTLS.enabled</code> Auto-create <code>BackendTLSPolicy</code> for secure internal hop. <code>true</code> <code>gateway.backendTLS.hostname</code> Override hostname for internal validation. Service DNS <code>gateway.listenerName</code> Attach generated Route to a specific Gateway listener (sectionName), e.g. <code>websecure</code>. All matching listeners <code>gateway.annotations</code> Custom annotations for the generated Route. None"},{"location":"user-guide/openbaocluster/configuration/gateway-api/#bluegreen-upgrade-integration","title":"Blue/Green Upgrade Integration","text":"<p>When combining Gateway API with Blue/Green upgrades, the Operator keeps the generated <code>HTTPRoute</code> stable by targeting the cluster's main external Service (<code>&lt;cluster&gt;-public</code>).</p> <p>During cutover, the operator updates that Service's selector to point at the Green revision.</p>"},{"location":"user-guide/openbaocluster/configuration/network/","title":"Network Configuration","text":"<p>OpenBao Operator automatically configures Kubernetes NetworkPolicies to secure your cluster by default using a \"Deny All\" + \"Allow Essential\" strategy.</p>"},{"location":"user-guide/openbaocluster/configuration/network/#default-topology","title":"Default Topology","text":"<p>The following diagram illustrates the allowed traffic flows.</p> <pre><code>flowchart TB\n    subgraph External[\"External World\"]\n        GW[Gateway / Ingress]\n        Client[Clients]\n    end\n\n    subgraph Cluster[\"Kubernetes Cluster\"]\n        API[K8s API]\n        DNS[CoreDNS]\n\n        subgraph OperatorNS[\"Operator Namespace\"]\n            Op[Operator]\n        end\n\n        subgraph TenantNS[\"Tenant Namespace\"]\n            Bao[OpenBao Pods]\n        end\n    end\n\n    %% Ingress Flows\n    GW &amp; Op --&gt;|\"HTTPS (8200)\"| Bao\n    Client -.-&gt;|\"HTTPS (443)\"| GW\n\n    %% Internal Flows\n    Bao &lt;--&gt;|\"Raft (8201)\"| Bao\n\n    %% Egress Flows\n    Bao --&gt;|\"DNS (53)\"| DNS\n    Bao --&gt;|\"K8s (443/6443)\"| API\n\n    %% Styling\n    style Bao fill:transparent,stroke:#00e676,stroke-width:2px\n    style Op fill:transparent,stroke:#2979ff,stroke-width:2px\n    style GW fill:transparent,stroke:#ffa726,stroke-width:2px\n    style API fill:transparent,stroke:#ab47bc,stroke-width:2px</code></pre>"},{"location":"user-guide/openbaocluster/configuration/network/#default-rules-reference","title":"Default Rules Reference","text":"<p>The Operator ensures these rules always exist to keep the cluster functional.</p> Direction Source / Dest Port Purpose Ingress Operator <code>8200</code> Health checks, Initialization, Unsealing. Ingress Self <code>8201</code> Raft consensus replication between peers. Ingress Gateway/Ingress <code>8200</code> External traffic (if Ingress/Gateway is enabled). Ingress Kube-System Any Readiness probes (often from kubelet/monitoring). Egress Kube-DNS <code>53</code> Service discovery. Egress K8s API <code>443</code> Kubernetes Auth Method validation. Egress Self <code>8201</code> Raft consensus replication."},{"location":"user-guide/openbaocluster/configuration/network/#custom-rules-advanced","title":"Custom Rules (Advanced)","text":"<p>You can append additional rules to the default policy to allow integrations like backups or monitoring.</p> Egress RulesIngress Rules <p>Allow OpenBao to connect to external services (e.g., Transit Vault, S3, Databases).</p> <pre><code>spec:\n  network:\n    egressRules:\n      # Example: Allow access to Transit Vault in operator namespace\n      - to:\n          - namespaceSelector:\n              matchLabels:\n                kubernetes.io/metadata.name: openbao-operator-system\n        ports:\n          - protocol: TCP\n            port: 8200\n\n      # Example: Allow access to S3 CIDR for Backups\n      - to:\n          - ipBlock:\n              cidr: 192.168.100.0/24\n        ports:\n          - protocol: TCP\n            port: 443\n</code></pre> <p>Allow external monitors to scrape OpenBao metrics.</p> <pre><code>spec:\n  network:\n    ingressRules:\n      # Example: Allow Prometheus from monitoring namespace\n      - from:\n          - namespaceSelector:\n              matchLabels:\n                kubernetes.io/metadata.name: monitoring\n        ports:\n          - protocol: TCP\n            port: 8200\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/network/#advanced-routing","title":"Advanced Routing","text":"<p>Configuring how OpenBao reaches the Kubernetes API server for Auth Method validation.</p> Auto-Detection (Default)Manual CIDREndpoint IPs <p>The Operator attempts to automatically detect the API Server CIDR by reading the <code>kubernetes</code> Service in the <code>default</code> namespace.</p> <p>Prerequisite: The Operator ServiceAccount must have permission to read Services in <code>default</code>.</p> <p>Use Case: Restricted environments where the Operator strictly cannot read <code>default</code> namespace.</p> <pre><code>spec:\n  network:\n    # E.g., EKS (10.100.0.0/16) or GKE (10.0.0.0/16)\n    apiServerCIDR: \"10.43.0.0/16\"\n</code></pre> <p>Use Case: Local clusters (Kind/k3d/minikube) where CNI enforcement happens after NAT (Post-NAT destination), meaning the destination IP is the API Server IP, not the Service CIDR.</p> <pre><code>spec:\n  network:\n    apiServerEndpointIPs:\n      - \"192.168.166.2\" # The IP of the API Server container/node\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/","title":"Observability","text":"<p>The OpenBao Operator exposes comprehensive metrics, structured logs, and health endpoints to integrate with your existing monitoring stack.</p>"},{"location":"user-guide/openbaocluster/configuration/observability/#metrics","title":"Metrics","text":"<p>The Operator exposes Prometheus metrics on port <code>8080</code> by default (configurable via Helm).</p>"},{"location":"user-guide/openbaocluster/configuration/observability/#enabling-metrics-scraping","title":"Enabling Metrics Scraping","text":"Prometheus Operator (ServiceMonitor)Prometheus Annotations <p>The Helm chart can create a <code>ServiceMonitor</code> automatically:</p> <pre><code># values.yaml\nmetrics:\n  enabled: true\n  port: 8080\n  serviceMonitor:\n    enabled: true\n    interval: 30s\n    scrapeTimeout: 10s\n</code></pre> <p>If not using the Prometheus Operator, annotate the service:</p> <pre><code># values.yaml\nmetrics:\n  enabled: true\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"8080\"\n    prometheus.io/path: \"/metrics\"\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#available-metrics","title":"Available Metrics","text":""},{"location":"user-guide/openbaocluster/configuration/observability/#reconciliation-metrics","title":"Reconciliation Metrics","text":"Metric Type Labels Description <code>openbao_reconcile_duration_seconds</code> Histogram <code>namespace</code>, <code>name</code>, <code>controller</code> Duration of reconciliation loops <code>openbao_reconcile_errors_total</code> Counter <code>namespace</code>, <code>name</code>, <code>controller</code>, <code>reason</code> Total reconciliation errors <p>Alerting on Reconciliation Errors</p> <p>Alert when the error rate exceeds a threshold:</p> <pre><code>rate(openbao_reconcile_errors_total[5m]) &gt; 0.1\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#cluster-state-metrics","title":"Cluster State Metrics","text":"Metric Type Labels Description <code>openbao_cluster_ready_replicas</code> Gauge <code>namespace</code>, <code>name</code> Number of ready replicas <code>openbao_cluster_phase</code> Gauge <code>namespace</code>, <code>name</code>, <code>phase</code> Current cluster phase (1 = active) <p>The <code>phase</code> label takes one of these values:</p> <ul> <li><code>Initializing</code> - Cluster is starting up</li> <li><code>Running</code> - Cluster is healthy</li> <li><code>Upgrading</code> - Upgrade in progress</li> <li><code>BackingUp</code> - Backup in progress</li> <li><code>Failed</code> - Cluster is in a failed state</li> </ul> <p>Cluster Availability</p> <p>Alert when ready replicas drop below expected:</p> <pre><code>openbao_cluster_ready_replicas &lt; 3\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#backup-metrics","title":"Backup Metrics","text":"Metric Type Labels Description <code>openbao_backup_total</code> Counter <code>namespace</code>, <code>name</code>, <code>type</code> Backups attempted <code>openbao_backup_success_total</code> Counter <code>namespace</code>, <code>name</code>, <code>type</code> Successful backups <code>openbao_backup_failure_total</code> Counter <code>namespace</code>, <code>name</code>, <code>type</code> Failed backups <code>openbao_backup_duration_seconds</code> Histogram <code>namespace</code>, <code>name</code>, <code>type</code> Backup duration <code>openbao_backup_last_success_timestamp</code> Gauge <code>namespace</code>, <code>name</code> Unix timestamp of last success <code>openbao_backup_size_bytes</code> Gauge <code>namespace</code>, <code>name</code> Size of last backup <p>The <code>type</code> label indicates the backup trigger:</p> <ul> <li><code>scheduled</code> - Cron-scheduled backup</li> <li><code>manual</code> - User-triggered backup</li> <li><code>pre-upgrade</code> - Automatic backup before upgrade</li> </ul> <p>Backup Staleness Alert</p> <p>Alert if backups are older than 24 hours:</p> <pre><code>time() - openbao_backup_last_success_timestamp &gt; 86400\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#restore-metrics","title":"Restore Metrics","text":"Metric Type Labels Description <code>openbao_restore_total</code> Counter <code>namespace</code>, <code>name</code> Restore operations attempted <code>openbao_restore_success_total</code> Counter <code>namespace</code>, <code>name</code> Successful restores <code>openbao_restore_failure_total</code> Counter <code>namespace</code>, <code>name</code> Failed restores <code>openbao_restore_duration_seconds</code> Histogram <code>namespace</code>, <code>name</code> Restore duration"},{"location":"user-guide/openbaocluster/configuration/observability/#upgrade-metrics","title":"Upgrade Metrics","text":"Metric Type Labels Description <code>openbao_upgrade_total</code> Counter <code>namespace</code>, <code>name</code>, <code>strategy</code> Upgrades initiated <code>openbao_upgrade_success_total</code> Counter <code>namespace</code>, <code>name</code>, <code>strategy</code> Successful upgrades <code>openbao_upgrade_failure_total</code> Counter <code>namespace</code>, <code>name</code>, <code>strategy</code> Failed upgrades <code>openbao_upgrade_rollback_total</code> Counter <code>namespace</code>, <code>name</code>, <code>strategy</code> Rollbacks triggered <code>openbao_upgrade_duration_seconds</code> Histogram <code>namespace</code>, <code>name</code>, <code>strategy</code> Upgrade duration <p>The <code>strategy</code> label is either <code>RollingUpdate</code> or <code>BlueGreen</code>.</p> <p>Upgrade Rollback Monitoring</p> <p>Track rollback frequency to identify problematic upgrades:</p> <pre><code>increase(openbao_upgrade_rollback_total[7d]) &gt; 0\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#drift-detection-metrics","title":"Drift Detection Metrics","text":"Metric Type Labels Description <code>openbao_drift_detected_total</code> Counter <code>namespace</code>, <code>name</code>, <code>resource_kind</code> Drift events detected <code>openbao_drift_corrected_total</code> Counter <code>namespace</code>, <code>name</code> Drift events corrected <code>openbao_drift_last_detected_timestamp</code> Gauge <code>namespace</code>, <code>name</code> Last drift detection time <p>Drift Indicates External Modifications</p> <p>High drift counts may indicate unauthorized changes or conflicting controllers. Investigate the <code>resource_kind</code> label to identify the source.</p>"},{"location":"user-guide/openbaocluster/configuration/observability/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>A pre-built Grafana dashboard is included with the Operator.</p>"},{"location":"user-guide/openbaocluster/configuration/observability/#installation","title":"Installation","text":"Kubernetes ConfigMapManual Import <p>Apply the dashboard as a ConfigMap for Grafana sidecar discovery:</p> <pre><code>kubectl apply -f config/grafana/\n</code></pre> <ol> <li>Open Grafana and navigate to Dashboards &gt; Import.</li> <li>Upload <code>config/grafana/dashboard.json</code>.</li> <li>Select your Prometheus data source.</li> </ol>"},{"location":"user-guide/openbaocluster/configuration/observability/#dashboard-panels","title":"Dashboard Panels","text":"<p>The dashboard includes:</p> Section Panels Overview Upgrade Status, Backup Status, Ready Replicas Reconciliation Duration (p50/p95/p99), Error Rate by Controller Backups Success/Failure Rate, Duration, Size, Last Success Upgrades Duration, Step-Down Operations, Progress Restores Success/Failure Rate, Duration Drift Detection &amp; Correction Rate TLS Certificate Expiry, Rotation Count"},{"location":"user-guide/openbaocluster/configuration/observability/#logging","title":"Logging","text":"<p>The Operator emits structured JSON logs with consistent fields for log aggregation.</p>"},{"location":"user-guide/openbaocluster/configuration/observability/#log-format","title":"Log Format","text":"<pre><code>{\n  \"level\": \"info\",\n  \"ts\": \"2024-01-15T10:30:00.000Z\",\n  \"logger\": \"openbaocluster\",\n  \"msg\": \"Reconciliation complete\",\n  \"cluster_name\": \"prod-cluster\",\n  \"cluster_namespace\": \"vault\",\n  \"controller\": \"openbaocluster\",\n  \"reconcileID\": \"abc123\"\n}\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#key-log-fields","title":"Key Log Fields","text":"Field Description <code>cluster_name</code> Name of the OpenBaoCluster <code>cluster_namespace</code> Namespace of the cluster <code>controller</code> Controller processing the event <code>reconcileID</code> Unique ID for correlating log entries"},{"location":"user-guide/openbaocluster/configuration/observability/#log-levels","title":"Log Levels","text":"<p>Configure the log level via Helm:</p> <pre><code># values.yaml\ncontroller:\n  args:\n    - --zap-log-level=info  # debug, info, error\n</code></pre> <p>Debug Logging</p> <p>Enable debug logging temporarily for troubleshooting:</p> <pre><code>controller:\n  args:\n    - --zap-log-level=debug\n    - --zap-stacktrace-level=error\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#example-log-queries","title":"Example Log Queries","text":"Loki (LogQL)Elasticsearch <pre><code>{namespace=\"openbao-operator\"} \n| json \n| cluster_name=\"prod-cluster\" \n| level=\"error\"\n</code></pre> <pre><code>{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"match\": { \"cluster_name\": \"prod-cluster\" } },\n        { \"match\": { \"level\": \"error\" } }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#health-probes","title":"Health Probes","text":"<p>The Operator exposes health endpoints for Kubernetes probes.</p>"},{"location":"user-guide/openbaocluster/configuration/observability/#endpoints","title":"Endpoints","text":"Endpoint Purpose Port <code>/healthz</code> Liveness probe 8081 <code>/readyz</code> Readiness probe 8081"},{"location":"user-guide/openbaocluster/configuration/observability/#configuring-probes","title":"Configuring Probes","text":"<pre><code># values.yaml\nhealthProbes:\n  port: 8081\n  livenessInitialDelaySeconds: 15\n  livenessPeriodSeconds: 20\n  readinessInitialDelaySeconds: 5\n  readinessPeriodSeconds: 10\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#recommended-alerts","title":"Recommended Alerts","text":"<p>Here are production-ready alert rules for the OpenBao Operator:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: openbao-operator-alerts\nspec:\n  groups:\n    - name: openbao-operator\n      rules:\n        # Cluster availability\n        - alert: OpenBaoClusterDegraded\n          expr: openbao_cluster_ready_replicas &lt; 3\n          for: 5m\n          labels:\n            severity: warning\n          annotations:\n            summary: \"OpenBao cluster {{ $labels.name }} has fewer than 3 ready replicas\"\n\n        - alert: OpenBaoClusterDown\n          expr: openbao_cluster_ready_replicas == 0\n          for: 1m\n          labels:\n            severity: critical\n          annotations:\n            summary: \"OpenBao cluster {{ $labels.name }} has no ready replicas\"\n\n        # Backup health\n        - alert: OpenBaoBackupStale\n          expr: time() - openbao_backup_last_success_timestamp &gt; 86400\n          for: 1h\n          labels:\n            severity: warning\n          annotations:\n            summary: \"OpenBao cluster {{ $labels.name }} has not had a successful backup in 24+ hours\"\n\n        - alert: OpenBaoBackupFailing\n          expr: rate(openbao_backup_failure_total[1h]) &gt; 0\n          for: 30m\n          labels:\n            severity: warning\n          annotations:\n            summary: \"OpenBao cluster {{ $labels.name }} backups are failing\"\n\n        # Reconciliation health\n        - alert: OpenBaoReconcileErrors\n          expr: rate(openbao_reconcile_errors_total[5m]) &gt; 0.5\n          for: 10m\n          labels:\n            severity: warning\n          annotations:\n            summary: \"OpenBao operator experiencing high reconciliation error rate\"\n\n        # Drift detection\n        - alert: OpenBaoExcessiveDrift\n          expr: rate(openbao_drift_detected_total[1h]) &gt; 10\n          for: 30m\n          labels:\n            severity: warning\n          annotations:\n            summary: \"OpenBao cluster {{ $labels.name }} experiencing excessive configuration drift\"\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/observability/#openbao-server-metrics","title":"OpenBao Server Metrics","text":"<p>In addition to Operator metrics, OpenBao itself exposes telemetry.</p>"},{"location":"user-guide/openbaocluster/configuration/observability/#enabling-openbao-telemetry","title":"Enabling OpenBao Telemetry","text":"<p>Configure telemetry in the cluster spec:</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: prod-cluster\nspec:\n  telemetry:\n    prometheusRetentionTime: \"30s\"\n    disableHostname: true\n</code></pre> <p>This exposes OpenBao metrics at <code>/v1/sys/metrics</code> on the OpenBao pods.</p> <p>Separate Scrape Config</p> <p>OpenBao server metrics require a separate scrape configuration targeting the OpenBao pods directly, not the Operator.</p>"},{"location":"user-guide/openbaocluster/configuration/resources-storage/","title":"Managed Resources","text":""},{"location":"user-guide/openbaocluster/configuration/resources-storage/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenBaoCluster: An active <code>OpenBaoCluster</code> CR.</li> </ul>"},{"location":"user-guide/openbaocluster/configuration/resources-storage/#overview","title":"Overview","text":"<p>The Operator creates and manages a set of Kubernetes resources to support the OpenBao cluster.</p> <pre><code>graph TD\n    CR[OpenBaoCluster] --&gt;|Owns| SS[StatefulSet]\n    CR --&gt;|Owns| SVC[Service]\n    CR --&gt;|Owns| CM[ConfigMap]\n    CR --&gt;|Owns| SEC[Secrets]\n    CR --&gt;|Owns| NP[NetworkPolicy]\n    SS --&gt;|Mounts| PVC[PVC: data]\n    SS --&gt;|Mounts| CM\n    SS --&gt;|Mounts| SEC</code></pre>"},{"location":"user-guide/openbaocluster/configuration/resources-storage/#core-workload","title":"Core Workload","text":"Resource Type Name Pattern Description StatefulSet <code>&lt;cluster&gt;</code> Manages the Pods. Mounts config, secrets, and data PVCs. Service <code>&lt;cluster&gt;</code> Headless Service (ClusterIP <code>None</code>) for stable network identity. ConfigMap <code>&lt;cluster&gt;-config</code> Contains rendered <code>config.hcl</code> and <code>init.sh</code> scripts. Service <code>&lt;cluster&gt;-public</code> (Optional) Created if <code>spec.service</code> or <code>spec.ingress</code> is enabled. PVC <code>data-&lt;cluster&gt;-*</code> Persistent volume for Raft storage, sized by <code>spec.storage.size</code>."},{"location":"user-guide/openbaocluster/configuration/resources-storage/#security-identity","title":"Security &amp; Identity","text":"<p>The Operator manages credentials, certificates, and tokens based on the cluster configuration.</p>"},{"location":"user-guide/openbaocluster/configuration/resources-storage/#tls-configuration","title":"TLS Configuration","text":"Operator Managed (Default)ExternalACME <p>When <code>spec.tls.mode</code> is <code>OperatorManaged</code> or omitted:</p> Secret Name Description <code>&lt;cluster&gt;-tls-ca</code> Root CA (<code>ca.crt</code>, <code>ca.key</code>). Generated and managed by the Operator. <code>&lt;cluster&gt;-tls-server</code> Server certificates (<code>tls.crt</code>, <code>tls.key</code>, <code>ca.crt</code>). Generated and managed by the Operator. <p>When <code>spec.tls.mode</code> is <code>External</code>:</p> Secret Name Requirements <code>&lt;cluster&gt;-tls-ca</code> User Provided. Must contain <code>ca.crt</code>. <code>&lt;cluster&gt;-tls-server</code> User Provided. Must contain <code>tls.crt</code>, <code>tls.key</code>, and <code>ca.crt</code>. <p>When <code>spec.tls.mode</code> is <code>ACME</code>:</p> Resource Description <code>&lt;cluster&gt;-tls-ca</code> User Provided (optional). Helper CA for clients (e.g. Let's Encrypt Root) if not in system store. Internal Certificates are requested and managed internally by OpenBao's ACME agent."},{"location":"user-guide/openbaocluster/configuration/resources-storage/#operational-secrets","title":"Operational Secrets","text":"<ul> <li>Unseal Key: <code>&lt;cluster&gt;-unseal-key</code></li> <li>Contains the 32-byte raw unseal key.</li> <li>Created only if <code>spec.unseal.type</code> is <code>static</code> (default).</li> <li>Root Token: <code>&lt;cluster&gt;-root-token</code></li> <li>Contains the initial root token after initialization.</li> <li>Note: Not created if Self-Initialization is used.</li> </ul>"},{"location":"user-guide/openbaocluster/configuration/resources-storage/#network-security","title":"Network Security","text":"<p>The Operator enforces a Zero Trust network model using a default <code>NetworkPolicy</code>.</p>"},{"location":"user-guide/openbaocluster/configuration/resources-storage/#default-policies","title":"Default Policies","text":"Direction Source / Destination Port Purpose Ingress Any - Deny All (Implicit) Ingress Within Cluster Matches <code>openbao.org/cluster=&lt;name&gt;</code> Any Intra-cluster Raft replication &amp; forwarding. Ingress Namespace <code>kube-system</code> Any DNS resolution &amp; Kubelet probes. Ingress OpenBao Operator 8200 Leader step-down &amp; health checks. Ingress Gateway Namespace Any (If Gateway API enabled) Ingress traffic. Egress CoreDNS 53 (UDP/TCP) Service Discovery. Egress K8s API Server 443 Kubernetes Auto-Join discovery. Egress Within Cluster 8200-8201 Raft Replication. <p>Customization</p> <p>You can append custom rules via <code>spec.network.ingressRules</code> and <code>spec.network.egressRules</code>. The Operator's default rules cannot be disabled. See Network Configuration for details.</p> <p>Backup Jobs</p> <p>Backup job pods (<code>openbao.org/component=backup</code>) are excluded from this NetworkPolicy to ensure they can access external Object Storage (S3/GCS/Azure). You may need to create a dedicated NetworkPolicy for backup jobs if you require strict egress filtering.</p>"},{"location":"user-guide/openbaocluster/configuration/resources-storage/#inspection","title":"Inspection","text":"<p>You can inspect the generated resources for a cluster named <code>dev-cluster</code> in namespace <code>security</code> with:</p> <pre><code>kubectl -n security get statefulsets,svc,cm,secrets,netpol -l openbao.org/cluster=dev-cluster\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/security-profiles/","title":"Security Profiles","text":"<p>Configure the security posture of your OpenBao cluster.</p> <p>Production Readiness</p> <p>Always use the <code>Hardened</code> profile for production deployments. The <code>Development</code> profile creates root tokens and stores them in Kubernetes Secrets, which is a critical security risk.</p>"},{"location":"user-guide/openbaocluster/configuration/security-profiles/#profile-comparison","title":"Profile Comparison","text":"<p>The Operator supports two distinct security profiles via <code>spec.profile</code>.</p> Feature Development Hardened (Production) Use Case Local Testing, POC Production Workloads Root Token Created &amp; Stored in Secret Never Created Unseal Static (Kubernetes Secret) External KMS (AWS, GCP, Azure, etc.) TLS Optional / Self-Signed Mandatory (External or ACME) Status <code>ConditionSecurityRisk=True</code> Secure by Default <pre><code>flowchart LR\n    Cluster[\"OpenBaoCluster\"]\n    Dev[\"Development Profile\"]\n    Hard[\"Hardened Profile\"]\n\n    Cluster --&gt;|spec.profile| Dev\n    Cluster --&gt;|spec.profile| Hard\n\n    Dev -.-&gt;|Risk| RootToken[Root Token Created]\n    Hard -.-&gt;|Secure| NoRoot[No Root Token]\n    Hard -.-&gt;|Secure| KMS[External KMS Unseal]\n    Hard -.-&gt;|Secure| EXT_TLS[Verified TLS]</code></pre>"},{"location":"user-guide/openbaocluster/configuration/security-profiles/#configuration","title":"Configuration","text":"Hardened (Production)Development <p>The <code>Hardened</code> profile enforces strict security best practices. It is REQUIRED for all production environments.</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: prod-cluster\nspec:\n  profile: Hardened  # REQUIRED\n  replicas: 3          # Minimum 3 for HA (Raft quorum)\n  version: \"2.4.4\"\n  tls:\n    enabled: true\n    mode: External   # Required (or ACME)\n  unseal:\n    type: awskms     # Required (External KMS)\n    awskms:\n      region: us-east-1\n      kmsKeyID: alias/openbao-unseal\n  selfInit:\n    enabled: true    # Required\n    requests:\n      - name: enable-audit\n        operation: update\n        path: sys/audit/file\n        auditDevice:\n          type: file\n          fileOptions:\n            filePath: /tmp/audit.log\n</code></pre> <p>The <code>Development</code> profile allows relaxed security settings for rapid iteration and testing.</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: dev-cluster\nspec:\n  profile: Development\n  version: \"2.4.4\"\n  # TLS and Self-Init are optional\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/security-profiles/#requirements","title":"Requirements","text":"<ul> <li> External TLS: <code>spec.tls.mode</code> MUST be <code>External</code> or <code>ACME</code>.</li> <li> External KMS: <code>spec.unseal.type</code> MUST use a cloud provider (<code>awskms</code>, <code>gcpckms</code>, <code>azurekeyvault</code>, <code>transit</code>).</li> <li> Self-Initialization: <code>spec.selfInit.enabled</code> MUST be <code>true</code>.</li> <li> High Availability: <code>spec.replicas</code> MUST be at least <code>3</code> for Raft quorum.</li> <li> Secure Network: If backups are enabled, explicit egress rules are required (fail-closed networking).</li> </ul>"},{"location":"user-guide/openbaocluster/configuration/security-profiles/#benefits","title":"Benefits","text":"<ul> <li>Zero Trust: No root tokens are ever generated.</li> <li>Identity: Automatic JWT identity bootstrapping.</li> <li>Encryption: Root of trust is delegated to a hardware-backed KMS, not Kubernetes etcd.</li> </ul>"},{"location":"user-guide/openbaocluster/configuration/security-profiles/#characteristics","title":"Characteristics","text":"<ul> <li> Relaxed TLS: Allows <code>OperatorManaged</code> (self-signed) TLS.</li> <li> Static Unseal: Uses a simple Kubernetes Secret for the unseal key.</li> <li> Root Token: Generates and stores a root token in a Secret if self-init is disabled.</li> <li> Risk Indicator: Sets <code>ConditionSecurityRisk=True</code> on the CR status.</li> </ul> <p>Risk Acceptance</p> <p>By using this profile, you accept the risk of storing sensitive keys and root tokens in the cluster. Do not expose this cluster to public traffic.</p>"},{"location":"user-guide/openbaocluster/configuration/security-profiles/#workload-hardening-apparmor","title":"Workload Hardening (AppArmor)","text":"<p>AppArmor support is opt-in as it depends on the underlying Kubernetes node OS support.</p> <p>To enable <code>RuntimeDefault</code> AppArmor profiles on all OpenBao Pods:</p> <pre><code>spec:\n  workloadHardening:\n    appArmorEnabled: true\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/self-init/","title":"Self-Initialization","text":"<p>OpenBao supports self-initialization, allowing declarative configuration of the cluster during first startup.</p> <p>GitOps Ready</p> <p>Self-initialization eliminates the need for manual setup scripts or post-install hooks. The entire cluster state (Auth, Secrets, Policies) is defined in your CRD.</p>"},{"location":"user-guide/openbaocluster/configuration/self-init/#standard-vs-self-initialization","title":"Standard vs Self-Initialization","text":"Feature Standard Init Self-Initialization Root Token Created &amp; Stored in Secret Auto-Revoked (Never Stored) Configuration Manual Post-Install Steps Declarative (in CRD) Recovery Root Token Cloud KMS / Other Auth Methods Security  Lower (Root Token Risk) High (Zero Trust) <pre><code>flowchart LR\n    Start[\"OpenBaoCluster Created\"]\n    Method{\"selfInit.enabled?\"}\n\n    subgraph Standard [Standard Init]\n        STD_A[Ops calls /v1/sys/init]\n        STD_B[Root Token Generated]\n        STD_C[Root Token Stored in Secret]\n    end\n\n    subgraph Self [Self-Init]\n        SI_A[OpenBao Auto-Unseals]\n        SI_B[Executes Requests]\n        SI_C[Root Token Revoked]\n    end\n\n    Start --&gt; Method\n    Method -- No --&gt; Standard\n    Method -- Yes --&gt; Self\n\n    style Self fill:transparent,stroke:#00e676,stroke-width:2px\n    style Standard fill:transparent,stroke:#ff5252,stroke-width:2px</code></pre>"},{"location":"user-guide/openbaocluster/configuration/self-init/#configuration","title":"Configuration","text":"<p>To enable self-initialization, set <code>spec.selfInit.enabled: true</code> and define your initial <code>requests</code>.</p> <pre><code>spec:\n  selfInit:\n    enabled: true\n    requests:\n      - name: enable-audit\n        operation: update\n        path: sys/audit/file\n        auditDevice:\n          type: file\n          fileOptions:\n            filePath: /tmp/audit.log\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/self-init/#request-structure","title":"Request Structure","text":"<p>Each item in <code>requests[]</code> maps to an OpenBao API call.</p> Field Description <code>name</code> Unique ID (e.g., <code>enable-jwt</code>). <code>operation</code> <code>update</code> (most common), <code>create</code>, <code>delete</code>. <code>path</code> API Path (e.g., <code>sys/auth/jwt</code>). <code>data</code> Raw JSON payload (legacy/generic). <code>authMethod</code> Structured config for <code>sys/auth/*</code>. <code>secretEngine</code> Structured config for <code>sys/mounts/*</code>. <code>auditDevice</code> Structured config for <code>sys/audit/*</code>. <code>policy</code> Structured config for <code>sys/policies/*</code>. <p>Sensitive Data</p> <p>Do not place raw secrets (passwords, tokens) in <code>data</code>. Use Kubernetes Secrets and reference them if supported, or use a secure GitOps workflow with sealed secrets.</p>"},{"location":"user-guide/openbaocluster/configuration/self-init/#examples","title":"Examples","text":"Secret EnginesAuth MethodsPoliciesAudit Devices <p>Enable and configure Secret Engines (<code>sys/mounts/*</code>).</p> <pre><code>- name: enable-kv-v2\n  operation: update\n  path: sys/mounts/secret\n  secretEngine:\n    type: kv\n    description: \"General purpose KV store\"\n    options:\n      version: \"2\"\n</code></pre> <pre><code>- name: enable-transit\n  operation: update\n  path: sys/mounts/transit\n  secretEngine:\n    type: transit\n    description: \"Encryption as a Service\"\n</code></pre> <p>Enable Authentication Methods (<code>sys/auth/*</code>).</p> <pre><code>- name: enable-jwt\n  operation: update\n  path: sys/auth/jwt\n  authMethod:\n    type: jwt\n    description: \"Kubernetes JWT Auth\"\n    config:\n        default_lease_ttl: \"1h\"\n        max_lease_ttl: \"24h\"\n</code></pre> <pre><code>- name: configure-jwt\n  operation: update\n  path: auth/jwt/config  # Note: Config path is distinct from mount path\n  data:\n    bound_issuer: \"https://kubernetes.default.svc\"\n    jwt_validation_pubkeys:\n      - \"&lt;PEM_KEYS&gt;\"\n</code></pre> <p>Create ACL Policies (<code>sys/policies/acl/*</code>).</p> <pre><code>- name: app-policy\n  operation: update\n  path: sys/policies/acl/app-policy\n  policy:\n    policy: |\n      path \"secret/data/app/*\" {\n        capabilities = [\"read\", \"list\"]\n      }\n</code></pre> <p>Enable Audit Logging (<code>sys/audit/*</code>).</p> <pre><code>- name: enable-file-audit\n  operation: update\n  path: sys/audit/file\n  auditDevice:\n    type: file\n    fileOptions:\n      filePath: /var/log/openbao/audit.log\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/self-init/#verification","title":"Verification","text":"<p>Check the status field to confirm self-initialization succeeded.</p> <pre><code>kubectl get openbaocluster &lt;name&gt; -o jsonpath='{.status.selfInitialized}'\n# Output: true\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/server/","title":"Advanced Configuration","text":"<p>The <code>OpenBaoCluster</code> Custom Resource provides comprehensive configuration options for the OpenBao server.</p>"},{"location":"user-guide/openbaocluster/configuration/server/#configuration-groups","title":"Configuration Groups","text":"CoreObservabilitySecurityPluginsOperations <p>Configure the fundamental server behaviors like UI, Listening, and Storage tuning via <code>spec.configuration</code>.</p> <pre><code>spec:\n  configuration:\n    # User Interface\n    ui: true\n\n    # Performance Tuning\n    cacheSize: 134217728  # 128MB\n    disableCache: false\n\n    # Raft Storage\n    raft:\n      performanceMultiplier: 2\n      # Autopilot (enabled by default)\n      autopilot:\n        cleanupDeadServers: true\n        deadServerLastContactThreshold: \"5m\"\n        minQuorum: 2\n\n    # Lease Management\n    defaultLeaseTTL: \"720h\" # 30 days\n    maxLeaseTTL: \"8760h\"    # 1 year\n\n    # Listener Settings\n    listener:\n      proxyProtocolBehavior: \"use_proxy_protocol\"\n</code></pre> Field Description <code>ui</code> Enable/Disable the web interface. <code>listener</code> Configure TLS and Proxy Protocol usage. <code>raft.performanceMultiplier</code> Tune Raft timing for high-latency environments. <code>raft.autopilot</code> Configure Autopilot dead server cleanup (enabled by default). <code>defaultLeaseTTL</code> Default Time-To-Live for leases. <p>Configure Logging and Telemetry for monitoring.</p> <pre><code>spec:\n  configuration:\n    logLevel: \"info\"\n    logging:\n      format: \"json\"\n      file: \"/var/log/openbao/openbao.log\"\n      rotateDuration: \"24h\"\n      rotateMaxFiles: 7\n\n  telemetry:\n    prometheusRetentionTime: \"24h\"\n    disableHostname: true\n    metricsPrefix: \"openbao_\"\n</code></pre> <p>See Telemetry Documentation for all provider options (StatsD, DogStatsD, etc.).</p> <p>Configure declarative Audit Devices. These are automatically enabled on startup.</p> <pre><code>spec:\n  audit:\n    - type: file\n      path: stdout\n      description: \"Stdout audit device for debugging\"\n      options:\n        file_path: \"/dev/stdout\"\n        log_raw: \"true\"\n    - type: file\n      path: secure-audit\n      description: \"Secure audit logging\"\n      options:\n        file_path: \"/var/log/openbao/audit.log\"\n        format: \"json\"\n</code></pre> <p>Configure OCI-based Plugins which are automatically downloaded and registered.</p> <pre><code>spec:\n  configuration:\n    plugin:\n      autoDownload: true\n      downloadBehavior: \"direct\"\n\n  plugins:\n    - type: secret\n      name: aws\n      image: \"ghcr.io/openbao/openbao-plugin-secrets-aws\"\n      version: \"v1.0.0\"\n      binaryName: \"openbao-plugin-secrets-aws\"\n      sha256sum: \"9fdd8be7947e4a4caf7cce4f0e02695081b6c85178aa912df5d37be97363144c\"\n</code></pre> <p>Configure Images, Backups, and Init Containers.</p> <pre><code>spec:\n  # Override Images for Air-Gapped Environments\n  image: \"internal-registry.example.com/openbao/openbao:2.4.0\"\n\n  initContainer:\n    enabled: true\n    image: \"internal-registry.example.com/openbao/openbao-config-init:v1.0.0\"\n\n  backup:\n    executorImage: \"internal-registry.example.com/openbao/backup-executor:v0.1.0\"\n    schedule: \"0 3 * * *\"\n    retention:\n      maxCount: 7\n    target:\n      endpoint: \"https://s3.amazonaws.com\"\n      bucket: \"backups\"\n      region: \"us-east-1\"\n      credentialsSecretRef:\n        name: s3-credentials\n</code></pre>"},{"location":"user-guide/openbaocluster/configuration/server/#feature-reference","title":"Feature Reference","text":"<p>For a complete list of <code>spec.configuration</code> fields, run:</p> <pre><code>kubectl explain openbaocluster.spec.configuration\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/backups/","title":"Backups","text":"<p>The Operator provides a robust, Kubernetes-native backup system that streams Raft snapshots directly to object storage.</p> <p>Note: For restore procedures, see Restore from Backup.</p>"},{"location":"user-guide/openbaocluster/operations/backups/#architecture","title":"Architecture","text":"<p>Backups run as transient Kubernetes Jobs, triggered by a Cron schedule or manually.</p> <pre><code>flowchart LR\n    Cron[Cron Schedule] --&gt;|Triggers| Job[Backup Job]\n    Job --&gt;|Auths via JWT| Bao[OpenBao Cluster]\n    Bao --&gt;|Streams Snapshot| Job\n    Job --&gt;|Uploads| S3[Object Storage]\n\n    style Job fill:transparent,stroke:#2979ff,stroke-width:2px\n    style Bao fill:transparent,stroke:#00e676,stroke-width:2px\n    style S3 fill:transparent,stroke:#ffa726,stroke-width:2px</code></pre>"},{"location":"user-guide/openbaocluster/operations/backups/#prerequisites","title":"Prerequisites","text":"<ul> <li> Object Storage: S3-compatible bucket (AWS, MinIO, GCS, Azure)</li> <li> Credentials: Write permissions for the bucket</li> <li> Network: Egress allowed to the S3 endpoint (Critical for <code>Hardened</code> profile)</li> </ul>"},{"location":"user-guide/openbaocluster/operations/backups/#configuration","title":"Configuration","text":"<p>Select your authentication method. JWT Auth is recommended for security (auto-rotating tokens).</p> JWT Auth (Recommended)Static Token (Legacy) <p>This method uses a projected ServiceAccount token to authenticate with OpenBao.</p> One-time Setup: Configure JWT Auth <p>Before creating the cluster, ensure OpenBao is configured to accept the Kubernetes JWT token.</p> <p>Using Self-Init: <pre><code>spec:\n  selfInit:\n    requests:\n      # 1. Enable JWT Auth\n      - name: enable-jwt-auth\n        operation: update\n        path: sys/auth/jwt\n        authMethod: { type: jwt }\n      # 2. Configure JWT Validation\n      - name: configure-jwt-auth\n        operation: update\n        path: auth/jwt/config\n        data:\n          bound_issuer: \"https://kubernetes.default.svc\"\n          jwt_validation_pubkeys: [\"&lt;K8S_JWT_PUBLIC_KEY&gt;\"]\n      # 3. Create Backup Policy\n      - name: create-backup-policy\n        operation: update\n        path: sys/policies/acl/backup\n        policy:\n          policy: |\n            path \"sys/storage/raft/snapshot\" { capabilities = [\"read\"] }\n      # 4. Create Role bound to ServiceAccount\n      - name: create-backup-jwt-role\n        operation: update\n        path: auth/jwt/role/backup\n        data:\n          role_type: jwt\n          bound_audiences: [\"openbao-internal\"]\n          bound_claims:\n            kubernetes.io/namespace: openbao\n            kubernetes.io/serviceaccount/name: backup-cluster-backup-serviceaccount\n          token_policies: backup\n          ttl: 1h\n</code></pre></p> <p>Cluster Configuration:</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: backup-cluster\nspec:\n  backup:\n    schedule: \"0 3 * * *\"  # Daily at 3 AM\n    executorImage: \"openbao/backup-executor:v0.1.0\"\n    jwtAuthRole: backup    # Matches the role name above\n\n    target:\n      endpoint: \"https://s3.amazonaws.com\"\n      bucket: \"openbao-backups\"\n      region: \"us-east-1\"\n      pathPrefix: \"clusters/backup-cluster\"\n      credentialsSecretRef:\n        name: s3-credentials\n</code></pre> <p>This method uses a static OpenBao token stored in a Kubernetes Secret.</p> <p>Same-Namespace Requirement</p> <p>All secret references must exist in the same namespace as the <code>OpenBaoCluster</code>. Cross-namespace references are not allowed for security reasons.</p> Prerequisite: Create Token Secret <ol> <li>Generate a generic token in OpenBao with snapshot read permissions.</li> <li>Store it in a Secret:    <pre><code>kubectl create secret generic backup-token \\\n  --from-literal=token=hvs.yourtoken...\n</code></pre></li> </ol> <p>Cluster Configuration:</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoCluster\nmetadata:\n  name: backup-cluster\nspec:\n  backup:\n    schedule: \"0 3 * * *\"\n    executorImage: \"openbao/backup-executor:v0.1.0\"\n    tokenSecretRef:\n      name: backup-token  # Must be in the same namespace as the OpenBaoCluster\n\n    target:\n      endpoint: \"https://s3.amazonaws.com\"\n      bucket: \"openbao-backups\"\n      credentialsSecretRef:\n        name: s3-credentials\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/backups/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"user-guide/openbaocluster/operations/backups/#retention-policy","title":"Retention Policy","text":"<p>Automatically clean up old backups from object storage.</p> <pre><code>spec:\n  backup:\n    retention:\n      maxCount: 7      # Keep last 7 backups\n      maxAge: \"168h\"   # Keep backups for 7 days\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/backups/#performance-tuning","title":"Performance Tuning","text":"<p>Tune multipart upload settings for large datasets or specific network conditions.</p> Parameter Default Description <code>partSize</code> <code>10MB</code> Size of each upload chunk. Increase for high-bandwidth networks. <code>concurrency</code> <code>3</code> Parallel uploads. Increase for throughput, decrease for memory constraints. <pre><code>spec:\n  backup:\n    target:\n      partSize: 20971520  # 20MB\n      concurrency: 5\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/backups/#pre-upgrade-snapshots","title":"Pre-Upgrade Snapshots","text":"<p>Ensure safety during upgrades by taking a snapshot immediately before the rolling update or blue/green deployment begins.</p> <pre><code>spec:\n  upgrade:\n    preUpgradeSnapshot: true\n  backup:\n    # Backup config must be present!\n    target: { ... }\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/backups/#operations","title":"Operations","text":"<p>Check Status:</p> <pre><code>kubectl get openbaocluster my-cluster -o jsonpath='{.status.backup}'\n</code></pre> <p>Trigger Manual Backup:</p> <pre><code>kubectl create job --from=cronjob/my-cluster-backup manual-backup-1\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/deletion/","title":"Deletion Policy","text":"<p>When an <code>OpenBaoCluster</code> is deleted, the Operator uses Kubernetes standard cascading deletion (<code>OwnerReferences</code>) to clean up resources. You can configure how aggressive this cleanup is.</p>"},{"location":"user-guide/openbaocluster/operations/deletion/#deletion-policies","title":"Deletion Policies","text":"<p>Control what happens to critical data (Persistent Volumes) and external backups when a cluster is removed.</p> Retain (Default)DeletePVCs <p>Best for: Production settings where data safety is paramount.</p> <p>The Operator cleans up compute resources (StatefulSet, Services, ConfigMaps) but PRESERVES critical secrets and storage.</p> <pre><code>spec:\n  deletionPolicy: Retain  # Default behavior\n</code></pre> <p>Cleanup Scope:</p> <ul> <li>Deleted: Pods, StatefulSets, Services, ConfigMaps, TLS Secrets</li> <li>Retained: PVCs (Data), Unseal Key Secret, Root Token Secret, S3 Backups</li> </ul> <p>Why Unseal Key is Retained</p> <p>The unseal key secret is essential to decrypt your PVC data. Without it, your encrypted data becomes unrecoverable. The Operator automatically orphans these secrets (removes owner references) so Kubernetes garbage collection won't delete them.</p> <p>Best for: CI/CD pipelines, ephemeral dev environments.</p> <p>The Operator actively deletes the associated PersistentVolumeClaims (PVCs) when the cluster is deleted.</p> <pre><code>spec:\n  deletionPolicy: DeletePVCs\n</code></pre> <p>Data Loss Warning</p> <p>This policy permanently deletes the underlying disk volumes when the Custom Resource is deleted. This action cannot be undone.</p> <p>Cleanup Scope:</p> <ul> <li>Deleted: Pods, StatefulSets, Services, ConfigMaps, Secrets, PVCs (Data)</li> <li>Retained: S3 Backups</li> </ul>"},{"location":"user-guide/openbaocluster/operations/deletion/#performing-deletion","title":"Performing Deletion","text":"<p>To delete a cluster:</p> <pre><code>kubectl -n security delete openbaocluster dev-cluster\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/deletion/#verifying-cleanup","title":"Verifying Cleanup","text":"<p>After deletion, you can check for any leftover resources (like PVCs if using <code>Retain</code> policy):</p> <pre><code>kubectl -n security get pvc -l openbao.org/cluster=dev-cluster\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/maintenance/","title":"Cluster Maintenance","text":"<p>This guide covers maintenance operations for OpenBao clusters, including node drains, voluntary disruptions, and cluster scaling.</p>"},{"location":"user-guide/openbaocluster/operations/maintenance/#pod-disruption-budgets","title":"Pod Disruption Budgets","text":"<p>The Operator automatically creates a <code>PodDisruptionBudget</code> (PDB) for each cluster with 2 or more replicas. This protects against accidental quorum loss during voluntary disruptions.</p>"},{"location":"user-guide/openbaocluster/operations/maintenance/#default-behavior","title":"Default Behavior","text":"Replicas PDB Created Max Unavailable Notes 1 No N/A Single-replica clusters have no redundancy 3 Yes 1 Ensures quorum (\u2154) is always maintained 5 Yes 1 Conservative setting; 4 pods remain available <p>The PDB uses <code>maxUnavailable: 1</code>, meaning Kubernetes will block eviction requests that would take more than one pod offline simultaneously.</p>"},{"location":"user-guide/openbaocluster/operations/maintenance/#during-node-drains","title":"During Node Drains","text":"<p>When you drain a node hosting OpenBao pods:</p> <ol> <li>Kubernetes checks the PDB before evicting any pods.</li> <li>If eviction would violate the PDB, the drain blocks and waits.</li> <li>Pods are evicted one at a time, allowing the cluster to maintain quorum.</li> </ol> <pre><code># Safe: Node drain respects PDB\nkubectl drain node-1 --ignore-daemonsets --delete-emptydir-data\n\n# Check if PDB is blocking eviction\nkubectl get pdb -n &lt;namespace&gt;\n</code></pre> <p>Drain Timeouts</p> <p>If multiple OpenBao pods are scheduled on the same node, draining that node may take longer as pods are evicted sequentially. Consider using pod anti-affinity to spread pods across nodes.</p>"},{"location":"user-guide/openbaocluster/operations/maintenance/#limitations","title":"Limitations","text":"<p>The PDB only protects against voluntary disruptions:</p> <ul> <li>Node drains (<code>kubectl drain</code>)</li> <li>Cluster autoscaler scale-down</li> <li>Pod eviction API calls</li> </ul> <p>It does not protect against:</p> <ul> <li>Node failures/crashes</li> <li>Pod OOM kills</li> <li>Node kernel panics</li> </ul> <p>For these scenarios, rely on Raft's built-in quorum-based replication.</p>"},{"location":"user-guide/openbaocluster/operations/maintenance/#scaling","title":"Scaling","text":""},{"location":"user-guide/openbaocluster/operations/maintenance/#scaling-up","title":"Scaling Up","text":"<p>Increase replicas to add capacity or improve fault tolerance:</p> <pre><code>spec:\n  replicas: 5  # Was 3\n</code></pre> <p>The Operator will:</p> <ol> <li>Create new pods via the StatefulSet</li> <li>Wait for each pod to join the Raft cluster</li> <li>Update the PDB to match the new replica count</li> </ol>"},{"location":"user-guide/openbaocluster/operations/maintenance/#scaling-down","title":"Scaling Down","text":"<p>Reduce replicas to save resources:</p> <pre><code>spec:\n  replicas: 3  # Was 5\n</code></pre> <p>Minimum Replicas</p> <p>Never scale below 3 replicas in production. Scaling to 1 replica means any pod failure results in complete unavailability.</p> <p>The Operator will:</p> <ol> <li>Gracefully remove Raft voters starting from the highest ordinal</li> <li>Wait for Raft configuration to update</li> <li>Delete the excess pods</li> <li>Update the PDB</li> </ol>"},{"location":"user-guide/openbaocluster/operations/maintenance/#leader-step-down","title":"Leader Step-Down","text":"<p>When maintenance requires the active leader to be evicted, the Operator automatically triggers a graceful step-down:</p> <ol> <li>Pre-eviction hook detects the leader is being terminated</li> <li>Step-down request is sent to <code>/sys/step-down</code></li> <li>New leader election completes before the old leader terminates</li> <li>Eviction proceeds without service interruption</li> </ol> <p>This behavior is automatic and requires no manual intervention.</p>"},{"location":"user-guide/openbaocluster/operations/maintenance/#maintenance-windows","title":"Maintenance Windows","text":"<p>For planned maintenance, consider:</p> <ol> <li> <p>Pause reconciliation during complex operations:    <pre><code>spec:\n  paused: true\n</code></pre></p> </li> <li> <p>Monitor cluster health before and after:    <pre><code>kubectl get openbaocluster &lt;name&gt; -o jsonpath='{.status.phase}'\nkubectl get pods -l openbao.org/cluster=&lt;name&gt;\n</code></pre></p> </li> <li> <p>Check Raft peer status if needed:    <pre><code>kubectl exec -it &lt;pod-name&gt; -- bao operator raft list-peers\n</code></pre></p> </li> </ol>"},{"location":"user-guide/openbaocluster/operations/pausing/","title":"Pausing Reconciliation","text":"<p>You can temporarily stop the Operator from managing a cluster by pausing it. This is useful for manual intervention, debugging, or preventing changes during maintenance windows.</p>"},{"location":"user-guide/openbaocluster/operations/pausing/#how-to-pause","title":"How to Pause","text":"<p>Set <code>spec.paused</code> to <code>true</code> to suspend reconciliation:</p> <pre><code>kubectl -n security patch openbaocluster dev-cluster \\\n  --type merge -p '{\"spec\":{\"paused\":true}}'\n</code></pre> <p>Impact of Pausing</p> <p>While the cluster is paused:</p> <ul> <li>No Mutation: The Operator will NOT update StatefulSets, Secrets, ConfigMaps, or Services.</li> <li>No Healing: If a pod crashes or is deleted, the Operator will NOT take action to fix it (though Kubernetes StatefulSet controller might).</li> <li>Finalizers Run: Deletion logic still operates if the CR is deleted.</li> </ul>"},{"location":"user-guide/openbaocluster/operations/pausing/#how-to-resume","title":"How to Resume","text":"<p>Set <code>spec.paused</code> to <code>false</code> (or remove the field) to resume normal operations:</p> <pre><code>kubectl -n security patch openbaocluster dev-cluster \\\n  --type merge -p '{\"spec\":{\"paused\":false}}'\n</code></pre> <p>The Operator will immediately reconcile the cluster state to match the CR.</p>"},{"location":"user-guide/openbaocluster/operations/pausing/#troubleshooting","title":"Troubleshooting","text":"<p>Is my cluster paused?</p> <p>If a cluster seems unresponsive to updates, check the paused status:</p> <pre><code>kubectl -n security get openbaocluster dev-cluster -o jsonpath='{.spec.paused}'\n</code></pre> <p>Break Glass Recovery</p> <p>If the Operator has entered Safe Mode (due to quorum loss or critical failures), simply unpausing is not sufficient. You must follow the Break Glass procedure to explicitly acknowledge the risk.</p> <p>See Break Glass / Safe Mode for details.</p>"},{"location":"user-guide/openbaocluster/operations/production-checklist/","title":"Production Checklist","text":"<p>Before deploying OpenBao Operator in production, complete this checklist to ensure a secure, reliable, and compliant deployment.</p>"},{"location":"user-guide/openbaocluster/operations/production-checklist/#critical-security","title":"Critical Security","text":"<p>Security Hardening (Required)</p> <p>Failure to configure these settings puts your cluster at significant risk.</p> <ul> <li> Hardened Profile: Set <code>spec.profile: Hardened</code> to enforce secure defaults.<ul> <li>Learn more</li> </ul> </li> <li> External Unseal: Use Transit or Cloud KMS. Do NOT use auto-unseal with Kubernetes Secrets in production.<ul> <li>Learn more</li> </ul> </li> <li> Etcd Encryption: Ensure your Kubernetes cluster enables encryption at rest for Secrets (where unseal keys might be stored).</li> <li> TLS Mode: Use <code>ACME</code> (Let's Encrypt) or <code>External</code> (Custom CA). Avoid <code>OperatorManaged</code> for public-facing endpoints.<ul> <li>Learn more</li> </ul> </li> <li> Self-Initialization: Enable <code>spec.selfInit</code> to prevent the initial root token from ever being surfaced to the operator or logs.<ul> <li>Learn more</li> </ul> </li> </ul> <p>Admission Control</p> <p>Without these policies, tenant isolation cannot be guaranteed.</p> <ul> <li> ValidatingAdmissionPolicies: Verify that <code>validate-openbaocluster</code> and <code>openbao-restrict-provisioner-delegate</code> are installed and Enforced.<ul> <li>Learn more</li> </ul> </li> </ul>"},{"location":"user-guide/openbaocluster/operations/production-checklist/#reliability-scale","title":"Reliability &amp; Scale","text":"<p>Resource Planning</p> <ul> <li> Resources: Set explicit <code>requests</code> and <code>limits</code>. Minimum 256Mi memory for small clusters; scale CPU based on expected request rate.<ul> <li>Learn more</li> </ul> </li> <li> Storage Class: Use a high-performance (SSD), low-latency StorageClass. Raft requires low fsync latency.</li> <li> Volume Size: Plan for growth. Raft snapshots can consume significant space.</li> </ul> <p>Availability</p> <ul> <li> Topology Spread: Ensure your <code>Kubernetes</code> cluster has nodes in multiple zones. The Operator automatically sets standard anti-affinity.</li> <li> Replica Count: Use at least 3 replicas for high availability.</li> </ul>"},{"location":"user-guide/openbaocluster/operations/production-checklist/#day-2-operations","title":"Day 2 Operations","text":"<p>Operational Readiness</p> <ul> <li> Backups: Configure scheduled backups to S3/GCS. Test a restore before going live.<ul> <li>Learn more</li> </ul> </li> <li> Network Policy: Verify <code>egressRules</code> allow access to necessary external services (Cloud KMS, S3, OIDC providers).<ul> <li>Learn more</li> </ul> </li> <li> Monitoring: Ensure Prometheus is scraping <code>openbao_*</code> metrics and alerts are configured for high error rates or leader loss.<ul> <li>Learn more</li> </ul> </li> <li> Logs: Verify structured logs (<code>cluster_name</code>, <code>cluster_namespace</code>) are reaching your log aggregator.<ul> <li>Learn more</li> </ul> </li> <li> Alerts: Configure alerts for backup staleness, cluster degradation, and reconciliation errors.<ul> <li>Learn more</li> </ul> </li> </ul>"},{"location":"user-guide/openbaocluster/operations/production-checklist/#final-verification","title":"Final Verification","text":"<p>Check the cluster status one last time before routing traffic:</p> <pre><code>kubectl describe openbaocluster &lt;name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Success Criteria:</p> <ul> <li> Condition <code>ProductionReady</code> is True.</li> <li> Condition <code>Available</code> is True.</li> <li> <code>Status.Phase</code> is Running.</li> </ul>"},{"location":"user-guide/openbaocluster/operations/upgrades/","title":"Cluster Upgrades","text":"<p>The Operator supports two powerful upgrade strategies: Rolling Update (default) for efficiency, and Blue/Green for zero-downtime safety.</p>"},{"location":"user-guide/openbaocluster/operations/upgrades/#one-time-setup","title":"One-Time Setup","text":"<p>To perform upgrades safely, the Operator uses a temporary \"Upgrade Executor\" job that requires permissions to talk to OpenBao.</p> Prerequisite: Configure Upgrade Authentication <p>The Upgrade Executor needs a JWT Auth Role to authenticate with the cluster during upgrades.</p> <p>1. Configure OpenBao (via <code>selfInit</code>)</p> <pre><code>spec:\n  selfInit:\n    requests:\n      # Create policy for upgrade operations\n      - name: create-upgrade-policy\n        operation: update\n        path: sys/policies/acl/upgrade\n        policy:\n          policy: |\n            path \"sys/health\" { capabilities = [\"read\"] }\n            path \"sys/step-down\" { capabilities = [\"update\"] }\n            path \"sys/storage/raft/snapshot\" { capabilities = [\"read\"] }\n      # Create JWT role bound to the upgrade ServiceAccount\n      - name: create-upgrade-jwt-role\n        operation: update\n        path: auth/jwt/role/upgrade\n        data:\n          role_type: jwt\n          bound_audiences: [\"openbao-internal\"]\n          bound_claims:\n            kubernetes.io/namespace: openbao\n            kubernetes.io/serviceaccount/name: upgrade-cluster-upgrade-serviceaccount\n          token_policies: upgrade\n          ttl: 1h\n</code></pre> <p>2. Configure the Operator to use this role</p> <pre><code>spec:\n  upgrade:\n    executorImage: openbao/upgrade-executor:v0.1.0\n    jwtAuthRole: upgrade\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/upgrades/#executing-upgrades","title":"Executing Upgrades","text":"<p>To upgrade, simply update the <code>spec.version</code> field. The <code>updateStrategy</code> determines how this change is applied.</p> Rolling Update (Default)Blue/Green (Zero Downtime) <p>Best for: Standard upgrades, Dev/Test environments, Minimizing resource usage.</p> <p>The Operator updates pods one by one, ensuring the active leader steps down gracefully before termination to maintain availability.</p> <pre><code>spec:\n  version: \"2.4.4\"\n  updateStrategy:\n    type: RollingUpdate\n</code></pre> <p>How it works: 1.  Validation: Checks if the new version is valid. 2.  Snapshot (Optional): Takes a pre-upgrade backup. 3.  Rolling Replace: Updates Pod 0 -&gt; Pod 1 -&gt; Pod 2. 4.  Leader Handling: If updating the active leader, triggers <code>sys/step-down</code> first.</p> <p>Best for: Production critical paths, Major version jumps, Instant rollback capability.</p> <p>The Operator spins up a parallel \"Green\" cluster, syncs data, validates it, and then switches traffic over atomically.</p> <pre><code>flowchart TB\n    Start[Start Upgrade]\n\n    subgraph Blue[\"Blue Revision (Current)\"]\n        B[Active Cluster]\n    end\n\n    subgraph Green[\"Green Revision (New)\"]\n        direction TB\n        Deploy[1. Deploy Green Pods]\n        Sync[2. Sync Data form Blue]\n        Test[3. Run Verification]\n    end\n\n    Start --&gt; Deploy\n    Deploy --&gt; Sync\n    Sync --&gt; Test\n    Test -- \"Success\" --&gt; Switch[4. Switch Traffic to Green]\n    Switch --&gt; Cleanup[5. Delete Blue Cluster]\n\n    style Blue fill:transparent,stroke:#2979ff,stroke-width:2px\n    style Green fill:transparent,stroke:#00e676,stroke-width:2px,color:#fff\n    style Switch fill:transparent,stroke:#ffa726,stroke-width:2px</code></pre> <p>Configuration:</p> <pre><code>spec:\n  version: \"2.4.4\"\n  updateStrategy:\n    type: BlueGreen\n    blueGreen:\n      autoPromote: true          # Automatically switch traffic if healthy\n      preUpgradeSnapshot: true   # Backup before starting\n      autoRollback:\n        enabled: true            # Revert if Green fails validation\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/upgrades/#advanced-upgrade-options","title":"Advanced Upgrade Options","text":""},{"location":"user-guide/openbaocluster/operations/upgrades/#verification-hooks","title":"Verification Hooks","text":"<p>Run a custom container to \"smoke test\" the Green cluster before cutover.</p> <pre><code>spec:\n  updateStrategy:\n    blueGreen:\n      verification:\n        prePromotionHook:\n          image: curlimages/curl\n          command: [\"curl\", \"-f\", \"https://green-cluster:8200/v1/sys/health\"]\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/upgrades/#auto-rollback","title":"Auto-Rollback","text":"<p>If the Green cluster fails validation or upgrade jobs fail during the early upgrade phases, the Operator can automatically roll back.</p> <pre><code>spec:\n  updateStrategy:\n    blueGreen:\n      autoRollback:\n        enabled: true\n        onJobFailure: true\n        onValidationFailure: true\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/upgrades/#gateway-api-and-bluegreen-upgrades","title":"Gateway API and Blue/Green upgrades","text":"<p>When using Gateway API, the Operator creates an <code>HTTPRoute</code> that targets the cluster's main external Service (<code>&lt;cluster&gt;-public</code>). During cutover, the operator updates that Service's selector to point at the Green revision.</p> <pre><code>spec:\n  gateway:\n    enabled: true\n    hostname: bao.example.com\n    gatewayRef:\n      name: main-gateway\n  updateStrategy:\n    type: BlueGreen\n    blueGreen:\n      autoPromote: true\n</code></pre>"},{"location":"user-guide/openbaocluster/operations/upgrades/#monitoring-progress","title":"Monitoring Progress","text":"<p>Track the upgrade status directly on the CR:</p> <pre><code>kubectl get openbaocluster my-cluster -o jsonpath='{.status.blueGreen}'\n</code></pre>"},{"location":"user-guide/openbaocluster/recovery/failed-rollback/","title":"Recovering From a Failed Rollback","text":"<p>This runbook addresses a Failed Blue/Green Rollback. This occurs when you attempt to rollback an upgrade (e.g., changing <code>spec.version</code> back to an older version), but the Operator halts the process to prevent data corruption.</p> <p>Split Brain Risk</p> <p>The Operator stops automation because it suspects a Split Brain scenario where both the Blue (Old) and Green (New) clusters might have accepted writes, or Raft consensus cannot be safely transferred back.</p>"},{"location":"user-guide/openbaocluster/recovery/failed-rollback/#1-assess-the-situation","title":"1. Assess the Situation","text":"<p>Check the Safe Mode / Break Glass status to understand why the Operator halted.</p> <pre><code>kubectl -n security get openbaocluster prod-cluster -o jsonpath='{.status.breakGlass}' | jq\n</code></pre> <p>Common Reasons:</p> <ul> <li><code>ConsensusFailure</code>: The Blue cluster could not rejoin the Raft quorum.</li> <li><code>DirtyWrite</code>: The Green cluster accepted writes that would be lost on rollback (if configured to check).</li> <li><code>Timeout</code>: The rollback job took too long.</li> </ul>"},{"location":"user-guide/openbaocluster/recovery/failed-rollback/#2-inspect-the-rollback-job","title":"2. Inspect the Rollback Job","text":"<p>The logic for transferring leadership back to the Blue cluster runs in a Kubernetes Job named <code>inter-cluster-rev&lt;Revision&gt;</code>.</p> <p>Find and log the failed job:</p> <pre><code># Find the job\nkubectl -n security get jobs -l openbao.org/cluster=prod-cluster\n\n# View logs\nkubectl -n security logs job/inter-cluster-rev4-rollback\n</code></pre> <p>Log Analysis:</p> Log Message Meaning Resolution <code>failed to join blue pilot to green leader</code> Network connectivity issue between clusters. Check NetworkPolicies / DNS. Retry. <code>raft log index mismatch</code> The Blue cluster is too far behind (Green accepted too many writes). Cannot Rollback. You must proceed with Green or Restore from Snapshot. <code>context deadline exceeded</code> Operation timed out. Retry (Acknowledge Break Glass)."},{"location":"user-guide/openbaocluster/recovery/failed-rollback/#3-resolution-paths","title":"3. Resolution Paths","text":"<p>Choose the path that matches your diagnosis.</p> Path A: Retry (Transient)Path B: Abort (Stay Green)Path C: Restore (Data Loss) <p>If the failure was due to a network blip or timeout, and you believe the clusters are healthy:</p> <ol> <li> <p>Acknowledge the Break Glass nonce. This tells the Operator to \"Try Again\".</p> <pre><code>kubectl -n security patch openbaocluster prod-cluster --type merge \\\n  -p '{\"spec\":{\"breakGlassAck\":\"&lt;NONCE_FROM_STEP_1&gt;\"}}'\n</code></pre> </li> <li> <p>The Operator will spawn a new Rollback Job. Monitor it closely.</p> </li> </ol> <p>If the rollback is impossible (e.g., data divergence), it may be safer to stay on the new version (Green) and fix the application issues forward, or perform a fresh restore.</p> <ol> <li>Update Spec: Set <code>spec.version</code> back to the Green (New) version.</li> <li>Acknowledge: Acknowledge the nonce.</li> <li>The Operator will reconcile the Green cluster as the primary.</li> </ol> <p>If the cluster state is corrupted beyond repair:</p> <ol> <li>Stop Operator: Scale down the operator deployment.</li> <li>Delete PVCs: Clean up the corrupted data.</li> <li>Restore: Follow the Restore Guide to bring back the cluster from the last known good snapshot (Pre-Upgrade).</li> </ol>"},{"location":"user-guide/openbaocluster/recovery/failed-rollback/#preventative-measures","title":"Preventative Measures","text":"<ul> <li>Always ensure Snapshots are taken before triggering a Rollback (The Operator does this automatically if <code>spec.backup.enabled</code> is true).</li> <li>Monitor <code>etcd</code> or <code>raft</code> metrics during upgrades.</li> </ul>"},{"location":"user-guide/openbaocluster/recovery/no-leader/","title":"Recovering From No Leader / No Quorum","text":"<p>This runbook applies when the OpenBao cluster cannot elect a leader due to network partitions, crash-looping pods, or loss of quorum (e.g., 2 out of 3 pods lost).</p> <p>Symptoms</p> <ul> <li><code>kubectl get openbaocluster</code> shows <code>Ready=False</code>.</li> <li>Pod logs show <code>[WARN] raft: no known peers, aborting election</code>.</li> <li><code>bao status</code> returns <code>Sealed</code> or <code>Error checking seal status</code>.</li> </ul>"},{"location":"user-guide/openbaocluster/recovery/no-leader/#troubleshooting-flow","title":"Troubleshooting Flow","text":"<pre><code>graph TD\n    Start([\"Start\"]) --&gt; CheckPods{\"Are Pods Running?\"}\n    CheckPods -- No --&gt; FixPods[\"Fix CrashLoop / PVCs\"]\n    CheckPods -- Yes --&gt; CheckNet{\"Network OK?\"}\n\n    CheckNet -- No --&gt; FixNet[\"Check DNS / NetPol\"]\n    CheckNet -- Yes --&gt; CheckRaft{\"Raft State?\"}\n\n    CheckRaft -- \"Stale Peer\" --&gt; RemovePeer[\"Prune Dead Peer\"]\n    CheckRaft -- \"Split Brain\" --&gt; ManualRec[\"Manual Recovery\"]\n    CheckRaft -- \"Unknown\" --&gt; Logs[\"Inspect Logs\"]</code></pre>"},{"location":"user-guide/openbaocluster/recovery/no-leader/#phase-1-diagnosis","title":"Phase 1: Diagnosis","text":"<p>First, identify if the pods are healthy enough to communicate.</p>"},{"location":"user-guide/openbaocluster/recovery/no-leader/#1-check-pod-status","title":"1. Check Pod Status","text":"<pre><code>kubectl -n security get pods -l openbao.org/cluster=prod-cluster -o wide\n</code></pre> <p>If pods are <code>CrashLoopBackOff</code>, check logs for configuration or storage errors distinct from Raft issues.</p>"},{"location":"user-guide/openbaocluster/recovery/no-leader/#2-inspect-raft-peers","title":"2. Inspect Raft Peers","text":"<p>Exec into each running pod and check its view of the cluster.</p> <pre><code># Run this on pod-0, pod-1, and pod-2\nkubectl -n security exec -it prod-cluster-0 -- bao operator raft list-peers\n</code></pre> <p>Healthy Output:</p> <pre><code>Node      Address              State       Voter\n----      -------              -----       -----\ncluster-0 10.2.1.5:8201       leader      true\ncluster-1 10.2.2.8:8201       follower    true\ncluster-2 10.2.3.12:8201      follower    true\n</code></pre> <p>Unhealthy Output:</p> <ul> <li><code>error: context deadline exceeded</code> (Network issue)</li> <li><code>no leader elected</code> (Quorum issue)</li> </ul>"},{"location":"user-guide/openbaocluster/recovery/no-leader/#phase-2-network-verification","title":"Phase 2: Network Verification","text":"<p>If <code>raft list-peers</code> hangs, verify connectivity. Raft requires TCP port 8201 (or configured <code>cluster_port</code>) to be open between pods.</p> <pre><code># Test connectivity from pod-0 to pod-1\nkubectl -n security exec -ti prod-cluster-0 -- nc -zv prod-cluster-1.prod-cluster-internal 8201\n</code></pre> <p>If this fails:</p> <ol> <li>Check NetworkPolicies (ensure <code>openbao-cluster-isolation</code> allows ingress on 8201).</li> <li>Check DNS resolution (<code>nslookup prod-cluster-1.prod-cluster-internal</code>).</li> </ol>"},{"location":"user-guide/openbaocluster/recovery/no-leader/#phase-3-recovery-options","title":"Phase 3: Recovery Options","text":"<p>Choose the scenario that matches your state.</p>"},{"location":"user-guide/openbaocluster/recovery/no-leader/#scenario-a-removing-a-stale-peer","title":"Scenario A: Removing a Stale Peer","text":"<p>If you replaced a node and the old IP allows ghosts to linger, the cluster might think it needs 4 votes instead of 3.</p> <p>Symptoms:</p> <ul> <li><code>raft list-peers</code> shows a node causing <code>(failed)</code> status.</li> <li>You have a functioning leader, or a node that could be leader if the ghost was gone.</li> </ul> <p>Action: Run the <code>remove-peer</code> command from a healthy node, targeting the ID of the failed node.</p> <pre><code>kubectl -n security exec -ti prod-cluster-0 -- \\\n  bao operator raft remove-peer -id \"prod-cluster-lost-node\"\n</code></pre>"},{"location":"user-guide/openbaocluster/recovery/no-leader/#scenario-b-manual-quorum-recovery-peersjson","title":"Scenario B: Manual Quorum Recovery (peers.json)","text":"<p>The Nuclear Option</p> <p>Use this only if the cluster is completely down and cannot form a quorum automatically. This process manually forces a single pod to become a leader, potentially causing data loss if that pod was behind.</p> <p>Steps:</p> <ol> <li> <p>Scale Down: Stop the operator to prevent it from interfering.</p> <pre><code>kubectl -n openbao-operator-system scale deploy openbao-operator-controller-manager --replicas=0\n</code></pre> </li> <li> <p>Identify Survivor: Choose the pod with the most up-to-date data (usually the one with the largest <code>raft/raft.db</code> file, or the last known leader).</p> </li> <li> <p>Inject peers.json: Create a <code>peers.json</code> file inside the survivor pod's data directory. This tells the pod: \"You are the only member. Start a new cluster.\"</p> <pre><code># Content for peers.json (replace IP/Address/NodeID)\ncat &lt;&lt;EOF &gt; peers.json\n[\n  {\n    \"id\": \"prod-cluster-0\",\n    \"address\": \"prod-cluster-0.prod-cluster-internal:8201\",\n    \"non_voter\": false\n  }\n]\nEOF\n\n# Copy to pod\nkubectl cp peers.json security/prod-cluster-0:/bao/data/raft/peers.json\n</code></pre> </li> <li> <p>Restart Pod: Delete the pod to force a restart. OpenBao will detect <code>peers.json</code> on boot and reset the Raft configuration.</p> <pre><code>kubectl -n security delete pod prod-cluster-0\n</code></pre> </li> <li> <p>Verify Leader: Once running, check <code>bao status</code>. It should be sealed but have a leader. Unseal it.</p> </li> <li> <p>Join Others: Delete the other pods (<code>prod-cluster-1</code>, <code>prod-cluster-2</code>). When they restart, they should automatically join the new leader via the Operator's auto-join mechanism (or you can manually join them).</p> </li> <li> <p>Resume Operator:</p> <pre><code>kubectl -n openbao-operator-system scale deploy openbao-operator-controller-manager --replicas=1\n</code></pre> </li> </ol>"},{"location":"user-guide/openbaocluster/recovery/no-leader/#post-mortem","title":"Post-Mortem","text":"<p>After recovery, assume the Operator is in Safe Mode if it detected the failure.</p> <p>Check and acknowledge:</p> <pre><code>kubectl -n security get openbaocluster prod-cluster -o jsonpath='{.status.breakGlass}'\n</code></pre> <p>See Break Glass / Safe Mode for details.</p>"},{"location":"user-guide/openbaocluster/recovery/safe-mode/","title":"Break Glass / Safe Mode","text":"<p>Critical State: Automation Halted</p> <p>The Operator has entered Safe Mode because it detected a high-risk failure (e.g., loss of quorum during an upgrade). All automation is paused to prevent data loss.</p>"},{"location":"user-guide/openbaocluster/recovery/safe-mode/#overview","title":"Overview","text":"<p>Safe Mode (also known as \"Break Glass\") is a safety mechanism. When the Operator encounters a situation where continuing an automated workflow (like a rolling upgrade or rollback) could compromise data integrity or availability, it stops and waits for human operator intervention.</p> <p>Common triggers:</p> <ul> <li>Blue/Green rollback failure (risk of split-brain).</li> <li>Quorum loss during critical reconfiguration.</li> </ul> <p>When active:</p> <ol> <li>Automation Stops: The Operator stops reconciling the specific <code>OpenBaoCluster</code>.</li> <li>Status Updates: The <code>status.breakGlass</code> field is populated with diagnostic info.</li> <li>Manual Ack Required: You must explicitly \"break the glass\" to resume automation.</li> </ol>"},{"location":"user-guide/openbaocluster/recovery/safe-mode/#1-inspect-the-situation","title":"1. Inspect the Situation","text":"<p>Check if your cluster is in Safe Mode by inspecting its status.</p> <pre><code>kubectl -n security get openbaocluster prod-cluster -o jsonpath='{.status.breakGlass}' | jq\n</code></pre> <p>Example Output:</p> <pre><code>{\n  \"active\": true,\n  \"reason\": \"QuorumRisk\",\n  \"message\": \"Detected split-brain potential during rollback. Manual intervention required.\",\n  \"nonce\": \"abc-123-def-456\",\n  \"steps\": \"1. Verify network connectivity. 2. Restore quorum manually. 3. Acknowledge.\"\n}\n</code></pre>"},{"location":"user-guide/openbaocluster/recovery/safe-mode/#2-fix-the-underlying-issue","title":"2. Fix the Underlying Issue","text":"<p>Follow the specific guidance provided in the <code>message</code> and <code>steps</code> fields.</p> <ul> <li>If Quorum is lost: See Recovering from No Leader.</li> <li>If Sealed: See Recovering from Sealed Cluster.</li> <li>If Network Partitioned: Verify CNI and network policies.</li> </ul>"},{"location":"user-guide/openbaocluster/recovery/safe-mode/#3-acknowledge-and-resume","title":"3. Acknowledge and Resume","text":"<p>Once you have performed the necessary manual repairs, you must tell the Operator it is safe to proceed. This is done by acknowledging the unique nonce.</p> <p>Action Required</p> <p>Copy the <code>nonce</code> from step 1 and use it in the command below.</p> <pre><code># Replace 'abc-123-def-456' with your actual nonce\nkubectl -n security patch openbaocluster prod-cluster --type merge \\\n  -p '{\"spec\":{\"breakGlassAck\":\"abc-123-def-456\"}}'\n</code></pre> <p>If the issue persists, the Operator may re-enter Safe Mode with a new nonce, requiring you to repeat the diagnosis.</p>"},{"location":"user-guide/openbaocluster/recovery/safe-mode/#related-runbooks","title":"Related Runbooks","text":"<ul> <li> <p> No Leader / No Quorum</p> <p>Recovery steps when the Raft cluster loses consensus.</p> </li> <li> <p> Sealed Cluster</p> <p>How to unseal a cluster manually or diagnose auto-unseal failures.</p> </li> <li> <p> Failed Rollback</p> <p>Specific steps for handling a failed Blue/Green rollback.</p> </li> </ul>"},{"location":"user-guide/openbaocluster/recovery/sealed-cluster/","title":"Recovering From a Sealed Cluster","text":"<p>This runbook applies when OpenBao pods are running (<code>Running</code> state) but remain sealed, preventing the application from starting.</p> <p>Symptoms</p> <ul> <li><code>kubectl get openbaocluster</code> reports <code>Sealed=True</code>.</li> <li>Pods are ready <code>0/1</code> in <code>kubectl get pods</code>.</li> <li><code>bao status</code> shows <code>Sealed: true</code>.</li> </ul>"},{"location":"user-guide/openbaocluster/recovery/sealed-cluster/#troubleshooting-flow","title":"Troubleshooting Flow","text":"<pre><code>graph TD\n    Start([\"Start\"]) --&gt; CheckStatus{\"Sealed?\"}\n    CheckStatus -- No --&gt; Done([\"Healthy\"])\n    CheckStatus -- Yes --&gt; IdentifyMode{\"Unseal Mode?\"}\n\n    IdentifyMode -- \"Static\" --&gt; CheckSecret[\"Check Secret\"]\n    IdentifyMode -- \"Auto-Unseal\" --&gt; CheckLogs[\"Check Logs\"]\n\n    CheckSecret -- \"Missing\" --&gt; CreateSecret[\"Create Secret\"]\n    CheckLogs -- \"403/Auth\" --&gt; FixIAM[\"Fix IAM Permissions\"]\n    CheckLogs -- \"Timeout\" --&gt; FixNet[\"Fix Network/DNS\"]\n\n    FixIAM --&gt; ManualTry[\"Restart Pods\"]\n    FixNet --&gt; ManualTry\n\n    ManualTry -- \"Still Fails\" --&gt; ManualUnseal[\"Manual Unseal (Emergency)\"]</code></pre>"},{"location":"user-guide/openbaocluster/recovery/sealed-cluster/#diagnostics-by-mode","title":"Diagnostics by Mode","text":"<p>Identify your unseal mode in the <code>OpenBaoCluster</code> configuration:</p> <pre><code>spec:\n  unsealConfig:\n    type: awskms # or static, gcpckms, etc.\n</code></pre> Static (Default)Auto-Unseal (Cloud KMS)Manual (Emergency) <p>In Static mode, the operator assumes a Kubernetes Secret named <code>&lt;cluster-name&gt;-unseal-key</code> contains the key.</p> <p>Common Failure: The Secret is missing or has the wrong key name.</p> <ol> <li>Verify Secret Existence:     <pre><code>kubectl -n security get secret prod-cluster-unseal-key\n</code></pre></li> <li>Verify Key Format:     The Secret must have a key named <code>bao-root</code> (or as configured).     <pre><code>kubectl -n security get secret prod-cluster-unseal-key -o jsonpath='{.data}'\n</code></pre></li> </ol> <p>Fix: If missing, you must provide the unseal key (e.g., from a backup). <pre><code>kubectl -n security create secret generic prod-cluster-unseal-key --from-literal=bao-root=YOUR_UNSEAL_KEY\n</code></pre></p> <p>In Auto-Unseal mode, OpenBao connects to a remote KMS (AWS, GCP, Azure, OCI). failures are usually due to Identity or Network.</p> <p>1. Check OpenBao Logs</p> <p>Inspect the logs for \"failed to unseal\" messages.</p> <pre><code>kubectl -n security logs prod-cluster-0 | grep -i \"unseal\"\n</code></pre> <p>Common Errors:</p> Log Message Root Cause Fix <code>403 Forbidden</code> / <code>AccessDeniedPath</code> The IAM Role / ServiceAccount lacks permission to <code>Decrypt</code>. Grant <code>kms:Decrypt</code> (AWS) or <code>cloudkms.cryptoKeyVersions.useToDecrypt</code> (GCP) to the role. <code>context deadline exceeded</code> Network connectivity to the KMS endpoint is blocked. Check NetworkPolicies (<code>egress</code>), Istio Sidecars, or Firewall rules blocking HTTPS (443). <code>Internal (500)</code> The Cloud Provider is experiencing an outage. Check configured Region status. <p>Emergency Only</p> <p>Use this only if automation is permanently broken and you need immediate access.</p> <p>If the Operator cannot unseal the pods, you can manually unseal them using the <code>bao</code> CLI (if you have the unseal keys/shares).</p> <ol> <li>Exec into Pod 0:     <pre><code>kubectl -n security exec -ti prod-cluster-0 -- sh\n</code></pre></li> <li>Run Unseal:     <pre><code>bao operator unseal\n# Paste Unseal Key 1\n# Paste Unseal Key 2 (if shamir)\n...\n</code></pre></li> <li>Repeat:     You must perform this on every pod in the cluster (<code>prod-cluster-1</code>, <code>cluster-2</code>...).</li> </ol>"},{"location":"user-guide/openbaocluster/recovery/sealed-cluster/#post-recovery","title":"Post-Recovery","text":"<p>Once unsealed, verify the cluster is initialized and active.</p> <pre><code>kubectl -n security get openbaocluster prod-cluster\n</code></pre> <p>If the cluster unsealed successfully but assumes a Standby role (no active leader), check the No Leader guide.</p>"},{"location":"user-guide/openbaorestore/overview/","title":"OpenBaoRestore","text":"<p><code>OpenBaoRestore</code> is an operational CRD used to request a detailed restore operation from object storage into an existing <code>OpenBaoCluster</code>.</p> <p>It is designed to be declarative and immutable\u2014once a restore is completed, the CR status captures the result for audit purposes.</p>"},{"location":"user-guide/openbaorestore/overview/#restore-flow","title":"Restore Flow","text":"<p>The restore process involves downloading a specific snapshot from Cloud Storage and injecting it into the cluster's leader.</p> <pre><code>graph LR\n    Bucket[(\"fa:fa-bucket Cloud Storage\\n(S3/GCS)\")] --&gt;|Download| Job[[\"fa:fa-file-arrow-down Restore Job\"]]\n    Job --&gt;|Inject Snapshot| Cluster[(\"fa:fa-server OpenBao Cluster\")]\n    Cluster --&gt;|Reset| Key[\"fa:fa-key New Unseal Key\"]\n\n    classDef storage fill:transparent,stroke:#2563eb,stroke-width:2px,color:#000;\n    classDef process fill:transparent,stroke:#9333ea,stroke-width:2px,color:#000;\n\n    class Bucket,Cluster,Key storage;\n    class Job process;</code></pre>"},{"location":"user-guide/openbaorestore/overview/#use-cases","title":"Use Cases","text":"<ul> <li> <p> Disaster Recovery</p> <p>Recover from total cluster loss or data corruption by restoring from the latest healthy backup.</p> </li> <li> <p> Environment Cloning</p> <p>Clone a Production dataset into a Staging or Dev environment for realistic testing.</p> </li> <li> <p> Migration</p> <p>Move data between Kubernetes clusters or regions by backing up Source and restoring Target.</p> </li> </ul> <p>Data Overwrite</p> <p>A Restore operation completely overwrites the existing data in the target OpenBaoCluster. Ensure you are targeting the correct cluster.</p>"},{"location":"user-guide/openbaorestore/overview/#configuration","title":"Configuration","text":"<pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoRestore\nmetadata:\n  name: restore-job-001\n  namespace: security\nspec:\n  # The cluster to overwrite\n  cluster: prod-cluster\n\n  # The source of the backup\n  source:\n    type: s3\n    s3:\n      bucket: my-backups\n      region: us-east-1\n      key: backup-2024-01-01.snap\n</code></pre>"},{"location":"user-guide/openbaorestore/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Performing a Restore</li> <li>Backup Configuration</li> </ul>"},{"location":"user-guide/openbaorestore/recovery-restore-after-upgrade/","title":"Emergency Restore (Override Lock)","text":"<p>This guide describes how to Force Restore a cluster that is stuck in a locked state, typically after a Failed Upgrade or a crashed automation loop.</p> <p>Operation Locked</p> <p>The OpenBao Operator enforces Mutual Exclusion using <code>status.operationLock</code> on the <code>OpenBaoCluster</code>.</p> <p>Normally, you cannot start a Restore while an Upgrade or Backup is active (or presumed active). This prevents split-brain scenarios.</p>"},{"location":"user-guide/openbaorestore/recovery-restore-after-upgrade/#1-when-to-use-this","title":"1. When to use this","text":"<p>Use this procedure ONLY if:</p> <ol> <li>Use are recovering from a Failed Rollback where the cluster state is corrupted.</li> <li>The Operator is blocking your standard <code>OpenBaoRestore</code> with a \"Cluster is locked\" error.</li> <li>You accept Total Data Loss of the current cluster state (replacing it with the snapshot).</li> </ol>"},{"location":"user-guide/openbaorestore/recovery-restore-after-upgrade/#2-break-glass-configuration","title":"2. Break-Glass Configuration","text":"<p>To bypass the safety check, you must perform a \"Break Glass\" procedure by setting <code>spec.overrideOperationLock: true</code>.</p> <p>Data Overwrite &amp; Lock Removal</p> <p>This action will:</p> <ol> <li>Ignore the existing Operation Lock (clearing it).</li> <li>Overwrite the cluster data with the snapshot.</li> <li>Reset the cluster status to match the snapshot.</li> </ol> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoRestore\nmetadata:\n  name: emergency-restore-001\n  namespace: security\nspec:\n  cluster: prod-cluster\n\n  # Standard Source Config (see Restore Guide)\n  source:\n    target:\n      type: s3\n      s3:\n        bucket: openbao-backups\n        region: us-east-1\n        credentialsSecretRef:\n          name: s3-credentials\n    key: clusters/prod/last-good-snapshot.snap\n\n  jwtAuthRole: restore\n\n  # --- BREAK GLASS CONFIGURATION ---\n  force: true                  # (1)!\n  overrideOperationLock: true  # (2)!\n</code></pre> <ol> <li>Required to run restore on an \"Unhealthy\" cluster.</li> <li>Required to bypass the <code>status.operationLock</code>.</li> </ol>"},{"location":"user-guide/openbaorestore/recovery-restore-after-upgrade/#3-post-restore-recovery","title":"3. Post-Restore Recovery","text":"<p>After the emergency restore completes:</p> <ol> <li>Check Seal Status: The restored cluster may be sealed.<ul> <li> Recover Sealed Cluster</li> </ul> </li> <li>Check Raft Quorum: If the restored snapshot had different peers, you might need to fix the peer list.<ul> <li> Recover No Leader</li> </ul> </li> <li>Acknowledge Safe Mode: If the operator was in Safe Mode, you may need to patch the <code>breakGlassAck</code>.<ul> <li> Safe Mode Guide</li> </ul> </li> </ol>"},{"location":"user-guide/openbaorestore/restore/","title":"Restore Operations","text":"<p>The OpenBao Operator supports restoring clusters from snapshots stored in object storage (S3, GCS, Azure) using the <code>OpenBaoRestore</code> CRD.</p> <p>DATA OVERWRITE</p> <p>A Restore operation completely overwrites the existing data in the target OpenBaoCluster.</p> <p>All secrets, policies, auth methods, and keys will be replaced by the snapshot's state. This is destructive and irreversible.</p>"},{"location":"user-guide/openbaorestore/restore/#1-prerequisites","title":"1. Prerequisites","text":"<p>Network Requirements</p> <p>The target OpenBao cluster must be able to reach your Object Storage endpoint. Use <code>spec.network.egressRules</code> in your <code>OpenBaoCluster</code> configuration if you are running in a restricted environment.</p> <ul> <li> A valid snapshot in your Object Storage bucket (see Backups).</li> <li> The Target Cluster must exist and be initialized (even if it's just a fresh, empty cluster).</li> <li> Authentication credentials (JWT Role or Admin Token) to perform the restore.</li> </ul>"},{"location":"user-guide/openbaorestore/restore/#2-restore-workflow","title":"2. Restore Workflow","text":"<p>The restore process involves multiple phases to validate, download, and inject the snapshot.</p> <pre><code>graph TD\n    User([User]) --&gt;|Apply CRD| Pending{Pending}\n    Pending --&gt;|Validate| Validating\n    Validating --&gt;|Download Snapshot| Job[Restore Job]\n\n    subgraph Execution [\"Phase: Running\"]\n        Job --&gt;|Authenticate| Cluster[(\"fa:fa-server OpenBao Leader\")]\n        Job --&gt;|Force Restore /sys/storage/raft/snapshot-force| Cluster\n    end\n\n    Cluster --&gt;|Success| Completed([Completed])\n    Cluster --&gt;|Error| Failed([Failed])\n\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef process fill:transparent,stroke:#9333ea,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Completed write;\n    class Job,Execution process;\n    class User,Pending,Validating read;</code></pre>"},{"location":"user-guide/openbaorestore/restore/#3-configuration","title":"3. Configuration","text":""},{"location":"user-guide/openbaorestore/restore/#source-configuration","title":"Source Configuration","text":"<p>Define where your snapshot is located.</p> S3 (AWS/MinIO)GCS (Google Cloud)Azure Blob Storage <pre><code>source:\n  target:\n    type: s3\n    s3:\n      endpoint: https://s3.amazonaws.com\n      bucket: openbao-backups\n      region: us-east-1\n      credentialsSecretRef:\n        name: s3-credentials\n  key: clusters/prod/snapshot-latest.snap\n</code></pre> <pre><code>source:\n  target:\n    type: gcs\n    gcs:\n      bucket: my-gcs-backups\n      credentialsSecretRef:\n        name: gcs-credentials\n  key: clusters/prod/snapshot-latest.snap\n</code></pre> <pre><code>source:\n  target:\n    type: azure\n    azure:\n      container: my-container\n      accountName: myaccount\n      accountKeySecretRef:\n        name: azure-credentials\n        key: account-key\n  key: clusters/prod/snapshot-latest.snap\n</code></pre>"},{"location":"user-guide/openbaorestore/restore/#authentication","title":"Authentication","text":"<p>How the Restore Job authenticates to the OpenBao cluster leader.</p> JWT Auth (Recommended)Static Token <p>Uses a short-lived Kubernetes ServiceAccount token. Requires <code>sys/auth/jwt</code> to be enabled on the target.</p> <pre><code>spec:\n  jwtAuthRole: restore  # Must match the role configured in OpenBao\n</code></pre> OpenBao Config for JWT Auth <p>Run this in OpenBao to configure the role: <pre><code>bao write auth/jwt/role/restore \\\n    role_type=jwt \\\n    bound_audiences=openbao-internal \\\n    bound_subject=\"system:serviceaccount:openbao:prod-cluster-restore-serviceaccount\" \\\n    token_policies=restore \\\n    ttl=1h\n</code></pre></p> <p>Uses a long-lived OpenBao token stored in a Kubernetes Secret.</p> <p>Same-Namespace Requirement</p> <p>The token Secret must exist in the same namespace as the <code>OpenBaoRestore</code> resource. Cross-namespace references are not allowed for security reasons.</p> <pre><code>spec:\n  tokenSecretRef:\n    name: restore-token  # Must be in the same namespace as the OpenBaoRestore\n</code></pre>"},{"location":"user-guide/openbaorestore/restore/#4-full-example","title":"4. Full Example","text":"<pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoRestore\nmetadata:\n  name: dr-restore-001\n  namespace: security\nspec:\n  cluster: prod-cluster\n  force: true\n\n  source:\n    target:\n      type: s3\n      s3:\n         bucket: openbao-backups\n         region: us-east-1\n         credentialsSecretRef:\n           name: s3-creds\n    key: clusters/prod/backup-2024.snap\n\n  jwtAuthRole: restore\n</code></pre>"},{"location":"user-guide/openbaorestore/restore/#5-operations","title":"5. Operations","text":""},{"location":"user-guide/openbaorestore/restore/#monitoring-status","title":"Monitoring Status","text":"<p>Check the phases (<code>Pending</code> -&gt; <code>Running</code> -&gt; <code>Completed</code>).</p> <pre><code>kubectl get OBrestore -w\n</code></pre> <p>(Shortname <code>OBrestore</code> available)</p>"},{"location":"user-guide/openbaorestore/restore/#troubleshooting","title":"Troubleshooting","text":"Phase Common Error Resolution <code>Validating</code> <code>cluster not found</code> Ensure <code>spec.cluster</code> matches a valid <code>OpenBaoCluster</code> in the same namespace. <code>Running</code> <code>403 Forbidden</code> The Authentication (JWT Role/Token) lacks permission to <code>sys/storage/raft/snapshot-force</code>. <code>Running</code> <code>checksum mismatch</code> The snapshot size/hash changed during download. Check network stability. <code>Failed</code> <code>context deadline exceeded</code> The restore operation timed out. Check <code>spec.network</code> rules."},{"location":"user-guide/openbaorestore/restore/#6-safety-mechanics","title":"6. Safety Mechanics","text":""},{"location":"user-guide/openbaorestore/restore/#operation-lock","title":"Operation Lock","text":"<p>The Operator ensures Mutal Exclusion. You cannot run a Restore while an Upgrade or Backup is in progress.</p>"},{"location":"user-guide/openbaorestore/restore/#break-glass","title":"Break Glass","text":"<p>If the cluster is stuck in a locked state (e.g., a failed upgrade) and you MUST restore:</p> <pre><code>spec:\n  force: true\n  overrideOperationLock: true # (1)!\n</code></pre> <ol> <li>Bypasses the safety lock. Events will appear as Warnings.</li> </ol>"},{"location":"user-guide/openbaotenant/multi-tenancy/","title":"Multi-Tenancy Security","text":"<p>Running multiple <code>OpenBaoCluster</code> resources in a shared Kubernetes cluster requires strict isolation layers. This guide outlines how to secure tenants using RBAC, Network Policies, and Resource Quotas.</p> <p>Architecture Compatibility</p> <p>This guide applies to the default Multi-Tenant architecture. For Single-Tenant deployments, the controller manages the namespace directly without these isolation layers. See Single-Tenant Mode.</p>"},{"location":"user-guide/openbaotenant/multi-tenancy/#isolation-model","title":"Isolation Model","text":"<p>The Operator facilitates a Zero Trust model where each tenant is isolated by default.</p> <pre><code>graph TD\n    subgraph Cluster[\"Kubernetes Cluster\"]\n\n        subgraph TenantA[\"Tenant A (Namespace)\"]\n            RBAC_A[(\"RBAC Role\")]\n            NetPol_A[(\"NetworkPolicy\")]\n            Secret_A[\"Secrets (Root Token)\"]\n            Pod_A[\"OpenBao Pods\"]\n        end\n\n        subgraph TenantB[\"Tenant B (Namespace)\"]\n            RBAC_B[(\"RBAC Role\")]\n            NetPol_B[(\"NetworkPolicy\")]\n            Secret_B[\"Secrets (Root Token)\"]\n            Pod_B[\"OpenBao Pods\"]\n        end\n\n        Admin[\"Platform Admin\"] --&gt; RBAC_A\n        Admin --&gt; RBAC_B\n        UserA[\"User A\"] --&gt; RBAC_A\n        UserB[\"User B\"] --&gt; RBAC_B\n\n        RBAC_A -.-&gt;|Block| Secret_A\n        RBAC_B -.-&gt;|Block| Secret_B\n    end</code></pre>"},{"location":"user-guide/openbaotenant/multi-tenancy/#security-layers","title":"Security Layers","text":"<p>Configure isolation across four key layers:</p>  1. Identity (RBAC) 2. Network 3. Storage &amp; Quota 4. Compliance <p>The most critical layer is Identity. You must prevent tenants from accessing the \"Keys to the Kingdom\" (Root Tokens and Unseal Keys).</p> <p>By default, the Operator creates a <code>NetworkPolicy</code> that implements a Default Deny posture for Ingress.</p> <p>Prevent \"Noisy Neighbor\" issues where one tenant consumes all cluster resources.</p> <p>Ensure all workloads meet your organization's security baseline.</p>"},{"location":"user-guide/openbaotenant/multi-tenancy/#tenant-roles","title":"Tenant Roles","text":"<p>Use the provided ClusterRoles to granularly grant permissions.</p> Role Scope Description <code>openbaocluster-admin-role</code> Cluster Full control. Reserved for Platform Engineers. <code>openbaocluster-editor-role</code> Namespace Manage Clusters, CANNOT read Secrets. <code>openbaotenant-editor-role</code> Namespace Self-service onboarding via <code>OpenBaoTenant</code>."},{"location":"user-guide/openbaotenant/multi-tenancy/#blocking-secret-access","title":"Blocking Secret Access","text":"<p>Critical Restriction</p> <p>Tenants with <code>get secrets</code> permission can read the Root Token and Unseal Key, effectively becoming admins of their OpenBao cluster.</p> <p>Use a Policy Engine (Kyverno/Gatekeeper) to enforce this restriction.</p> Kyverno Policy: Block Sensitive Secrets<pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: block-openbao-sensitive-secrets\nspec:\n  validationFailureAction: Enforce\n  rules:\n    - name: block-root-token-access\n      match:\n        any:\n          - resources:\n              kinds:\n                - Secret\n              names:\n                - \"*-root-token\"\n                - \"*-unseal-key\"\n      exclude:\n        any:\n          - subjects:\n              - kind: ServiceAccount\n                name: openbao-operator-controller\n                namespace: openbao-operator-system\n      validate:\n        message: \"Access to OpenBao root token and unseal key is restricted.\"\n        deny: {}\n</code></pre>"},{"location":"user-guide/openbaotenant/multi-tenancy/#default-behavior","title":"Default Behavior","text":"<ul> <li>Ingress: Only allows traffic from within the cluster (OpenBao-to-OpenBao) and the generic Ingress Controller.</li> <li>Egress: Allows DNS, API Server, and cluster-internal traffic.</li> <li>Backup Jobs: Excluded from default policy to allow S3 access.</li> </ul>"},{"location":"user-guide/openbaotenant/multi-tenancy/#custom-restrictions","title":"Custom Restrictions","text":"<p>If you need strict egress control for backup jobs, apply a dedicated policy.</p> Backup Job Isolation<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backup-job-strict\n  namespace: tenant-a\nspec:\n  podSelector:\n    matchLabels:\n      openbao.org/component: backup\n  policyTypes: [Egress]\n  egress:\n    - to: # Allow S3\n        - ipBlock: { cidr: 0.0.0.0/0 }\n      ports: [{ port: 443, protocol: TCP }]\n</code></pre>"},{"location":"user-guide/openbaotenant/multi-tenancy/#resource-quotas","title":"Resource Quotas","text":"<p>Limit the number of nodes and storage a tenant can provision by configuring the <code>OpenBaoTenant</code> Custom Resource.</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoTenant\nmetadata:\n  name: tenant-a\nspec:\n  targetNamespace: tenant-a\n  quota:\n    hard:\n      pods: \"10\"\n      requests.storage: \"100Gi\"\n      requests.cpu: \"4\"\n      requests.memory: \"8Gi\"\n</code></pre>"},{"location":"user-guide/openbaotenant/multi-tenancy/#s3-bucket-isolation","title":"S3 Bucket Isolation","text":"<p>When using shared object storage for backups, ensure each tenant uses a unique Prefix and has credentials scoped only to that prefix.</p> AWS IAM Policy Example<pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": [\"s3:PutObject\", \"s3:GetObject\"],\n  \"Resource\": \"arn:aws:s3:::my-backups/tenant-a/*\"\n}\n</code></pre>"},{"location":"user-guide/openbaotenant/multi-tenancy/#pod-security-standards","title":"Pod Security Standards","text":"<p>The Operator is compatible with the Restricted profile.</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: tenant-a\n  labels:\n    # Enforce restricted profile\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n</code></pre>"},{"location":"user-guide/openbaotenant/multi-tenancy/#audit-logging","title":"Audit Logging","text":"<p>Enable Kubernetes Audit Logging to track who accesses the <code>OpenBaoCluster</code> CRD and related Secrets.</p>"},{"location":"user-guide/openbaotenant/multi-tenancy/#production-checklist","title":"Production Checklist","text":"<p>Verify these items for every tenant namespace.</p> <ul> <li> <p> RBAC Isolation</p> <p>Ensure users are bound to <code>openbaocluster-editor-role</code> (or stricter) and cannot read <code>*-root-token</code>.</p> </li> <li> <p> Network Policy</p> <p>Verify that cross-tenant traffic is blocked (Test: try <code>curl</code> from Tenant A to Tenant B).</p> </li> <li> <p> Storage Quota</p> <p>Confirm <code>ResourceQuota</code> is active to prevent storage exhaustion.</p> </li> <li> <p> S3 Isolation</p> <p>Verify backup credentials cannot list or read other tenants' prefixes.</p> </li> <li> <p> Global Checklist</p> <p>See the Production Readiness Checklist for cluster-level requirements.</p> </li> </ul>"},{"location":"user-guide/openbaotenant/onboarding/","title":"Tenant Onboarding &amp; Governance","text":"<p>Before creating an <code>OpenBaoCluster</code>, the target namespace must be provisioned with the necessary RBAC. The operator supports two governance models: Self-Service (decentralized) and Centralized Admin (strict control).</p>  Self-Service (Recommended) Centralized Admin <p>In this model, namespace owners can onboard themselves without cluster-admin intervention. This relies on the <code>Confused Deputy</code> prevention logic: users can only provision the namespace they already have access to.</p> <p>In this model, cluster administrators explicitly declare which namespaces are valid tenants. This is useful for strict environments where users should not self-provision.</p>"},{"location":"user-guide/openbaotenant/onboarding/#prerequisites","title":"Prerequisites","text":"<p>Ensure the <code>openbaotenant-editor-role</code> is bound to your user (this is aggregated to the standard <code>admin</code> and <code>edit</code> ClusterRoles by default).</p>"},{"location":"user-guide/openbaotenant/onboarding/#self-service-onboarding","title":"Steps","text":"<ol> <li> <p>Create an <code>OpenBaoTenant</code> resource in your own namespace, targeting that same namespace:</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoTenant\nmetadata:\n  name: my-tenant-onboarding\n  namespace: team-a-prod  # (1)!\nspec:\n  targetNamespace: team-a-prod # (2)!\n</code></pre> <ol> <li>Your namespace.</li> <li>MUST match metadata.namespace.</li> </ol> </li> <li> <p>Apply the resource:</p> <pre><code>kubectl apply -f my-tenant.yaml\n</code></pre> </li> <li> <p>The Provisioner controller will detect this valid request and create the necessary <code>Role</code> and <code>RoleBinding</code> in <code>team-a-prod</code> to allow the operator to manage resources.</p> </li> </ol>"},{"location":"user-guide/openbaotenant/onboarding/#security-note","title":"Security Note","text":"<p>If you attempt to target a different namespace (e.g., <code>targetNamespace: kube-system</code>), the controller will block the request and update the status with a <code>SecurityViolation</code> error.</p>"},{"location":"user-guide/openbaotenant/onboarding/#centralized-admin-onboarding","title":"Steps","text":"<ol> <li> <p>As a cluster administrator, create an <code>OpenBaoTenant</code> resource in the operator's namespace (typically <code>openbao-operator-system</code>):</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoTenant\nmetadata:\n  name: team-b-authorization\n  namespace: openbao-operator-system # (1)!\nspec:\n  targetNamespace: team-b-prod      # (2)!\n</code></pre> <ol> <li>Trusted namespace.</li> <li>Can be any namespace.</li> </ol> </li> <li> <p>Since the request originates from the trusted operator namespace, the controller allows cross-namespace provisioning.</p> </li> </ol>"},{"location":"user-guide/openbaotenant/onboarding/#3-verifying-provisioning","title":"3. Verifying Provisioning","text":"<p>Check the <code>OpenBaoTenant</code> status:</p> <pre><code>kubectl -n team-a-prod get openbaotenant my-tenant-onboarding -o yaml\n</code></pre> <p>Look for:</p> <ul> <li><code>status.provisioned: true</code>: RBAC successfully applied.</li> <li><code>status.lastError</code>: detailed error message if provisioning failed.</li> <li>Conditions:</li> <li><code>Type: Provisioned</code>, <code>Status: False</code>, <code>Reason: SecurityViolation</code>: You attempted an unauthorized cross-namespace provisioning.</li> </ul>"},{"location":"user-guide/openbaotenant/onboarding/#4-how-it-works-security-model","title":"4. How It Works (Security Model)","text":"<p>The operator uses a Trust-But-Verify approach:</p> <ol> <li>Trust: The Operator's own namespace (<code>openbao-operator-system</code>) is trusted. Resources created there can target any namespace.</li> <li>Verify: Resources created in user namespaces are verified. They must target their own namespace (<code>metadata.namespace == spec.targetNamespace</code>).</li> <li>Isolation: The Provisioner uses a delegated ServiceAccount with minimal permissions. It cannot list all namespaces in the cluster; it only acts on namespaces explicitly discovered via valid <code>OpenBaoTenant</code> CRs.</li> </ol>"},{"location":"user-guide/openbaotenant/overview/","title":"OpenBaoTenant","text":"<p><code>OpenBaoTenant</code> is the governance and onboarding CRD. It authorizes the Operator to manage OpenBao resources in a target namespace by provisioning tenant-scoped isolation.</p> <p>It is the key to Multi-Tenancy, ensuring that different teams can safely share a Kubernetes cluster without accessing each other's secrets.</p>"},{"location":"user-guide/openbaotenant/overview/#tenant-isolation-model","title":"Tenant Isolation Model","text":"<p>When you apply an <code>OpenBaoTenant</code>, the Operator creates a \"Sandbox\" around the target namespace.</p> <pre><code>graph TD\n    subgraph Namespace [\"Tenant Namespace\"]\n        direction TB\n        RBAC[\"fa:fa-id-badge RBAC RoleBinding\"]\n        NetPol[\"fa:fa-shield-halved NetworkPolicy\"]\n        Quota[\"fa:fa-chart-pie ResourceQuota\"]\n\n        App[[\"Tenant App\"]]\n\n        RBAC --&gt;|Binds| App\n        NetPol --&gt;|Isolates| App\n        Quota --&gt;|Limits| App\n    end\n\n    Op[\"fa:fa-gears Operator\"] --&gt;|Provisions| Namespace\n\n    classDef security fill:transparent,stroke:#dc2626,stroke-width:2px,color:#000;\n    class RBAC,NetPol,Quota security;</code></pre>"},{"location":"user-guide/openbaotenant/overview/#features","title":"Features","text":"<ul> <li> <p> Identity &amp; Access</p> <p>Automatically provisions Kubernetes RoleBindings to efficienty manage permissions for the Tenant.</p> </li> <li> <p> Network Isolation</p> <p>Enforces NetworkPolicies to block cross-tenant traffic, ensuring strict isolation between namespaces.</p> </li> <li> <p> Resource Quotas</p> <p>Applies ResourceQuotas to prevent a single tenant from consuming all cluster storage or compute. Limits are configurable via the <code>OpenBaoTenant</code> spec.</p> </li> </ul>"},{"location":"user-guide/openbaotenant/overview/#governance-models","title":"Governance Models","text":"<p>Choose the onboarding model that fits your organization.</p> <ul> <li> <p> Self-Service</p> <p>Developers create their own <code>OpenBaoTenant</code> in their own namespace.</p> <p>Best for: High-trash, low-friction environments.</p> <p> Self-Service Guide</p> </li> <li> <p> Centralized Admin</p> <p>Platform team creates <code>OpenBaoTenant</code> resources for teams.</p> <p>Best for: Strict compliance and audit trails.</p> <p> Admin Guide</p> </li> </ul>"},{"location":"user-guide/openbaotenant/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-Tenancy Security Guide</li> </ul>"},{"location":"user-guide/operator/installation/","title":"Operator Installation","text":"<p>This guide covers deploying the OpenBao Operator to your Kubernetes cluster.</p>"},{"location":"user-guide/operator/installation/#prerequisites","title":"Prerequisites","text":"<p>Requirements</p> <ul> <li>Kubernetes: v1.29+ (see Compatibility)</li> <li>kubectl: Installed and configured</li> <li>Permissions: Cluster-admin access for CRDs, RBAC, and ValidatingAdmissionPolicies</li> <li>Helm (optional): v3.12+ for Helm-based installation</li> </ul> <p>Deployment Modes</p> <p>The operator supports two deployment modes:</p> <ul> <li>Multi-Tenant (default): Platform teams providing OpenBao-as-a-Service</li> <li>Single-Tenant: Individual teams deploying OpenBao for their application</li> </ul> <p>See Single-Tenant Mode for single-tenant deployments.</p>"},{"location":"user-guide/operator/installation/#installation","title":"Installation","text":"Helm (Recommended) YAML Manifests Developer (Source) <p>Install the operator using the official Helm chart:</p> <pre><code>helm install openbao-operator oci://ghcr.io/dc-tec/charts/openbao-operator \\\n  --namespace openbao-operator-system \\\n  --create-namespace\n</code></pre> <p>Apply the installer manifest directly from the GitHub Release:</p> <pre><code>kubectl apply -f https://github.com/dc-tec/openbao-operator/releases/latest/download/install.yaml\n</code></pre> <p>Note</p> <p>This installs CRDs, RBAC, ValidatingAdmissionPolicies, and the operator deployments in <code>openbao-operator-system</code>.</p> <p>For local development and contribution:</p> <pre><code># Install CRDs\nmake install\n\n# Deploy operator (uses Kustomize)\nmake deploy IMG=ghcr.io/dc-tec/openbao-operator:dev\n</code></pre>"},{"location":"user-guide/operator/installation/#common-configuration","title":"Common Configuration","text":"<pre><code>helm install openbao-operator oci://ghcr.io/dc-tec/charts/openbao-operator \\\n  --namespace openbao-operator-system \\\n  --create-namespace \\\n  --set image.tag=v1.0.0 \\                          # (1)!\n  --set controller.replicas=2 \\                     # (2)!\n  --set controller.resources.limits.memory=512Mi    # (3)!\n</code></pre> <ol> <li>Pin to a specific version for production deployments.</li> <li>Run multiple replicas for high availability.</li> <li>Adjust resource limits based on cluster size.</li> </ol>"},{"location":"user-guide/operator/installation/#full-values-reference","title":"Full Values Reference","text":"Parameter Description Default <code>image.repository</code> Operator image repository <code>ghcr.io/dc-tec/openbao-operator</code> <code>image.tag</code> Image tag (defaults to appVersion) <code>\"\"</code> <code>image.pullPolicy</code> Image pull policy <code>IfNotPresent</code> <code>imagePullSecrets</code> Registry credentials <code>[]</code> <code>tenancy.mode</code> <code>multi</code> or <code>single</code> <code>multi</code> <code>tenancy.targetNamespace</code> Target namespace (single-tenant only) <code>\"\"</code> <code>controller.replicas</code> Controller replica count <code>1</code> <code>controller.resources</code> Controller resource requests/limits See values.yaml <code>provisioner.replicas</code> Provisioner replica count <code>1</code> <code>provisioner.resources</code> Provisioner resource requests/limits See values.yaml <code>admissionPolicies.enabled</code> Enable ValidatingAdmissionPolicies <code>true</code> <code>metrics.enabled</code> Enable metrics endpoints <code>true</code> <p> Full values.yaml</p>"},{"location":"user-guide/operator/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that the operator pods are running:</p> <pre><code>kubectl get pods -n openbao-operator-system\n</code></pre> <p>Expected output (multi-tenant mode):</p> <pre><code>NAME                                              READY   STATUS    RESTARTS   AGE\nopenbao-operator-controller-xxxxxxxxxx-xxxxx      1/1     Running   0          1m\nopenbao-operator-provisioner-xxxxxxxxxx-xxxxx     1/1     Running   0          1m\n</code></pre> <p>Ready</p> <p>Once both pods show <code>Running</code>, proceed to Getting Started to deploy your first OpenBao cluster.</p>"},{"location":"user-guide/operator/installation/#upgrading","title":"Upgrading","text":""},{"location":"user-guide/operator/installation/#helm-upgrades","title":"Helm Upgrades","text":"<p>CRD Updates</p> <p>Helm does not automatically upgrade CRDs. For releases with CRD changes:</p> <ol> <li>Apply CRDs from the release assets first:     <pre><code>kubectl apply -f https://github.com/dc-tec/openbao-operator/releases/download/vX.Y.Z/crds.yaml\n</code></pre></li> <li>Then upgrade the Helm release:     <pre><code>helm upgrade openbao-operator oci://ghcr.io/dc-tec/charts/openbao-operator \\\n  --namespace openbao-operator-system\n</code></pre></li> </ol>"},{"location":"user-guide/operator/installation/#yaml-manifest-upgrades","title":"YAML Manifest Upgrades","text":"<pre><code>kubectl apply -f https://github.com/dc-tec/openbao-operator/releases/download/vX.Y.Z/install.yaml\n</code></pre>"},{"location":"user-guide/operator/installation/#uninstallation","title":"Uninstallation","text":"Helm YAML Manifests <pre><code>helm uninstall openbao-operator --namespace openbao-operator-system\n</code></pre> <p>CRDs Retained</p> <p>Helm does not delete CRDs by design. To fully remove: <pre><code>kubectl delete crd openbaoclusters.openbao.org openbaorestores.openbao.org openbaotenants.openbao.org\n</code></pre></p> <pre><code>kubectl delete -f https://github.com/dc-tec/openbao-operator/releases/latest/download/install.yaml\n</code></pre>"},{"location":"user-guide/operator/installation/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Deploy a Cluster</p> <p>Create your first OpenBaoCluster.</p> <p> Getting Started</p> </li> <li> <p> Multi-Tenancy</p> <p>Onboard teams with OpenBaoTenant.</p> <p> Multi-Tenancy</p> </li> <li> <p> Single-Tenant</p> <p>Simplified deployment for single teams.</p> <p> Single-Tenant Mode</p> </li> </ul>"},{"location":"user-guide/operator/single-tenant-mode/","title":"Single-Tenant Mode","text":"<p>Single-tenant mode deploys only the Controller component, optimized for individual teams managing their own OpenBao cluster without multi-namespace orchestration.</p>"},{"location":"user-guide/operator/single-tenant-mode/#overview","title":"Overview","text":"<ul> <li> <p> Target Audience</p> <p>Individual teams deploying OpenBao for their application.</p> </li> <li> <p> Performance</p> <p>Event-driven reconciliation with namespace-scoped caching.</p> </li> <li> <p> Simplicity</p> <p>Controller only\u2014no Provisioner or OpenBaoTenant required.</p> </li> </ul>"},{"location":"user-guide/operator/single-tenant-mode/#architecture","title":"Architecture","text":"<p>In single-tenant mode, the Controller directly manages resources in a single namespace using efficient event-driven watches.</p> <pre><code>graph LR\n    subgraph OperatorNS[\"openbao-operator-system\"]\n        Controller[\"Controller\"]\n    end\n\n    subgraph TargetNS[\"Target Namespace\"]\n        Cluster[\"OpenBaoCluster\"]\n        STS[\"StatefulSet\"]\n        SVC[\"Services\"]\n    end\n\n    Controller --&gt;|Owns| Cluster\n    Cluster -.-&gt;|Creates| STS\n    Cluster -.-&gt;|Creates| SVC\n\n    classDef write fill:transparent,stroke:#22c55e,stroke-width:2px,color:#fff;\n    classDef read fill:transparent,stroke:#60a5fa,stroke-width:2px,color:#fff;\n\n    class Controller write;\n    class Cluster,STS,SVC read;</code></pre>"},{"location":"user-guide/operator/single-tenant-mode/#comparison","title":"Comparison","text":"Feature Multi-Tenant (Default) Single-Tenant Components Controller + Provisioner Controller only RBAC Model Per-namespace via OpenBaoTenant Direct RoleBinding Reconciliation Polling (cluster-wide) Event-driven (cached) Use Case Platform teams, shared infrastructure Individual teams, dedicated clusters"},{"location":"user-guide/operator/single-tenant-mode/#installation","title":"Installation","text":"Helm (Recommended) YAML Manifests <p>Deploy with tenancy mode set to <code>single</code>:</p> <pre><code>helm install openbao-operator oci://ghcr.io/dc-tec/charts/openbao-operator \\\n  --namespace openbao-operator-system \\\n  --create-namespace \\\n  --set tenancy.mode=single \\\n  --set tenancy.targetNamespace=openbao  # (1)!\n</code></pre> <ol> <li>The namespace where you will deploy your OpenBaoCluster. Defaults to the release namespace if not specified.</li> </ol> <p>For manual deployment without Helm:</p> <p>1. Apply CRDs</p> <pre><code>kubectl apply -f https://github.com/dc-tec/openbao-operator/releases/latest/download/crds.yaml\n</code></pre> <p>2. Apply ClusterRole</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/dc-tec/openbao-operator/main/config/rbac/single_tenant_clusterrole.yaml\n</code></pre> <p>3. Create Namespace and RoleBinding</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: openbao\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: openbao-operator-controller\n  namespace: openbao  # (1)!\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: openbao-operator-single-tenant\nsubjects:\n- kind: ServiceAccount\n  name: openbao-operator-controller\n  namespace: openbao-operator-system\n</code></pre> <ol> <li>The target namespace where OpenBaoCluster will be deployed.</li> </ol> <p>4. Patch Controller Deployment</p> <p>Add the <code>WATCH_NAMESPACE</code> environment variable:</p> <pre><code>kubectl set env deployment/openbao-operator-controller \\\n  -n openbao-operator-system \\\n  WATCH_NAMESPACE=openbao\n</code></pre>"},{"location":"user-guide/operator/single-tenant-mode/#configuration-options","title":"Configuration Options","text":"Parameter Description Default <code>tenancy.mode</code> Set to <code>single</code> for single-tenant mode <code>multi</code> <code>tenancy.targetNamespace</code> Target namespace for the controller <code>\"\"</code> (release namespace) <code>controller.replicas</code> Controller replica count <code>1</code> <code>controller.resources</code> Resource requests/limits See values.yaml <code>admissionPolicies.enabled</code> Enable ValidatingAdmissionPolicies <code>true</code> <p>Provisioner Excluded</p> <p>In single-tenant mode, the Provisioner deployment, its ServiceAccounts, and related RBAC are automatically excluded.</p>"},{"location":"user-guide/operator/single-tenant-mode/#verify-installation","title":"Verify Installation","text":"<pre><code>kubectl get pods -n openbao-operator-system\n</code></pre> <p>Expected output (single-tenant mode):</p> <pre><code>NAME                                              READY   STATUS    RESTARTS   AGE\nopenbao-operator-controller-xxxxxxxxxx-xxxxx      1/1     Running   0          1m\n</code></pre> <p>Ready</p> <p>Only the Controller is running. No Provisioner pod should be present.</p>"},{"location":"user-guide/operator/single-tenant-mode/#environment-variables","title":"Environment Variables","text":"Variable Description <code>WATCH_NAMESPACE</code> Required for manual deployments. Target namespace. Enables caching and event-driven reconciliation. Helm sets this automatically when <code>tenancy.mode=single</code>."},{"location":"user-guide/operator/single-tenant-mode/#migration","title":"Migration","text":"Multi-Tenant \u2192 Single-TenantSingle-Tenant \u2192 Multi-Tenant <ol> <li> <p>Backup OpenBaoCluster manifests</p> <pre><code>kubectl get openbaocluster -A -o yaml &gt; clusters-backup.yaml\n</code></pre> </li> <li> <p>Upgrade Helm release</p> <pre><code>helm upgrade openbao-operator oci://ghcr.io/dc-tec/charts/openbao-operator \\\n  --namespace openbao-operator-system \\\n  --set tenancy.mode=single \\\n  --set tenancy.targetNamespace=openbao\n</code></pre> </li> <li> <p>Cleanup OpenBaoTenants</p> <pre><code>kubectl delete openbaotenants --all\n</code></pre> </li> </ol> <p>RBAC Changes</p> <p>After migration, the Controller operates with direct namespace access instead of per-tenant RBAC.</p> <ol> <li> <p>Upgrade Helm release</p> <pre><code>helm upgrade openbao-operator oci://ghcr.io/dc-tec/charts/openbao-operator \\\n  --namespace openbao-operator-system \\\n  --set tenancy.mode=multi\n</code></pre> </li> <li> <p>Create OpenBaoTenants</p> <p>Onboard namespaces using <code>OpenBaoTenant</code> resources:</p> <pre><code>apiVersion: openbao.org/v1alpha1\nkind: OpenBaoTenant\nmetadata:\n  name: openbao-tenant\nspec:\n  namespaces:\n    - openbao\n</code></pre> </li> <li> <p>Cleanup manual RoleBindings</p> <pre><code>kubectl delete rolebinding openbao-operator-controller -n openbao\n</code></pre> </li> </ol>"},{"location":"user-guide/operator/single-tenant-mode/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Deploy a Cluster</p> <p>Create your OpenBaoCluster in the target namespace.</p> <p> Getting Started</p> </li> <li> <p> Configuration</p> <p>Configure TLS, storage, and security profiles.</p> <p> Configuration</p> </li> </ul>"}]}